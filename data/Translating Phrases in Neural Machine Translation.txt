Abstract Phrases play an important role in natural language understanding and machine translation Sag et 2002 Villavicencio et 2005 . However it is difficult to integrate them into current neural machine translation NMT which reads and generates sentences word by word . In this work we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a statistical machine translation SMT system into the architecture of NMT . At each decoding step the phrase memory is first by the SMT model which dynamically generates relevant target phrases with contextual information provided by the NMT model . Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory . If phrase generation is carried on the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase . Otherwise the NMT decoder generates a word from the vocabulary as the general NMT decoder does . Experiment results on the translation show that the proposed model achieves significant improvements over the baseline on various test sets . 1 Introduction Neural machine translation NMT has been receiving increasing attention due to its impressive author translation performance Kalchbrenner and Blunsom 2013 Cho et 2014 Sutskever et 2014 Bahdanau et 2015 Wu et 2016 . Significantly different from conventional statistical machine translation SMT Brown et 1993 Koehn et 2003 Chiang 2005 NMT adopts a big neural network to perform the entire translation process in one shot for which an encoderdecoder architecture is widely used . Specifically the encoder encodes a source sentence into a continuous vector representation then the decoder uses the continuous vector representation to generate the corresponding target translation word by word . The generation philosophy in NMT makes it difficult to translate phrases . Phrases especially expressions are crucial for natural language understanding and machine translation Sag et 2002 Villavicencio et 2005 as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts . Unfortunately current NMT is essentially a or Chung et 2016 and Fonollosa 2016 Luong and Manning 2016 translation system where phrases are not considered as translation units . In contrast phrases are much better than words as translation units in SMT and have made a significant advance in translation quality . Therefore a natural question arises: Can we translate phrases in NMT? Recently there have been some attempts on phrase generation in NMT Stahlberg et 2016b Zhang and Zong 2016 . However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the generation framework . To explore the phrase generation in NMT beyond the generation framework we propose a novel architecture that integrates a SMT 1421 model into NMT . Specifically we add an auxiliary phrase memory to store target phrases in symbolic form . At each decoding step guided by the decoding information from the NMT decoder the SMT model dynamically generates relevant target phrase translations and writes them to the memory . Then the NMT decoder scores phrases in the phrase memory and selects a proper phrase or word with the highest probability . If the phrase generation is carried out the NMT decoder generates a phrase and updates its decoding state by consuming the words in the selected phrase . Furthermore in order to enhance the ability of the NMT decoder to effectively select appropriate target phrases we modify the encoder of NMT to make it fit for exploring structural information of source sentences . Particularly we integrate syntactic chunk information into the NMT encoder to enrich the representation . We validate our proposed model on the translation task . Experiment results show that the proposed model significantly outperforms the conventional NMT by BLEU points on multiple NIST test sets . The rest of this paper is organized as follows . Section 2 briefly introduces the attentionbased NMT as background knowledge . Section 3 presents our proposed model which incorporates the phrase memory into the NMT architecture as well as the reading and writing procedures of the phrase memory . Section 4 presents our experiments on the translation task and reports the experiment results . Finally we discuss related work in Section 5 and conclude the paper in Section 6 . 2 Background Neural machine translation often adopts the architecture with recurrent neural networks RNN to model the translation process . The bidirectional RNN encoder which consists of a forward RNN and a backward RNN reads a source sentence x x1 x2 xTx and transforms it into word annotations of the entire source sentence h h1 h2 hTx . The decoder uses the annotations to emit a target sentence y y1 y2 yTy in a manner . In the training phase given a parallel sentence x y NMT models the conditional probability as follows P Y Ty P yi i x 1 where yi is the target word emitted by the decoder at step i and y i y1 y2 . The conditional probability P yi i x is computed as P yi i x sof tmax f si ci 2 where f is a function and si is the hidden state of the decoder at step i: si g ci 3 where g is a function . Here we adopt Gated Recurrent Unit Cho et 2014 as the recurrent unit for the encoder and decoder . ci is the context vector computed as a weighted sum of the annotations h: ci X Tx αt jhj 4 where hj is the annotation of source word xj and its weight αt j is computed by the attention model . We train the NMT model by maximizing the: C θ X N X Ty log P y n i n i x n 5 given the training data with N bilingual sentences Cho 2015 . In the testing phase given a source sentence x we use beam search strategy to search a target sentence yˆ that approximately maximizes the conditional probability P yˆ argmax y P 6 3 Approach In this section we introduce the proposed model which incorporates a phrase memory into the architecture of NMT . Inspired by the recent work on attaching an external structure to the architecture Gulcehre et 2016 Gu et 2016 Tang et 2016 Wang et 2017 we adopt a similar approach to incorporate the phrase memory into NMT . 1422 Figure 1: Architecture of the NMT decoder with the phrase memory . The NMT decoder performs phrase generation using the balancer and the phrase memory . Framework Figure 1 shows an example . Given the generated words President Bush emphasized that the model generates the next fragment either from a word generation mode or a phrase generation mode . If the model selects the word generation mode it generates a word by the NMT decoder as in the standard NMT framework . Otherwise it generates a phrase by enquiring a phrase memory which is written by an SMT decoder based on the dynamic decoding information from the NMT model for each step . The between word generation mode and phrase generation mode is balanced by a weight λ which is produced by a neural network based balancer . Formally a generated translation y y1 y2 . . . yTy consists of two sets of fragments: words generated by NMT decoder w w1 w2 . . . wK and phrases generated from the phrase memory p p1 p2 . . . pL . The probability of generating y is calculated by P Y 1 λt wk Pword wk Y λt pl Pphrase pl 7 where Pword wk is the probability of generating the word wk see Equation 2 Pphrase pl is that of generating the phrase pl which will be described in Section and t is the decoding step to generate the corresponding fragment . The balancing weight λ is produced by the balancer a network . The balancer network takes as input the decoding information including the context vector ci the previous decoding state and the previous generated word: λi σ fb si ci 8 where σ is a sigmoid function and fb is the activation function . Intuitively the weight λ can be treated as the estimated importance of the phrase to be generated . We expect λ to be high if the phrase is appropriate at the current decoding step . Phrases We employ a sourceside chunker to chunk the source sentence and only phrases that corresponds to a source chunk are used in our model . We restrict ourselves to the chunk phrases based on the following considerations : 1 In order to take advantage of dynamic programming we restrict ourselves to 2 We explicitly utilize the boundary information of the chunk phrases to better guide the proposed model to adopt a target phrase at an appropriate decoding step . 3 We enable the model to exploit the syntactic categories of chunk phrases to enhance the proposed model with its selection preference for special target phrases . With these information we enrich the context vector ci to enable the proposed model to make better decisions as described below . Following the strategy in sequence tagging tasks Xue and Shen 2003 we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word . For example the phrase E S information security is tagged as a noun phrase NP and the tag sequence should be NP B NP . Partially motivated by the work on integrating linguistic features into NMT Sennrich and Haddow 2016 we represent the encoder input as the combination of word embeddings and chunking tag embeddings instead of word embeddings alone in the conventional NMT . The new input is formulated as follows: E wxi Et ti 9 1Overlapped phrases may result in a high dimensionality in translation hypothesis representation and make it hard to employ shared fragments for efficient dynamic programming . 1423 where Ew R NMT is a word embedding matrix and dw is the word embedding dimensionality Et R T is a tag embedding matrix and dt is the tag embedding dimensionality . is the vector concatenation operation . Phrase Memory The phrase memory stores relevant target phrases provided by an SMT model which is trained on the same bilingual corpora . At each decoding step the memory is firstly erased and by the SMT model the decoding of which is based on the translation information provided by the NMT model . Then the proposed model enquires phrases along with their probabilities Pphrase from the memory . Writing to Phrase Memory Given a partial translation y i y1 y2 . . . generated from NMT the SMT model picks potential phrases extracted from the translation table . The phrases are scored with multiple SMT features including the language model score the translation probabilities the reordering score and so on . Specially the reordering score depends on alignment information between source and target words which is derived from attention distribution produced by the NMT model Wang et 2017 . SMT coverage vector in Wang et 2017 is also introduced to avoid repeat phrasal recommendations . In our work the potential phrase is phrase with high SMT score which is defined as following: SMTscore pl t x X M wmhm pl x pl 10 where pl is a target phrase and x pl is its corresponding source span . hm pl x pl is a SMT feature function and wm is its weight . The feature weights can be tuned by the minimum error rate training MERT algorithm Och 2003 . This leads to a better interaction between SMT and NMT models . It should be emphasized that our memory is dynamically updated at each decoding step based on the decoding history from both SMT and NMT models . The proposed model is very flexible where the phrase memory can be either fully dynamically generated by an SMT model or directly extracted from a bilingual dictionary or any other bilingual resources storing idiomatic translations or bilingual expressions which may lead to a further improvement . 2 Reading Phrase Memory When phrases are read from the memory they are rescored by a neural network based score function . The score function takes as input the phrase itself and decoding information from NMT i t pl denotes the current decoding step: scorephrase pl gs e pl si ci 11 where gs is either an identity or a function . e pl is the representation of phrase pl which is modeled by a recurrent neural networks . Again si is the decoder state is the lastly generated word and ci is the context vector . The scores are normalized for all phrases in the phrase memory and the probability for phrase pl is calculated as Pphrase pl sof tmax scorephrase pl 12 The probability calculation is controlled with parameters which are trained together with the parameters from the NMT model . Training Formally we train both the default parameters of standard NMT and the new parameters associated with phrase generation on a set of training examples x n y n N: C θ X N log P y n n 13 where P y n n is defined in Equation 7 . Ideally the trained model is expected to produce a higher balance weight λ and phrase probability Pphrase when a phrase is selected from the memory and lower scores in other cases . Decoding During testing the NMT decoder generates a target sentence which consists of a mixture of words and phrases . Due to the different granularities of words and phrases we design a variant of beam search strategy: At decoding step i we first compute Pphrase for all phrases in the phrase memory 2Bilingual resources can be utilized in two ways: First we can store the bilingual resources in a static memory and keep all items available to NMT in the whole decoding period . Second we can integrate the bilingual resources into SMT and then dynamically feed them into the phrase memory . 142 and Pword for all words in NMT vocabulary . Then the balancer outputs a balancing weight λi which is used to scale the phrase and word probabilities: λi Pphrase and 1 λi Pword . Now outputs are normalized probabilities on the concatenation of phrase memory and the general NMT vocabulary . At last the NMT decoder generates a proper phrase or word of the highest probability . If a target phrase in the phrase memory has the highest probability the decoder generates the target phrase to complete the phrase generation process and updates its decoding state by consuming the words in the selected phrase as described in Equation 3 . All translation hypotheses are placed in the corresponding beams according to the number of generated target words . 4 Experiments In this section we evaluated the effectiveness of our model on the machine translation task . The training corpora consisted of about million sentence pairs3 with million Chinese words and million English words respectively . We used NIST 2006 NIST06 dataset as development set and NIST 2004 NIST04 2005 NIST05 and 2008 NIST08 datasets as test sets . We report experiment results with BLEU score4 . We compared our proposed model with two systems: Moses: a SMT system Koehn et 2007 with its default settings where feature function weights are tuned by the minimum error rate training MERT algorithm Och 2003 . RNNSearch: an implementation of the NMT system Bahdanau et 2015 with its default settings . For Moses we used the full bilingual training data to train the SMT model and the target portion of the bilingual training data to train a language model using KenLM5 . We ran on the training data in both and 3The corpus includes LDC2002E18 LDC2003E07 LDC2003E14 Hansards portion of LDC2004T07 LDC2004T08 and LDC2005T06 . 4 ftp : 5 https: directions and applied the refinement rule Koehn et 2003 to obtain word alignments . The maximum phrase length is set to 7 . For RNNSearch we generally followed settings in the previous work Bahdanau et 2015 Tu et 2017a b . We only kept a shortlist of the most frequent words in Chinese and English covering approximately and of the data in the two languages respectively . We constrained our source and target sequences to have a maximum length of 50 words in the training data . The size of embedding layer of both sides was set to 620 and the size of hidden layer was set to 1000 . We used a minibatch stochastic gradient descent SGD algorithm of size 80 together with Adadelta Zeiler 2012 to train the NMT models . The decay rates ρ and were set as and . We clipped the gradient norm to Pascanu et 2013 . We also adopted the dropout technique . Dropout was applied only on the output layer and the dropout rate was set to . We used a simple beam search decoder with beam size 10 to find the most likely translation . For the proposed model we used a Chinese chunker6 Zhu et 2015 to chunk the sourceside Chinese sentences . 13 chunking tags appeared in our chunked sentences and the size of chunking tag embedding was set to 10 . We used the trained SMT to translate the chunks . The top 5 translations according to their translation scores Equation 10 were kept and among them phrases were used as phrasal recommendations for each source chunk phrase . For a chunk phrase if there exists phrasal recommendations from SMT the output chunk tag was used as its chunking tag feature as described in Section . Otherwise the words in the chunk were treated as general words by being tagged with the default tag . In the phrase memory we only keep the top 7 target translations with highest SMT scores at each decoding step . We used a forward neural network with two hidden layers for both the balancer Equation 8 and the scoring function Equation 11 . The numbers of units in the hidden layers were set to 2000 and 500 respectively . We used a backward RNN encoder to learn the phrase representations of target phrases in the phrase memory . 6 http : 1425 SYSTEM NIST04 NIST05 NIST08 Avg Moses RNNSearch tag Table 1: Main experiment results on the NIST translation task . BLEU scores in the table are case insensitive . Moses and RNNSearch are SMT and NMT baseline system respectively .: significantly better than RNNSearch p: significantly better than RNNSearch p . NIST04 NIST05 NIST08 tag Table 2: Percentages of sentences that contain phrases generated by the proposed model . Main Results Table 1 reports main results of different models measured in terms of BLEU score . We observe that our implementation of RNNSearch outperforms Moses by BLEU points . which is the proposed model with the phrase memory obtains an improvement of BLEU points over the baseline RNNSearch . With the sourceside chunking tag feature tag outperforms the baseline RNNSearch by BLEU points showing the effectiveness of chunking syntactic categories on the selection of appropriate target phrases . From here on we use tag as the default setting in the following experiments if not otherwise stated . Number of Sentences Affected by Generated Phrases We also check the number of translations that contain phrases generated by the proposed model as shown in Table 2 . As seen a large portion of translations take the recommended phrases and the number increases when the chunking tag feature is Considering BLEU scores reported in Table 1 we believe that the chunking tag feature benefits the proposed model on its phrase generation . Analysis on Generated Phrases Syntactic Categories of Generated Phrases We first investigate which category of phrases is more likely to be selected by the proposed approach . There are some phrases such as 7The numbers on NIST08 are relatively lower since part of the test set contains sentences from Web forums which contain less expressions . Type All New Total Correct Total Correct NP VP QP Others 0 0 Sum 100 Table 3: Percentages of phrase categories to the total number of generated ones . All denotes all generated phrases and New means new phrases that cannot be found in translations generated by the baseline system . Total is the total number of generated phrases and Correct denotes the fully correct ones . noun phrases NPs national laboratory and vietnam airlines and quantifier phrases QPs 15 seconds and two weeks that we expect to be favored by our approach . Statistics shown in Table 3 confirm our hypothesis . Let s first concern all generated phrases column All: most selected phrases are noun phrases and quantifier phrases . Among them percent of them are fully correct8 . Specifically NPs have relative higher generation accuracy while VPs have lower accuracy . By looking into the wrong cases we found most errors are related to verb tense which is the drawback of SMT models . Concerning the newly introduced phrases that cannot be found in baseline translations column New of generated phrases are both new and fully correct which contribute most to the performance improvement . We can also find that most newly introduced verb phrases and quantifier phrases are not correct the patterns of which can be well learned by NMT models . 8 Fully correct means that the generated phrases can be retrieved in corresponding references as a whole unit . 1426 Words All New Total Correct Total Correct 2 3 4 5 Table 4: Percentages of phrases with different word counts to the total number of generated ones . Number of Words in Generated Phrases Table 4 lists the distribution of generated phrases based on the number of inside words . As seen most generated phrases are short phrases gram and phrases which also contribute most to the new and fully correct phrases . Focusing on long phrases order 4 most of them are newly introduced out of . Unfortunately only a few portion of these phrases are fully correct since long phrases have higher chance to contain one or two unmatched words . SYSTEM Test tag tag Table 5: Additional experiment results on the translation task to directly measure the improvement obtained by the phrase generation . denotes that we replace the generated target phrases with a special symbol in test sets . BLEU scores in the table are case insensitive . Effect of Generated Phrases on Translation Performance Note that the proposed model benefits not only from fully matched phrases but also from partially matched phrases . For example the baseline system translates I ò ò in a manner and outputs state aviation and space department . The generated phrase provided by SMT is national aviation and space administration but the only correct reference is national aeronautics and space administration . The generated phrase is not fully correct but still useful . To directly measure the improvement obtained by the phrase generation we replace the generated target phrases with a special symbol NULL in test sets . As shown in Table 5 when deleting the generated target phrases tag and translation performances decrease by BLEU points and BLEU points respectively . Moreover translation performances on NIST08 decrease less than those on NIST04 and NIST05 in both settings . The reason is that NIST08 which contains sentences from web data has little influence on generating target phrases which are provided from a different domain 9 . The overall results demonstrate that neural machine translation benefits from phrase translation . Effect of Balancer Weight Test Dynamic Constant λ Table 6: Translation performance with a variety of balancing weight strategies . Dynamic is the proposed approach and Constant λ denotes fixing the balancing weight to . BLEU scores in the table are case insensitive . The balancer which is used to coordinate the phrase generation and word generation is very crucial for the proposed model . We conducted an additional experiment to validate the effectiveness of the neural network based balancer . We use the setting tag as baseline system to conduct the experiments . In this experiment we fixed the balancing weight λ Equation 8 to during training and testing and report the results . As shown in Table 6 we find that using the fixed value for the balancing weight Constant λ decreases the translation performance sharply . This demonstrates that the neural network based balancer is an essential component for the proposed model . Comparison to Recommendations and Discussions Our approach is related to our previous work Wang et 2017 which integrates the SMT knowledge into NMT . To make a comparison we conducted experiments followed settings in Wang et 2017 . The comparison results are reported in Table 7 . We find that our approach is marginally better than the 9The parallel training data are mainly from news domain . 1427 SYSTEM Test level recommendation tag Table 7: Experiment results on the translation task . level recommendation is the proposed model in Wang et 2017 . BLEU scores in the table are case insensitive . model proposed in Wang et 2017 by BLEU points . In our approach the SMT model translates chunk phrases using the NMT decoding information . Although we use target phrases as phrasal recommendations our approach still suffers from the errors in segmentation and chunking . For example the target phrase laptop computers cannot be recommended by the SMT model if the Chinese phrase Ã J M is not chunked as a phrase unit . This is the reason why some sentences do not have corresponding phrasal recommendations Table 2 . Therefore our approach can be further enhanced if we can reduce the error propagations from the segmenter or chunker for example by using chunk sequences instead of the single best chunk sequence . Additionally we also observe that some target phrasal recommendations have been also generated by the baseline system in a manner . These phrases even taken as parts of final translations by the proposed model do not lead to improvements in terms of BLEU as they have already occurred in translations from the baseline system . For example the proposed model successfully carries out the phrase generation mode to generate a target phrase guangdong province the translation of Chinese phrase 2 é which has appeared in the baseline system . As external resources bilingual dictionary which are complementary to the SMT phrasal recommendations are compatible with the proposed model we believe that the proposed model will get further improvement by using external resources . 5 Related work Our work is related to the following research topics on NMT: Generating phrases for NMT In these studies the generated NMT phrases are either from an SMT model or a bilingual dictionary . In syntactically guided neural machine translation SGNMT the NMT decoder uses phrase translations produced by the hierarchical phrasebased SMT system Hiero as hard decoding constraints . In this way syntactic phrases are generated by the NMT decoder Stahlberg et 2016b . Zhang and Zong 2016 use an SMT translation system which is integrated an additional bilingual dictionary to synthesize sentences and feed the sentences into the training of NMT in order to translate words or phrases . Tang et al . 2016 propose an external phrase memory that stores phrase pairs in symbolic forms for NMT . During decoding the NMT decoder enquires the phrase memory and properly generates phrase translations . The significant differences between these efforts and ours are 1 that we dynamically generate phrase translations via an SMT model and 2 that at the same time we modify the encoder to incorporate structural information to enhance the capability of NMT in phrase translation . Incorporating linguistic information into NMT NMT is essentially a sequence to sequence mapping network that treats the units words subwords Sennrich et 2016 characters Chung et 2016 and losa 2016 as symbols . However linguistic information can be viewed as the taskspecific knowledge which may be a useful supplementary to the sequence to sequence mapping network . To this end various kinds of linguistic annotations have been introduced into NMT to improve its translation performance . Sennrich and Haddow 2016 enrich the input units of NMT with various linguistic features including lemmas tags syntactic dependency labels and morphological features . et al . 2016 propose factored NMT using the morphological and grammatical decomposition of the words factors in output units . Eriguchi et al . 2016 explore the phrase structures of input sentences and propose a attention model for the vanilla NMT model . Li et al . 2017 propose to linearize parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT while Aharoni and Goldberg 2017 propose to incorporate syntactic information into NMT by serializing the target sequences into linearized lexicalized constituency trees . Zhang 1428 et al . 2016 integrate topic knowledge into NMT for adaptation . Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT . He et al . 2016 integrate SMT features with the NMT model under the framework in order to help NMT alleviate the limited vocabulary problem Luong et 2015 Jean et 2015 and coverage problem Tu et 2016 . Arthur et al . 2016 observe that NMT is prone to making mistakes in translating content words and therefore attempt at incorporating discrete translation lexicons into the NMT model to alliterate the imprecise translation problem Wang et 2017 . Motivated by the complementary strengths of syntactical SMT and NMT different combination schemes of Hiero and NMT have been exploited to form SGNMT Stahlberg et 2016a b . Wang et al . 2017 propose an approach to incorporate the SMT model into NMT . They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights . Niehues et al . 2016 propose to use SMT to the inputs into target translations and employ the target as input sequences in NMT . Zhou et al . 2017 propose a neural system combination framework to directly combine NMT and SMT outputs . The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system s suggestion quality Wuebker et 2016 . In addition word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT Cohn et 2016 Mi et 2016 Liu et 2016 . 6 Conclusion In this paper we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the architecture . At decoding the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory . Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory . Finally the NMT decoder selects a phrase from the phrase memory or a word from the vocabulary of the highest probability to generate . Experiment results on translation have demonstrated that the proposed model can significantly improve the translation performance . Acknowledgments We would like to thank three anonymous reviewers for their insightful comments and also acknowledge Zhengdong Lu Lili Mou for useful discussions . This work was supported by the National Natural Science Foundation of China Grants 61373095 and 61622209.