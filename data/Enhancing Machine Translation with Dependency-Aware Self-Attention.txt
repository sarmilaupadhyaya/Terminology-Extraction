Abstract Most neural machine translation models only rely on pairs of parallel sentences assuming syntactic information is automatically learned by an attention mechanism . In this work we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel mechanism that improves its translation quality especially for long sentences and in scenarios . We show the efficacy of each approach on WMT and and WAT translation tasks . 1 Introduction Research in neural machine translation NMT has mostly exploited corpora consisting of pairs of parallel sentences with the assumption that a model can automatically learn prior linguistic knowledge via an attention mechanism Luong et 2015 . However Shi et al . 2006 found that these models still fail to capture deep structural details and several studies Sennrich and Haddow 2016 Eriguchi et 2017 Chen et 2017 2018 have shown that syntactic information has the potential to improve these models . Nevertheless the majority of NMT models are based on recurrent neural networks RNNs Elman 1990 with only a few recent studies that have investigated methods for the Transformer model Vaswani et 2017 . Wu et al . 2018 evaluated an approach to incorporate syntax in NMT with a Transformer model which not only required three encoders and two decoders but also dependency relations precluding its use to target languages . Zhang et al . 2019 integrate syntax by concatenating the intermediate representations of a dependency parser to word embeddings . done while at Tokyo Institute of Technology . In contrast to ours this approach does not allow to learn units at the source side requiring a larger vocabulary to minimize words . Saunders et al . 2018 interleave words with syntax representations which results in longer sequences requiring gradient accumulation for effective training while only leading to BLEU on WAT when using ensembles of Transformers . Finally Currey and Heafield 2019 propose two simple data augmentation techniques to incorporate syntax: one that works well on data and one that achieves a high score on a task . Our approach on the other hand performs equally well in both settings . While these studies improve the translation quality of the Transformer they do not exploit its properties . In response we propose to explicitly enhance the its mechanism a core component of this architecture to include syntactic information without compromising its flexibility . Recent studies have in fact shown that networks benefit from modeling local contexts by reducing the dispersion of the attention distribution Shaw et 2018 Yang et 2018 2019 and that they might not capture the inherent syntactic structure of languages as well as recurrent models especially in settings Tran et 2018 Tang et 2018 . Here we present parentscaled PASCAL: a novel parameterfree local attention mechanism that lets the model focus on the dependency parent of each token when encoding the source sentence . Our method is simple yet effective improving translation quality with no additional parameter or computational overhead . Our main contributions are: introducing PASCAL: an effective parameterfree local mechanism to incorporate syntax into Transformers adapting LISA Strubell et 2018 to subword representations and applying it to NMT 1619 Dᵖ dmodel WV ʰ WQ ʰ WK ʰ d dmodel T X d T Vʰ Qʰ Kʰ d T T Sʰ softmax p 2 3 3 5 3 The monkey eats a banana Input: The monkey eats a banana T T T T Nʰ T d Mʰ T dist Figure 1: PASCAL head for the input sequence The monkey eats a banana . similar to concurrent work Pham et 2019 we find that modeling linguistic knowledge into the mechanism leads to better translations than other approaches . Our extensive experiments on standard and translation tasks also show that a approaches to embed syntax in RNNs do not always transfer to the Transformer and b PASCAL consistently exhibits significant improvements in translation quality especially for long sentences . 2 Model In order to design a neural network that is efficient to train and that exploits syntactic information while producing translations we base our model on the Transformer architecture Vaswani et 2017 and upgrade its encoder with PASCAL heads at layer ls . PASCAL heads enforce contextualization from the syntactic dependencies of each source token and in practice we replace standard selfattention heads with PASCAL ones in the first layer as its inputs are word embeddings that lack any contextual information . Our PASCAL has the same number H of attention heads as other layers . Source syntax Similar to previous work instead of just providing sequences of tokens we supply the encoder with dependency relations given by an external parser . Our approach explicitly exploits units which enable translation: after generating units we compute the middle position of each word in terms of number of tokens . For instance if a word in position 4 is split into three tokens now in positions 6 7 and 8 its middle position is 7 . We then map each of a given word to the middle position of its parent . For the root word we define its parent to be itself resulting in a parse that is a directed graph . The input to our encoder is a sequence of T tokens and the absolute positions of their parents . Figure 1 shows our sublayer . Here for a sequence of length T the input to each head is a matrix X R T of token embeddings and a vector p R T whose entry pt is the middle position of the token s dependency parent . Following Vaswani et al . 2017 in each attention head h we compute three vectors called query key and value for each token resulting in the three matrices Kh R T Qh R T and Vh R T for the whole sequence where d . We then compute dot products between each query and all the keys giving scores of how much focus to place on other parts of the input when encoding a token at a given position . The scores are divided by d to alleviate the vanishing gradient problem arising if dot products are large: S h Qh Kh 1 Our main contribution is in weighing the scores of the token at position t st by the distance of each token from the position of t s dependency parent: n h tj s h tj d p tj for j 1 T 2 where n h t is the row of the matrix Nh R T representing scores normalized by the proximity to t s parent . d p tj dist pt j is the t j th entry of the matrix Dp R T containing for each row dt the distances of every token j from the middle position of token t s dependency parent pt . In this paper we compute this distance as the value of the probability density of a normal distribution centered at pt and with variance σ 2 N pt σ2: dist pt j fN j pt σ2 1 2πσ2 e 2 2σ2 . 1620 Finally we apply a softmax function to yield a distribution of weights for each token over all the tokens in the sentence and multiply the resulting matrix with the value matrix V h obtaining the final representations Mh for PASCAL head One of the major strengths of our proposal is being: no additional parameter is required to train our PASCAL as D p is obtained by computing a distance function that only depends on the vector of tokens parent positions and can be evaluated using fast matrix operations . Parent ignoring Due to the lack of parallel corpora with parses we rely on noisy annotations from an external parser . However the performance of syntactic parsers drops abruptly when evaluated on data Dredze et 2007 . To prevent our model from overfitting to noisy dependencies we introduce a regularization technique for the PASCAL: parent ignoring . In a similar vein as dropout Srivastava et 2014 we disregard information during the training phase . Here we ignore the position of the parent of a given token by randomly setting each row of Dp to 1 R T with some probability Gaussian weighing function The choice of weighing each score by a Gaussian probability density is motivated by two of its properties . First its curve: It allows us to focus most of the probability density at the mean of the distribution which we set to the middle position of the units of the dependency parent of each token . In our experiments we find that most words in the vocabularies are not split into hence allowing PASCAL to mostly focus on the actual parent . In addition weights are placed on the neighbors of the parent token allowing the attention mechanism to also attend to them . This could be useful for instance to learn idiomatic expressions such as prepositional verbs in English . The second property of distributions that we exploit is their support: While most of the weight is placed in a small window of tokens around the mean of the distribution all the values in the sequence are actually multiplied by factors allowing a token j farther away from the parent of token t pt to still play a role in the representation of t if its score s h tj is high . PASCAL can be seen as an extension of the local attention mechanism of Luong et al . 2015 with the alignment now guided by syntactic information . Yang et al . 2018 proposed a method to learn a Gaussian bias that is added to instead of multiplied by the original attention distribution . As we will see next our model significantly outperforms this . 3 Experiments Experimental Setup Data We evaluate the efficacy of our approach on standard benchmarks and on lowresource scenarios where the Transformer was shown to induce poorer syntax . Following Bastings et al . 2017 we use News Commentary v11 NC11 with and tasks to simulate low resources and test multiple source languages . To compare with previous work we train our models on WMT16 and WAT tasks removing sentences in incorrect languages from WMT16 data sets . For a thorough comparison with concurrent work we also evaluate on the largescale WMT17 and WMT18 tasks . We rely on Stanford CoreNLP Manning et 2014 to parse source Training We implement our models in PyTorch on top of the Fairseq Hyperparameters including the number of PASCAL heads that achieved the highest validation BLEU Papineni et 2002 score were selected via a small grid search . We report previous results in NMT for completeness and train a Transformer model as a strong standard baseline . We also investigate the following Transformer: The model presented in . The variance of the normal distribution was set to 1 an effective window size of 3 as of the source words in our training sets are at most split into 7 units .: We adapt LISA Strubell et 2018 to NMT and units by defining the parent of a given token as its first which represents the root of the parent word .: Our implementation of the approach by Currey and Heafield 2019 where a standard Transformer learns to both parse and translate source sentences . H: Following Sennrich and Haddow 2016 we introduce syntactic information in the form of dependency labels in the embedding matrix of the Transformer encoder . 1 For a detailed description see Appendix A . 2https: . 1621 0 10 10 20 20 30 30 40 40 50 50 100 Source sentence length B L E U NC11 NC11 WMT18 WMT16 WMT17 WAT 0 10 10 20 20 30 30 40 40 50 50 100 Source sentence length 0 10 20 30 40 Sentences Figure 2: Analysis by sentence length: with the Transformer above and percentage of data below . Method NC11 NC11 WMT18 WMT16 WMT17 WAT B R Eriguchi et al . 2016 Bastings et al . 2017 Hashimoto and Tsuruoka 2017 Bisk and Tran 2018 Wu et 2018 Wu et 2018 Mixed Enc . Currey and Heafield 2019 Currey and Heafield 2019 Transformer PASCAL LISA S H Table 1: Test BLEU and RIBES for scores on left and right data sets . Models that also require syntax information are marked with while indicates statistical significance p against the Transformer baseline via bootstrap Koehn 2004 . Results Table 1 presents the main results of our experiments . Clearly the base Transformer outperforms previous approaches proving it to be a strong baseline in our experiments . The table shows that the simple approach of Sennrich and Haddow 2016 does not lead to notable advantages when applied to the embeddings of the Transformer model . We also see that the approach benefits from better parameterization but it only attains comparable performance with the baseline on most tasks . On the other hand LISA which embeds syntax in a head leads to modest but consistent gains across all tasks proving that it is also useful for NMT . Finally PASCAL outperforms all other methods with consistent gains over the Transformer baseline independently of the source language and corpus size: It gains up to BLEU points on most tasks and a substantial in RIBES Isozaki et 2010 a metric with stronger correlation with human judgments than BLEU in translations . On WMT17 our slim model compares favorably to other methods achieving the highest BLEU score across all Overall our model achieves substantial gains given the grammatically rigorous structure of English and German . Not only do we expect performance gains to further increase on less rigorous sources and with better parses Zhang et 2019 but also higher robustness to noisier syntax trees obtained from with parent ignoring . Performance by sentence length As shown in Figure 2 our model is particularly useful when translating long sentences obtaining more than BLEU points when translating long sentences in all experiments and BLEU points on the distant pair . However only a few sentences 1 in the evaluation datasets are long . 3Note that modest improvements in this task should not be surprising as Transformers learn better syntactic relationships from larger data sets Raganato and Tiedemann 2018 . 1622 SRC In a cooling experiment only a tendency agreed BASE 冷却 実験 で は な 傾向 が 一致 し た OURS 冷却 実験 で は 傾向 のみ 一致 し た SRC Of course I don t hate you BASE Naturlich hass te ich dich nicht OURS Naturlich hass e ich dich nicht SRC What are those people fighting for? BASE Was sind die Menschen fur die k ampfen? OURS Wofur k ampfen diese Menschen? Table 2: Example of correct translation by PASCAL . Qualitative performance Table 2 presents examples where our model correctly translated the source sentence while the Transformer baseline made a syntactic error . For instance in the first example the Transformer misinterprets the adverb only as an adjective of tendency: the word only is an adverb modifying the verb In the second example don t is incorrectly translated to the past tense instead of present . PASCAL layer When we introduced our model we motivated our design choice of placing PASCAL heads in the first layer in order to enrich the representations of words from their isolated embeddings by introducing contextualization from their parents . We ran an ablation study on the NC11 data in order to verify our hypothesis . As shown in Table 3a the performance of our model on the validation sets is lower when placing Pascal heads in upper layers a trend that we also observed with the LISA mechanism . These results corroborate the findings of Raganato and Tiedemann 2018 who noticed that in the first layer more attention heads solely focus on the word to be translated itself rather than its context . We can then deduce that enforcing syntactic dependencies in the first layer effectively leads to better word representations which further enhance the translation accuracy of the Transformer model . Investigating the performance of multiple layers is left as future work . Gaussian variance Another design choice we made was the variance of the Gaussian weighing function . We set it to 1 in our experiments motivated by the statistics of our datasets where the vast majority of words is at most split into a few tokens after applying BPE . Table 3b corroborates our choice showing higher BLEU scores on the NC11 validation sets when the variance equals 1 . Here is the case where weights are only placed to the middle token . the parent . Layer 1 2 3 4 5 6 a Variance 1 4 9 16 25 b Table 3: Validation BLEU as a function of PASCAL layer a and Gaussian s variance b on NC11 data . Sensitivity to hyperparameters Due to the large computational cost required to train Transformer models we only searched hyperparameters in a small grid . In order to estimate the sensitivity of the proposed approach to hyperparameters we trained the NC11 model with the hyperparameters of the one . In fact despite being trained on the same data set we find that more PASCAL heads help when German which has a higher syntactic complexity than English is used as the source language . In this test we only find BLEU points with respect to the score listed in Table 1 showing that our general approach is effective regardless of extensive . Additional analyses are reported in Appendix B . 4 Conclusion This study provides a thorough investigation of approaches to induce syntactic knowledge into networks . Through extensive evaluations on various translation tasks we find that approaches effective for RNNs do not necessarily transfer to Transformers . H . Conversely mechanisms LISA and PASCAL best embed syntax for all corpus sizes with PASCAL consistently outperforming other all approaches . Our results show that exploiting core components of the Transformer to embed linguistic knowledge leads to higher and consistent gains than previous approaches . Acknowledgments We are grateful to the anonymous reviewers Desmond Elliott and the CoAStaL NLP group for their constructive feedback . The research results have been achieved by Research and Development of Deep Learning Technology for Advanced Multilingual Speech Translation the Commissioned Research of National Institute of Information and Communications Technology NICT Japan . 1623