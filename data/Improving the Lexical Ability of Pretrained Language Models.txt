Abstract Successful methods for unsupervised neural machine translation UNMT employ crosslingual pretraining via often in the form of a masked language modeling or a sequence generation task which requires the model to align the and representations of the two languages . While pretraining works for similar languages with abundant corpora it performs poorly in and distant languages . Previous research has shown that this is because the representations are not sufficiently aligned . In this paper we enhance the bilingual masked language model pretraining with information by using subword embeddings . Empirical results demonstrate improved performance both on UNMT up to BLEU and bilingual lexicon induction using our method compared to a UNMT baseline . 1 Introduction UNMT is an effective approach for translation without parallel data . Early approaches transfer information from static pretrained embeddings to the model to provide an implicit bilingual signal Lample et 2018a Artetxe et 2018c . Lample and Conneau 2019 suggest to instead pretrain a bilingual language model XLM and use it to initialize UNMT as it can successfully encode text representations . This approach largely improves translation scores for language pairs with plentiful monolingual data . However while UNMT is effective for languages it yields poor results when one of the two languages is Guzmán et 2019 . Marchisio et al . 2020 show that there is a strong correlation between bilingual lexicon induction BLI and final translation performance when using pretrained embeddings converted to as initialization of a UNMT model Artetxe et 2019 . Vulic et al . 2020 observe that static embeddings achieve higher BLI scores compared to multilingual language models LMs meaning that they obtain a better alignment . Since bilingual LM pretraining is an effective form of initializing a UNMT model improving the overall representation of the masked language model MLM is essential to obtaining a higher translation performance . In this paper we propose a new method to enhance the embedding alignment of a bilingual language model entitled lexically aligned MLM that serves as initialization for UNMT . Specifically we learn embeddings separately for the two languages of interest . We map these monolingual embeddings to a common space and use them to initialize the embedding layer of an MLM . Then we train the MLM on both languages . Finally we transfer the trained model to the encoder and decoder of an NMT system . We train the NMT system in an unsupervised way . We outperform a UNMT baseline and demonstrate the importance of mapping of representations . We also conduct an analysis to investigate the correlation between BLI precision and translation results . We finally investigate whether embeddings should be updated or not during the MLM training process in order to preserve lexicallevel information useful for UNMT . We make the code used for this paper publicly available1 . 2 Proposed Approach Our approach has three distinct steps which are described in the following subsections . VecMap Embeddings Initially we split the monolingual data from both languages using BPE tokenization Sennrich et 1https : 174 en en en en en en en en Language embeddings Position embeddings Token embeddings Transformer cat the cat мачка подлога mat sat седеше dog куче carpet килим stand стојат unsupervised embeddings 0 1 2 3 4 5 6 7 s the MASK sat on MASK mat Figure 1: Lexically aligned masked language model . 2016b . We build subword monolingual embeddings with fastText Bojanowski et 2017 . Then we map the monolingual embeddings of the two languages to a shared space using VecMap Artetxe et 2018a with identical tokens occurring in both languages serving as the initial seed dictionary as we do not have any bilingual signal . This is different from the original VecMap approach which operates at the word level . We use the mapped embeddings of the two languages to initialize the embedding layer of a encoder Vaswani et 2017 . Masked Language Model Training We initialize the token embedding layer of the MLM Transformer encoder with pretrained VecMap embeddings which provide an informative mapping lexical representations . We train the model on data from both languages using masked language modeling . Training a masked language model enhances the signal by encoding contextual representations . This step is illustrated in Figure 1 . Unsupervised NMT Finally we transfer the encoder Transformer to an translation model . We note that the attention of the Transformer is randomly initialized . We then train the model for NMT in an unsupervised way using denoising Vincent et 2008 and Sennrich et 2016a which is performed in an online manner . This follows work by Artetxe et al . 2018b Lample et al . 2018a c . 3 Experiments Datasets . We conduct experiments on EnglishMacedonian and EnSq as Mk Sq are languages where alignment can be most beneficial . We use 3K randomly sampled sentences of SETIMES Tiedemann 2012 as sets . We also use 68M En sentences from NewsCrawl . For Sq and Mk we use all the CommonCrawl corpora from Ortiz Suárez et al . 2019 which are 4M Sq and Mk sentences . Baseline . We use a method that relies on crosslingual language model pretraining namely XLM Lample and Conneau 2019 . This approach trains a bilingual MLM separately for and which is used to initialize the of the corresponding NMT system . Each system is then trained in an unsupervised way . Comparison to . We apply our proposed approach to Chronopoulou et 2020 a approach for UNMT . This method trains a monolingual En MLM model monolingual pretraining step . Upon convergence a vocabulary extension method is used that randomly initializes the newly added vocabulary items . Then the MLM is to the two languages MLM step and used to initialize an model . This method outperforms XLM on scenarios . Lexically aligned language models . When applied to the baseline our method initializes the embedding layer of XLM with unsupervised crosslingual embeddings . Then we train XLM on the two languages of interest with a masked language modeling objective . Upon convergence we transfer it to the encoder and decoder of an NMT model which is trained in an unsupervised way . In the case of our method is applied to the MLM step . Instead of randomly initializing the new embedding vectors added in this step we use pretrained unsupervised embeddings . We obtain them by applying VecMap to fastText pretrained embeddings and the English MLM embeddings . Then the MLM is on both languages . Finally it is used to initialize an encoderdecoder NMT model . 175 BLEU CHRF1 BLEU CHRF1 BLEU CHRF1 BLEU CHRF1 XLM lexically aligned XLM lexically aligned Table 1: UNMT results for translations to and from English . The first column indicates the pretraining method used . The scores presented are significantly different p from the respective baseline . CHRF1 refers to character F1 score 2015 . The models in italics are ours . Unsupervised VecMap bilingual embeddings . We build monolingual embeddings with the fastText model with 1024 dimensions using our Sennrich et 2016b monolingual corpora . We map them to a shared space using VecMap with identical tokens . We concatenate the aligned embeddings of the two languages and use them to initialize the embedding layer of XLM or the new vocabulary items of . Preprocessing . We tokenize the monolingual data and sets using Moses Koehn et 2006 . For XLM Lample and Conneau 2019 we use BPE splitting with 32K operations jointly learned on both languages . For Chronopoulou et 2020 we learn 32K BPEs on En for pretraining and then 32K BPEs on both languages for the and UNMT steps . The BPE merges are learned on a subset of the En corpus and the full Sq or Mk corpus . Model hyperparameters . We use a Transformer architecture for both the baselines and UNMT models using the same hyperparameters as XLM . For the encoder Transformer used for masked language modeling the embedding and model size is 1024 and the number of attention heads is 8 . The encoder Transformer has 6 layers while the NMT model is a Transformer . The learning rate is set to for XLM and UNMT . We train the models on 8 NVIDIA GTX 11 GB GPUs . To be comparable with we retrain it on 8 GPUs as that work reports UNMT results with only 1 GPU . The batch size is 32 during XLM and 26 during UNMT . Our models are built on the publicly available XLM and codebases . We generate final translations with beam search of size 5 and we evaluate with SacreBLEU2 Post 2018 . 4 Results Table 1 shows the results of our approach compared to two pretraining approaches that rely on 2 Signature MLM training namely XLM and . The lexically aligned XLM improves translation results over the baseline XLM model . We obtain substantial improvements on in both directions of at most BLEU and CHRF1 while on we get an even larger performance boost of up to points in terms of BLEU and in terms of CHRF1 . Our lexically aligned also consistently outperforms most notably in the direction by up to BLEU . At the same time CHRF1 score improves by up to points using the lexically aligned pretraining approach compared to . In the case of XLM the effect of lexical alignment is more evident for as Mk is less similar to En compared to Sq . This is mainly the case because the two languages use a different alphabet Latin for En and Cyrillic for Mk . This is also true for when translating out of En showing that enhancing the step of MLM with pretrained embeddings is helpful and improves the final UNMT performance . In general our method provides better alignment of the representations of the MLM thanks to the transferred VecMap embeddings . We hypothesize that static embeddings enhance the knowledge that a masked language model obtains during training . As a result using them to bootstrap the pretraining procedure improves the ability of the model to map the distributions of the two languages and yields higher translation scores . Overall our approach consistently outperforms two pretraining models for UNMT providing for the highest BLEU and CHRF1 scores on all translation directions . 5 Analysis We conduct an analysis to assess the contribution of alignment in the MLM training . We present Bilingual Lexicon Induction BLI and 176 NN CSLS NN CSLS XLM lexically aligned XLM lexically aligned Table 2: P 5 results for the BLI task on the MUSE Lample et 2018b dictionaries . We evaluate the alignment of the embedding layer of each trained MLM . BLEU precision scores . We also investigate the best method to leverage pretrained embeddings during MLM training in terms of final UNMT performance . Bilingual Lexicon Induction BLI . We use BLI a standard way of evaluating lexical quality of embedding representations Gouws et 2015 Ruder et 2019 to explore the effect of the alignment of our method . We compare the BLI score of different pretrained language models . We report precision 5 P 5 using nearest neighbors NN and semantic similarity CSLS . The results are presented in Table 2 . We use the embedding layer of each MLM for this task . We also experimented with averages over different layers but noticed the same trend in terms of BLI scores . We obtain representations by averaging over the corresponding subword embeddings . It is worth noting that we compute the representation of each vocabulary word in isolation similar to Vulic et al . 2020 . In Table 2 we observe that lexical alignment is more beneficial for . This can be explained by the limited vocabulary overlap of the two languages which does not provide sufficient crosslingual signal for the training of MLM . By contrast initializing an MLM with pretrained embeddings largely improves performance even for a higherperforming model such as . In the effect of our approach is smaller yet consistent . This can be attributed to the fact that the two languages use the same script . Overall our method enhances the information captured by pretrained MLMs as shown empirically . This is consistent with our intuition that embeddings capture a bilingual signal that can benefit MLM representations . precision scores . To examine whether the improved translation performance is a result of the information provided by static embeddings we present precision scores in XLM lexically aligned XLM lexically aligned Table 3: BLEU precision scores . ble 3 as they can be directly attributed to lexical alignment . The biggest performance gains up to are obtained when the proposed approach is applied to XLM . This correlates with the BLEU scores of Table 1 . Moreover the language pair benefits more than from the lexicallevel alignment both in terms of precision and BLEU . These results show that the improved BLEU scores can be attributed to the enhanced lexical representations . Alignment Method lexically aligned MLM frozen embeddings embeddings ours Table 4: BLEU scores using different initializations of the XLM embedding layer . XLM is then trained on the respective language pair and used to initialize a UNMT system . Both embeddings are aligned using VecMap . How should static embeddings be integrated in the MLM training? We explore different ways of incorporating the lexical knowledge of pretrained embeddings to the second masked language modeling stage of our approach . Specifically we keep the aligned embeddings fixed frozen during XLM training and compare the performance of the final UNMT model to the proposed method . We point out that after we transfer the trained MLM to an model all layers are trained for UNMT . Table 4 summarizes our results . The finetuning approach which is adopted in our proposed method provides a higher performance both in EnMk and with the improvement being more evident in . Our findings generally show that it is preferable to train the bilingual embeddings together with the rest of the model in the MLM step . 6 Related Work Artetxe et al . 2018c Lample et al . 2018a initialize UNMT models with 177 tions based on a bilingual lexicon inducted in an unsupervised way by the same monolingual data or simply with embeddings . Lample et al . 2018c also use pretrained embeddings learned on joint monolingual corpora of the two languages of interest to initialize the embedding layer of the . Lample and Conneau 2019 remove pretrained embeddings from the UNMT pipeline and align language distributions by simply pretraining a MLM on both languages in order to learn a mapping . However it has been shown that this pretraining method provides a weak alignment of the language distributions Ren et 2019 . While that work identified as a cause the lack of sharing level information we address the lack of information at the lexical level . Moreover most prior work on UNMT focuses on languages with abundant monolingual corpora . In scenarios though especially when the languages are not related pretraining a MLM for unsupervised NMT does not yield good results Guzmán et 2019 Chronopoulou et 2020 . We propose a method that overcomes this issue by enhancing the MLM with representations . Another line of work tries to enrich the representations of multilingual MLMs with additional knowledge Wang et 2020 Pfeiffer et 2020 without harming the representations . In our work we identify lexical information as a source of knowledge that is missing from MLMs especially when it comes to languages . Surprisingly static embeddings such as fastText largely outperform representations extracted by multilingual MLMs in terms of crosslingual lexical alignment Vulic et al . 2020 . Motivated by this we aim to narrow the gap between the lexical representations of bilingual MLMs and static embeddings in order to achieve a higher translation quality when transferring the MLM to an UNMT model . 7 Conclusion We propose a method to improve the lexical ability of a Transformer encoder by initializing its embedding layer with pretrained embeddings . The Transformer is trained for masked language modeling on the language pair of interest . After that it is used to initialize an model which is trained for UNMT and outperforms relevant baselines . Results confirm our intuition that masked language modeling which provides contextual representations benefits from embeddings which capture lexicallevel information . In the future we would like to investigate whether lexical knowledge can be infused to multilingual MLMs . We would also like to experiment with other schemes of training the MLM in terms of how the embedding layer is updated such as regularizer annealing strategies which would enable keeping the embeddings relatively fixed but still allow for some limited training . 8 Ethical Considerations In this work we propose a novel unsupervised neural machine translation approach which is tailored to languages in terms of monolingual data . We experiment with unsupervised translation between English Albanian and Macedonian . For English we use data from news articles . The Albanian and Macedonian monolingual data originates from the OSCAR project Ortiz Suárez et 2019 . The corpora are shuffled and stripped of all metadata . Therefore the data should not be easily attributable to specific individuals . Nevertheless the project offers easy ways to remove data upon request . The and EnMk parallel development and test data are obtained from OPUS Tiedemann 2012 and consist of highquality news articles . Our work is partly based on training embeddings which are not computationally expensive . However training masked language models requires significant computational resources . To lower environmental impact we do not conduct search and use wellestablished values for all . Acknowledgments This project has received funding from the European Research Council under the European Union s Horizon 2020 research and innovation program grant agreement 640550 . This work was also supported by DFG grant FR . We thank Katerina Margatina Giorgos Vernikos and Viktor Hangya for their thoughtful and valuable feedback . 178