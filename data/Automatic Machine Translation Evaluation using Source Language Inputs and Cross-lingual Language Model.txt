Abstract We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references . The proposed method evaluates a translation hypothesis in a regression model . The model takes the paired source reference and hypothesis sentence all together as an input . A pretrained large scale language model encodes the input to vectors and the model predicts a human evaluation score with those vectors . Our experiments show that our proposed method using Crosslingual Language Model XLM trained with a translation language modeling TLM objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences . Additionally using source sentences in our proposed method is confirmed to improve the evaluation performance . 1 Introduction Automatic machine translation evaluation MTE has been studied to substitute human evaluation in machine translation development because it is handy and stable to use . Popular automatic MTE metrics such as BLEU Papineni et 2002 calculate the evaluation score based on a similarity of a paired reference and translated hypothesis sentences . BLEU particularly evaluates the sentence similarity with the ngram word matching rate between a reference and hypothesis . However the evaluation score drops when a reference and hypothesis are dissimilar in the surface even if they share the same meaning . To counter this problem METEOR Banerjee and Lavie 2005 is proposed to mitigate the word matching of synonyms with a synonym dictionary . Yet still with mitigation of word matching surfacelevel similarity cannot fully compensate for semantics thus word representation instead of word symbols is used in Word Mover s Distance Kusner et 2015 and bleu2vec Tattar and Fishel 2017 . Besides sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings . RUSE Shimanaka et 2018 and BERT Devlin et 2019 based MTE BERT regressor Shimanaka et 2019 utilized sentence representation and performed well on WMT17 Metric Shared Task Bojar et 2017 . The metrics mentioned above compare a hypothesis translation to a reference . However a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors . This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu 2012 and Qin and Specia 2015 but preparing such multiple references is costly . Hereby we propose a method to incorporate source sentence into MTE as another pseudo reference since the source and reference sentences should be semantically equivalent . The proposed method uses Language Model XLM Lample and Conneau 2019 to handle source and target languages in a shared sentence embedding space . The proposed method with XLM trained with a translation language modeling TLM objective showed a higher correlation with human judgments than a baseline method using hypothesis and reference sentences . 2 Related Work Recent advances in embedding have been used in MTE . Shimanaka et al . 2018 proposed an MTE framework called RUSE Regressor Using Sentence Embeddings which uses 3554 Table 1: Available corpus size annotated with human judgments in Metrics Shared Task de ru tr zh 500 500 500 500 1000 2000 560 560 560 560 560 560 1680 3360 560 560 560 560 560 560 560 2240 3920 ALL 1620 1620 1620 560 560 1620 1120 560 4920 9280 level embeddings obtained by a pretrained model like InferSent Conneau et 2017 Quick Thought Logeswaran and Lee 2018 and Universal Sentence Encoder Cer et 2018 . Its regressor takes sentence vectors for a reference and translation hypothesis as inputs and returns a score which is trained to correlate well with human evaluation Graham et 2015 . RUSE achieved the best correlation score with human judgments in the Metrics Shared Task Bojar et 2017 . BERT regressor Shimanaka et 2019 is a simple MTE metric based on BERT Devlin et 2019 encoder . It is composed of BERT encoder and a perceptron MLP regressor attached to the last layer of BERT . This BERT encoder is a 12 layers language model referring to BERTbase uncased 1 trained with masked language model MLM and next sentence prediction NSP . BERT regressor surpassed RUSE on the data . 3 Proposed method: Automatic evaluation using XLM We propose an MTE method using source language sentences as additional pseudo references . We use language models called XLM Lample and Conneau 2019 to encode both source and target language sentences into an embedding vector . XLM has three additional techniques to BERT: language independent subword based on Byte Pair Encoding Sennrich et 2016 a language embedding layer and a translation language modeling TLM objective that predicts masked words from surrounding words or a paired translation . The brief architecture of XLM is shown in Figure 1 . Lample and Conneau 2019 reported that XLM trained with TLM objective obtains better performance than multilingual BERT Devlin et 2019 on the XNLI classification task Conneau et 2018 . 1https: bert The proposed method has two variants for the use of source language sentences as illustrated in Figure 2 . The first one called uses two vectors for hypothesissource and encoded by a language model independently . These vectors are given to an regression model to predict the human evaluation scores . This can be regarded as an ensemble model using a monolingual vector based on the reference and a vector based on the source sentence . The other one called takes a concatenation of hypotheses source and reference sentences as an input to a language model to obtain a vector . This vector is expected to be directly learned to represent the quality of the translation hypothesis given two correct sentences aligned aside . 4 Experiments We conducted experiments to evaluate the performance of the proposed method in MTE by comparing with some existing methods . Setting The experiments were conducted with a corpus of all language pairs to English translation from WMT2017 Metrics Shared Task Bojar et 2017 . We split sentences in WMT15 and WMT16 to training and development data with the ratio of and whole sentences in WMT17 are used for evaluation of MTE methods . The corpus size for each language pair is shown in Table 1 . We used two different models from all available XLM family models2: XLM15 pretrained by MLM and TLM and XLM100 pretrained only by MLM . XLM15 is expected to perform better by the paired bilingual training of TLM but the number of available languages is limited . XLM15 is compatible with only German Russian Turkish and Chinese in the corpus which confines the model to partial access to the corpus . On the other hand XLM100 2https: XLM 3555 n layer transformer encoder vector token1 token1 n token2 token2 m V V1 V1 n V V2 V2 m V positional embedding input tokens hidden vectors L1 L 1 L 1 L 1 L 2 L 2 L 2 L2 L1 language embedding POS0 POS1 POS POSn POS L2 V Figure 1: The architecture of XLM encoder MLP evaluation score encoder hypothesis source concatenation hypothesis reference vector vector a MLP evaluation score encoder hypothesis source reference vector b Figure 2: Two variants in the proposed method is compatible with all language pairs in the corpus while it lacks supervised bilingual pretraining . Thus the experiments had two corpus settings One was a small corpus including German de Russian ru Turkish tr and Chinese zh to English en language pairs and the other was a whole corpus including Czech cz German de Finnish fi Latvian lv Romanian ro Russian ru Turkish tr and Chinese zh to English language pairs . The evaluation was conducted with Pearson s correlation to human judgments in the test set . We compared the proposed methods with SentBLEU Bojar et 2017 BERT regressor Shimanaka et 2019 by our implementation . We also conducted experiments using multilingual BERT BERTmulti cased to contrast language models and experiments limiting the model s input into only and referencehypothesis only to study the impact of adding source sentences . The on the proposed methods and BERT regressor was based on Mean Squared Error Table 2: Pearson s correlation scores in the small corpus de ru tr zh avg SentBLEU BERT regressor XLM15 XLM100 MSE loss in the training set to both MLP and XLM in order . The hyperparameters were selected through grid search for the following parameters . Since models are affected by randomness in training we ran ten experiments for each of the settings and report results of the average scores . Optimizer: Adam Learning rate: Number of epochs : 1 Dropout rate: Batch size : 2 4 8 16 Results The results of each small corpus and whole corpus experiments are shown in Tables 2 and 3 respectively . Note that XLM15 was not included in the 3556 Table 3: Pearson s correlation scores in the whole corpus avg SentBLEU BERT regressor XLM100 whole corpus experiment due to its limited language coverage . Performance of each language model As we can see from Table 2 the proposed method using XLM15 with structure surpassed BERT regressor in the small corpus . However XLM100 did not work well in the experiments its results were much worse than the others in the small corpus condition and it did not compete with BERT regressor in the whole corpus condition as shown in Table 3 . One possible reason is the lack of TLM objective pretraining in XLM100 . Since the TLM task allows the model for learning semantically equivalent sentences directly the TLM task can be concluded to be important for using source sentences in MTE . The results of multilingual BERT are worse than BERT regressor and XLM15 but close to XLM100 or slightly better in general . From this comparison of pretraining objectives and language models we report that our proposed method is influenced by the multilingualism of a language model . VS The results from Table 2 and Table 3 shows that structure is better than in most of the conditions although we expected to perform better because it can access 2 translation answers as references at the same time . This is probably because both of XLM and multilingual BERT was not pretrained to handle 3 sentences in a sequence . However it is perhaps possible that surpasses when a corpus is large enough . Table 4: Pearson s correlation score in the halved small corpus de ru tr zh avg BERT regressor XLM15 XLM100 Contribution of adding source sentences Every model with achieved a better score than both of and which indicates that source sentences contribute to the improvement of evaluation . 5 Analysis Training data size We conducted another experiment to see the effect of the training corpus size using randomly halved de ru tr zh small corpus . From the results in Table 4 BERT regressor stably performed well even when the number of training data is about 1000 sentences however XLM15 XLM100 and multilingual BERT deteriorated their performances . Since our proposed is an ensemble model and has a more complex network structure than the 3557 Table 5: Pearson s correlation score for low and high human judgement score range in the small corpus de ru tr zh All DA DA Reduction rate of Pearson s score from DA to DA BERT regressor XLM15 XLM15 XLM15 XLM15 XLM100 XLM100 XLM100 XLM100 use of XLM and with the proposed method requires a certain amount of training data . Therefore our proposed method deteriorated in the halved small corpus setting . On the other hand monolingual BERT and benefits from the large corpus because it has no language limitation other than English . Evaluation errors In order to see when models make errors to evaluate hypothesis sentences we plot scatters of evaluation scores and human judgement scores DA scores in Figure 3 a Figure 3 b Figure 3 c and Figure 3 d . Although in comparison the evaluation scores of our best model XLM15 are set more linearly than the baseline BERT regressor the scores of all models seem much dispersed in the low DA area DA . This indicates that all evaluation models listed here tend to when a hypothesis is poor . Furthermore we show Pearson s correlation score for each of high and low DA score range in Table 5 . As we confirmed in the scatter figures the correlation scores of low DA is low evaluation models work poorly when hypotheses are poor . However the reduction rate of Pearson s scores from high DA to low DA is small with XLM15 and . Therefore adding source sentences has an impact to stabilize the evaluation performance when hypotheses are . 6 Conclusion In this paper we proposed an MTE framework that utilizes source sentences using XLM . We show DA score evaluation score a BERT regressor r DA score evaluation score b XLM15 r DA score evaluation score c XLM15 r DA score evaluation score d XLM15 r Figure 3: Scatter plots of human judgement scores DA scores and evaluation scores that the proposed method with XLM showed a higher correlation with human judgments than the baseline method in the small corpus condition and stabilize the evaluation performance regardless of the quality of translation sentences by using additional source sentences . We also investigated why our proposed method worked poorly in the other conditions and found the importance of TLM training . In future work we will work around the problem of evaluation errors in the low DA range . Acknowledgments This work is supported by JST PRESTO JPMJPR1856 . 3558