Abstract An attentional mechanism has lately been used to improve neural machine translation NMT by selectively focusing on parts of the source sentence during translation . However there has been little work exploring useful architectures for NMT . This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time . We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions . With local attention we achieve a significant gain of BLEU points over systems that already incorporate known techniques such as dropout . Our ensemble model using different attention architectures yields a new result in the WMT 15 English to German translation task with BLEU points an improvement of BLEU points over the existing best system backed by NMT and an 1 Introduction Neural Machine Translation NMT achieved performances in translation tasks such as from English to French Luong et 2015 and English to German Jean et 2015 . NMT is appealing since it requires minimal domain knowledge and is conceptually simple . The model by Luong et al . 2015 reads through all the source words until the symbol eos is reached . It then starts emitting one target word at a time as illustrated in Figure 1 . NMT 1All our code and models are publicly available at http: . B C D eos X Y Z X Y Z eos A Figure 1: Neural machine translation a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z . Here eos marks the end of a sentence . is often a large neural network that is trained in an fashion and has the ability to generalize well to very long word sequences . This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT hence NMT has a small memory footprint . Lastly implementing NMT decoders is easy unlike the highly intricate decoders in standard MT Koehn et 2003 . In parallel the concept of attention has gained popularity recently in training neural networks allowing models to learn alignments between different modalities between image objects and agent actions in the dynamic control problem Mnih et 2014 between speech frames and text in the speech recognition task Chorowski et 2014 or between visual features of a picture and its text description in the image caption generation task Xu et 2015 . In the context of NMT Bahdanau et al . 2015 has successfully applied such attentional mechanism to jointly translate and align words . To the best of our knowledge there has not been any other work exploring the use of architectures for NMT . In this work we design with simplicity and effectiveness in mind two novel types of attention1412 based models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time . The former approach resembles the model of Bahdanau et 2015 but is simpler architecturally . The latter can be viewed as an interesting blend between the hard and soft attention models proposed in Xu et 2015: it is computationally less expensive than the global model or the soft attention at the same time unlike the hard attention the local attention is differentiable making it easier to implement and Besides we also examine various alignment functions for our models . Experimentally we demonstrate that both of our approaches are effective in the WMT translation tasks between English and German in both directions . Our attentional models yield a boost of up to BLEU over systems which already incorporate known techniques such as dropout . For English to German translation we achieve new SOTA results for both WMT 14 and WMT 15 outperforming previous SOTA systems backed by NMT models and LM rerankers by more than BLEU . We conduct extensive analysis to evaluate our models in terms of learning the ability to handle long sentences choices of attentional architectures alignment quality and translation outputs . 2 Neural Machine Translation A neural machine translation system is a neural network that directly models the conditional probability p of translating a source sentence x1 . . . xn to a target sentence y1 . . . ym . 3 A basic form of NMT consists of two components: a an encoder which computes a representation s for each source sentence and b a decoder which generates one target word at a time and hence decomposes the conditional probability as: log p Xm log p yj j s 1 A natural choice to model such a decomposition in the decoder is to use a recurrent neural network RNN architecture which most of the re2There is a recent work by Gregor et al . 2015 which is very similar to our local attention and applied to the image generation task . However as we detail later our model is much simpler and can achieve good performance for NMT . 3All sentences are assumed to terminate with a special token eos . cent NMT work such as Kalchbrenner and Blunsom 2013 Sutskever et 2014 Cho et 2014 Bahdanau et 2015 Luong et 2015 Jean et 2015 have in common . They however differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation Kalchbrenner and Blunsom 2013 used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation . On the other hand both Sutskever et al . 2014 and Luong et al . 2015 stacked multiple layers of an RNN with a Long Memory LSTM hidden unit for both the encoder and the decoder . Cho et al . 2014 Bahdanau et al . 2015 and Jean et al . 2015 all adopted a different version of the RNN with an hidden unit the gated recurrent unit GRU for both In more detail one can parameterize the probability of decoding each word yj as: p yj j s softmax g hj 2 with g being the transformation function that outputs a Here hj is the RNN hidden unit abstractly computed as: hj f s 3 where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit a GRU or an LSTM unit . In Kalchbrenner and Blunsom 2013 Sutskever et 2014 Cho et 2014 Luong et 2015 the source representation s is only used once to initialize the decoder hidden state . On the other hand in Bahdanau et 2015 Jean et 2015 and this work s in fact implies a set of source hidden states which are consulted throughout the entire course of the translation process . Such an approach is referred to as an attention mechanism which we will discuss next . In this work following Sutskever et 2014 Luong et 2015 we use the stacking LSTM architecture for our NMT systems as illustrated in Figure 1 . We use the LSTM unit defined in Zaremba et 2015 . Our training objective is formulated as follows: Jt X x y log p 4 4They all used a single RNN layer except for the latter two works which utilized a bidirectional RNN for the encoder . 5One can provide g with other inputs such as the currently predicted word yj as in Bahdanau et 2015 . 1413 with D being our parallel training corpus . 3 Models Our various models are classifed into two broad categories global and local . These classes differ in terms of whether the attention is placed on all source positions or on only a few source positions . We illustrate these two model types in Figure 2 and 3 respectively . Common to these two types of models is the fact that at each time step t in the decoding phase both approaches first take as input the hidden state ht at the top layer of a stacking LSTM . The goal is then to derive a context vector ct that captures relevant information to help predict the current target word yt . While these models differ in how the context vector ct is derived they share the same subsequent steps . Specifically given the target hidden state ht and the context vector ct we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows: tanh Wc ct ht 5 The attentional vector is then fed through the softmax layer to produce the predictive distribution formulated as: p yt t x softmax 6 We now detail how each model type computes the context vector ct . Global Attention The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector ct . In this model type a alignment vector at whose size equals the number of time steps on the source side is derived by comparing the current target hidden state ht with each source hidden state: at s align ht 7 exp score ht P s exp score ht Here score is referred as a function for which we consider three different alternatives: score ht h t dot h t general Wa ht concat 8 yt ct at ht Global align weights Attention Layer Context vector Figure 2: Global attentional model at each time step t the model infers a alignment weight vector at based on the current target state ht and all source states . A global context vector ct is then computed as the weighted average according to at over all the source states . Besides in our early attempts to build attentionbased models we use a function in which the alignment scores are computed from solely the target hidden state ht as follows: at softmax Waht location 9 Given the alignment vector as weights the context vector ct is computed as the weighted average over all the source hidden Comparison to Bahdanau et 2015 While our global attention approach is similar in spirit to the model proposed by Bahdanau et al . 2015 there are several key differences which reflect how we have both simplified and generalized from the original model . First we simply use hidden states at the top LSTM layers in both the encoder and decoder as illustrated in Figure 2 . Bahdanau et al . 2015 on the other hand use the concatenation of the forward and backward source hidden states in the encoder and target hidden states in their decoder . Second our computation path is simpler we go from ht at ct then make a prediction as detailed in Eq . 5 Eq . 6 and Figure 2 . On the other hand at any time t Bahdanau et al . 2015 build from the previous hidden state at ct ht which in turn 6Eq . 9 implies that all alignment vectors at are of the same length . For short sentences we only use the top part of at and for long sentences we ignore words near the end . 14 yt ct at ht pt Attention Layer Context vector Local weights Aligned position Figure 3: Local attention model the model first predicts a single aligned position pt for the current target word . A window centered around the source position pt is then used to compute a context vector ct a weighted average of the source hidden states in the window . The weights at are inferred from the current target state ht and those source states in the window . goes through a and a maxout layer before making Lastly Bahdanau et al . 2015 only experimented with one alignment function the concat product whereas we show later that the other alternatives are better . Local Attention The global attention has a drawback that it has to attend to all words on the source side for each target word which is expensive and can potentially render it impractical to translate longer sequences paragraphs or documents . To address this deficiency we propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word . This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al . 2015 to tackle the image caption generation task . In their work soft attention refers to the global attention approach in which weights are placed softly over all patches in the source image . The hard attention on the other hand selects one patch of the image to attend to at a time . While less expensive at inference time the hard attention model is and requires more complicated techniques such as variance reduction or reinforcement learning to train . 7We will refer to this difference again in Section . Our local attention mechanism selectively focuses on a small window of context and is differentiable . This approach has an advantage of avoiding the expensive computation incurred in the soft attention and at the same time is easier to train than the hard attention approach . In concrete details the model first generates an aligned position pt for each target word at time The context vector ct is then derived as a weighted average over the set of source hidden states within the window D is empirically Unlike the global approach the local alignment vector at is now R . We consider two variants of the model as below . Monotonic alignment we simply set pt t assuming that source and target sequences are roughly monotonically aligned . The alignment vector at is defined according to Eq . 7 Predictive alignment instead of assuming monotonic alignments our model predicts an aligned position as follows: pt S sigmoid v p tanh Wpht 10 Wp and vp are the model parameters which will be learned to predict positions . S is the source sentence length . As a result of sigmoid pt 0 S . To favor alignment points near pt we place a Gaussian distribution centered around pt . Specifically our alignment weights are now defined as: at s align ht exp s pt 2 2σ 2 11 We use the same align function as in Eq . 7 and the standard deviation is empirically set as D 2 . It is important to note that pt is a real nummber whereas s is an integer within the window centered at pt . 10 Comparison to Gregor et 2015 have proposed a selective attention mechanism very similar to our local attention for the image generation task . Their approach allows the model to select an image patch of varying location and zoom . We instead use the same zoom for all target positions which greatly simplifies the formulation and still achieves good performance . 8 If the window crosses the sentence boundaries we simply ignore the outside part and consider words in the window . 9 is the same as the global model except that the vector at is and shorter . is similar to the model except that we dynamically compute pt and use a Gaussian distribution to modify the original alignment weights align ht as shown in Eq . 11 . By utilizing pt to derive at we can compute backprop gradients for Wp and vp . 1415 Attention Layer B C D eos X Y Z X Y Z eos A Figure 4: approach Attentional vectors are fed as inputs to the next time steps to inform the model about past alignment decisions . Approach In our proposed global and local approaches the attentional decisions are made independently which is suboptimal . Whereas in standard MT a coverage set is often maintained during the translation process to keep track of which source words have been translated . Likewise in attentional NMTs alignment decisions should be made jointly taking into account past alignment information . To address that we propose an inputfeeding approach in which attentional vectors are concatenated with inputs at the next time steps as illustrated in Figure The effects of having such connections are: a we hope to make the model fully aware of previous alignment choices and b we create a very deep network spanning both horizontally and vertically . Comparison to other work Bahdanau et al . 2015 use context vectors similar to our ct in building subsequent hidden states which can also achieve the coverage effect . However there has not been any analysis of whether such connections are useful as done in this work . Also our approach is more general as illustrated in Figure 4 it can be applied to general stacking recurrent architectures including models . Xu et al . 2015 propose a doubly attentional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption generation process . Such a constraint can 11If n is the number of LSTM cells the input size of the first LSTM layer is 2n those of subsequent layers are also be useful to capture the coverage set effect in NMT that we mentioned earlier . However we chose to use the approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable . 4 Experiments We evaluate the effectiveness of our models on the WMT translation tasks between English and German in both directions . newstest2013 3000 sentences is used as a development set to select our hyperparameters . Translation performances are reported in BLEU Papineni et 2002 on newstest2014 2737 sentences and newstest2015 2169 sentences . Following Luong et 2015 we report translation quality using two types of BLEU: a tokenized12 BLEU to be comparable with existing NMT work and b NIST13 BLEU to be comparable with WMT results . Training Details All our models are trained on the WMT 14 training data consisting of sentences pairs 116M English words 110M German words . Similar to Jean et 2015 we limit our vocabularies to be the top 50K most frequent words for both languages . Words not in these shortlisted vocabularies are converted into a universal token unk . When training our NMT systems following Bahdanau et 2015 Jean et 2015 we filter out sentence pairs whose lengths exceed 50 words and shuffle as we proceed . Our stacking LSTM models have 4 layers each with 1000 cells and embeddings . We follow Sutskever et 2014 Luong et 2015 in training NMT with similar settings: a our parameters are uniformly initialized in b we train for 10 epochs using plain SGD c a simple learning rate schedule is employed we start with a learning rate of 1 after 5 epochs we begin to halve the learning rate every epoch d our size is 128 and e the normalized gradient is rescaled whenever its norm exceeds 5 . Additionally we also use dropout for our LSTMs as suggested by Zaremba et 2015 . For dropout models we train for 12 epochs and start halving the learning rate after 8 epochs . Our code is implemented in MATLAB . When 12All texts are tokenized with and BLEU scores are computed with . 13With the script as per WMT guideline . 1416 System Ppl BLEU Winning WMT 14 system large LM Buck et 2014 Existing NMT systems RNNsearch Jean et 2015 RNNsearch unk replace Jean et 2015 RNNsearch unk replace large vocab ensemble 8 models Jean et 2015 Our NMT systems Base Base reverse Base reverse dropout Base reverse dropout global attention location Base reverse dropout global attention location feed input Base reverse dropout attention general feed input Base reverse dropout attention general feed input unk replace Ensemble 8 models unk replace Table 1: WMT 14 results shown are the perplexities ppl and the tokenized BLEU scores of various systems on newstest2014 . We highlight the best system in bold and give progressive improvements in italic between consecutive systems . referes to the local attention with predictive alignments . We indicate for each attention model the alignment score function used in pararentheses . running on a single GPU device Tesla K40 we achieve a speed of 1K target words per second . It takes days to completely train a model . Results We compare our NMT systems in the EnglishGerman task with various other systems . These include the winning system in WMT 14 Buck et 2014 a system whose language models were trained on a huge monolingual text the Common Crawl corpus . For neural machine translation systems to the best of our knowledge Jean et 2015 is the only work experimenting with this language pair and currently the SOTA system . We only present results for some of our attention models and will later analyze the rest in Section 5 . As shown in Table 1 we achieve progressive improvements when a reversing the source sentence BLEU as proposed in Sutskever et 2014 and b using dropout BLEU . On top of that c the global attention approach gives a significant boost of BLEU making our model slightly better than the base attentional system of Bahdanau et al . 2015 row RNNSearch . When d using the approach we seize another notable gain of BLEU and outperform their system . The local attention model with predictive alignments row proves to be even better giving us a further improvement of BLEU on top of the global attention model . It is interesting to observe the trend previously reported in Luong et 2015 that perplexity strongly correlates with translation quality . In total we achieve a significant gain of BLEU points over the baseline which already includes known techniques such as source reversing and dropout . The unknown replacement technique proposed in Luong et 2015 Jean et 2015 yields another nice gain of BLEU demonstrating that our attentional models do learn useful alignments for unknown works . Finally by ensembling 8 different models of various settings using different attention approaches with and without dropout we were able to achieve a new SOTA result of BLEU outperforming the existing best system Jean et 2015 by BLEU . System BLEU SOTA NMT rerank MILA Our ensemble 8 models unk replace Table 2: WMT 15 results NIST BLEU scores of the existing WMT 15 SOTA system and our best one on newstest2015 . Latest results in WMT 15 despite the fact that our models were trained on WMT 14 with slightly less data we test them on newstest2015 to demonstrate that they can generalize well to different test sets . As shown in Table 2 our best system es1417 System Ppl . BLEU WMT 15 systems SOTA Edinburgh NMT rerank MILA Our NMT systems Base reverse global location global location feed global dot drop feed global dot drop feed unk Table 3: WMT 15 results performances of various systems similar to Table 1 . The base system already includes source reversing on which we add global attention dropout input feeding and unk replacement . tablishes a new SOTA performance of BLEU outperforming the existing best system backed by NMT and a LM reranker by BLEU . Results We carry out a similar set of experiments for the WMT 15 translation task from German to English . While our systems have not yet matched the performance of the SOTA system we nevertheless show the effectiveness of our approaches with large and progressive gains in terms of BLEU as illustrated in Table 3 . The attentional mechanism gives us BLEU gain and on top of that we obtain another boost of up to BLEU from the approach . Using a better alignment function the dot product one together with dropout yields another gain of BLEU . Lastly when applying the unknown word replacement technique we seize an additional BLEU demonstrating the usefulness of attention in aligning rare words . 5 Analysis We conduct extensive analysis to better understand our models in terms of learning the ability to handle long sentences choices of attentional architectures and alignment quality . All models considered here are NMT systems tested on newstest2014 . Learning curves We compare models built on top of one another as listed in Table 1 . It is pleasant to observe in Figure 5 a clear separation between and attentional models . The 1 x 105 2 3 4 5 6 Test cost basic Figure 5: Learning curves test cost ln perplexity on newstest2014 for NMTs as training progresses . proach and the local attention model also demonstrate their abilities in driving the test costs lower . The model with dropout the blue curve learns slower than other models but as time goes by it becomes more robust in terms of minimizing test errors . Effects of Translating Long Sentences We follow Bahdanau et 2015 to group sentences of similar lengths together and compute a BLEU score per group . As demonstrated in Figure 6 our attentional models are more effective than the other model in handling long sentences: the translation quality does not degrade as sentences become longer . Our best model the blue curve outperforms all other systems in all length buckets . 10 20 30 40 50 60 70 10 15 20 25 Sent Lengths BLEU ours no attn BLEU ours attn BLEU ours best system BLEU WMT 14 best BLEU Jeans et 2015 BLEU Figure 6: Length Analysis translation qualities of different systems as sentences become longer . Choices of Attentional Architectures We examine different attention models global and different alignment functions location dot general concat as described in Section 3 . Due to limited resources we cannot run all the possible combinations . However 1418 System Ppl BLEU Before After unk global location global dot global general dot x x general dot general 19 Table 4: Attentional Architectures performances of different attentional models . We trained two dot models both have ppl results in Table 4 do give us some idea about different choices . The function does not learn good alignments: the global location model can only obtain a small gain when performing unknown word replacement compared to using other alignment For functions our implementation of concat does not yield good performances and more analysis should be done to understand the It is interesting to observe that dot works well for the global attention and general is better for the local attention . Among the different models the local attention model with predictive alignments is best both in terms of perplexities and BLEU . Alignment Quality A of attentional models are word alignments . While Bahdanau et 2015 visualized alignments for some sample sentences and observed gains in translation quality as an indication of a working attention model no work has assessed the alignments learned as a whole . In contrast we set out to evaluate the alignment quality using the alignment error rate AER metric . Given the gold alignment data provided by RWTH for 508 Europarl sentences we force decode our attentional models to produce translations that match the references . We extract only alignments by selecting the source word with the highest alignment 14There is a subtle difference in how we retrieve alignments for the different alignment functions . At time step t in which we receive as input and then compute ht at ct and before predicting yt the alignment vector at is used as alignment weights for a the predicted word yt in the alignment functions and b the input word in the functions . 15With concat the perplexities achieved by different models are global and . Method AER global location general general ensemble Berkeley Aligner Table 6: AER scores results of various models on the RWTH alignment data . weight per target word . Nevertheless as shown in Table 6 we were able to achieve AER scores comparable to the alignments obtained by the Berkeley aligner Liang et 2006 We also found that the alignments produced by local attention models achieve lower AERs than those of the global one . The AER obtained by the ensemble while good is not better than the AER suggesting the observation that AER and translation scores are not well correlated Fraser and Marcu 2007 . Due to space constraint we can only show alignment visualizations in the arXiv version of our Sample Translations We show in Table 5 sample translations in both directions . It it appealing to observe the effect of attentional models in correctly translating names such as Miranda Kerr and Roger Dow . models while producing sensible names from a language model perspective lack the direct connections from the source side to make correct translations . We also observed an interesting case in the second example which requires translating the phrase not incompatible . The attentional model correctly produces nicht . . . unvereinbar whereas the model generates nicht vereinbar meaning not compatible The attentional model also demonstrates its superiority in translating long sentences as in the last example . 6 Conclusion In this paper we propose two simple and effective attentional mechanisms for neural machine 16We concatenate the 508 sentence pairs with 1M sentence pairs from WMT and run the Berkeley aligner . 17http : 18The reference uses a more fancy translation of incompatible which is im Widerspruch zu etwas stehen . Both models however failed to translate passenger experience . 1419 translations src Orlando Bloom and Miranda Kerr still love each other ref Orlando Bloom und Miranda Kerr lieben sich noch immer best Orlando Bloom und Miranda Kerr lieben einander noch immer . base Orlando Bloom und Lucas Miranda lieben einander noch immer . src We re pleased the FAA recognizes that an enjoyable passenger experience is not incompatible with safety and security said Roger Dow CEO of the Travel Association . ref Wir freuen uns dass die FAA erkennt dass ein angenehmes Passagiererlebnis nicht im Widerspruch zur Sicherheit steht sagte Roger Dow CEO der Travel Association . best Wir freuen uns dass die FAA anerkennt dass ein angenehmes ist nicht mit Sicherheit und Sicherheit unvereinbar ist sagte Roger Dow CEO der US die . base Wir freuen uns die unk dass ein unk unk mit Sicherheit nicht vereinbar ist mit Sicherheit und Sicherheit sagte Roger Cameron CEO der US unk . translations src In einem Interview sagte Bloom jedoch dass er und Kerr sich noch immer lieben . ref However in an interview Bloom has said that he and Kerr still love each other . best In an interview however Bloom said that he and Kerr still love . base However in an interview Bloom said that he and Tina were still unk . src Wegen der von Berlin und der Zentralbank strengen Sparpolitik in Verbindung mit der Zwangsjacke in die die jeweilige nationale Wirtschaft durch das Festhalten an der gemeinsamen wird sind viele Menschen der Ansicht das Projekt Europa sei zu weit gegangen ref The austerity imposed by Berlin and the European Central Bank coupled with the straitjacket imposed on national economies through adherence to the common currency has led many people to think Project Europe has gone too far . best Because of the strict austerity measures imposed by Berlin and the European Central Bank in connection with the straitjacket in which the respective national economy is forced to adhere to the common currency many people believe that the European project has gone too far . base Because of the pressure imposed by the European Central Bank and the Federal Central Bank with the strict austerity imposed on the national economy in the face of the single currency many people believe that the European project has gone too far . Table 5: Sample translations for each example we show the source src the human translation ref the translation from our best model best and the translation of a model base . We italicize some correct translation segments and highlight a few wrong ones in bold . translation: the global approach which always looks at all source positions and the local one that only attends to a subset of source positions at a time . We test the effectiveness of our models in the WMT translation tasks between English and German in both directions . Our local attention yields large gains of up to BLEU over models that already incorporate known techniques such as dropout . For the English to German translation direction our ensemble model has established new results for both WMT 14 and WMT 15 . We have compared various alignment functions and shed light on which functions are best for which attentional models . Our analysis shows that NMT models are superior to nonattentional ones in many cases for example in translating names and handling long sentences . Acknowledgment We gratefully acknowledge support from a gift from Bloomberg and the support of NVIDIA Corporation with the donation of Tesla K40 GPUs . We thank Andrew Ng and his group as well as the Stanford Research Computing for letting us use their computing resources . We thank Russell Stewart for helpful discussions on the models . Lastly we thank Quoc Le Ilya Sutskever Oriol Vinyals Richard Socher Michael Kayser Jiwei Li Panupong Pasupat Kelvin Gu members of the Stanford NLP Group and the annonymous reviewers for their valuable comments and feedback . 1420