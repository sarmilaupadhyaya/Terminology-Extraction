Abstract Previously neural methods in grammatical error correction GEC did not reach results compared to statistical machine translation SMT baselines . We demonstrate parallels between neural GEC and neural MT and successfully adapt several methods from MT to neural GEC . We further establish guidelines for trustable results in neural GEC and propose a set of methods for neural GEC that can be easily applied in most GEC settings . Proposed methods include adding noise techniques a transfer learning with monolingual data and ensembling of independently trained GEC models and language models . The combined effects of these methods result in better than neural GEC models that outperform previously best neural GEC systems by more than 10 M2 on the benchmark and on the JFLEG test set . systems are outperformed by more than 2 on the benchmark and by 4 on JFLEG . 1 Introduction Most successful approaches to automated grammatical error correction GEC are based on methods from statistical machine translation SMT especially the variant . For the CoNLL 2014 benchmark on grammatical error correction Ng et 2014 and Grundkiewicz 2016 established a set of methods for GEC by SMT that remain . Systems Chollampatt and Ng 2017 Yannakoudakis et 2017 that improve on results by and Grundkiewicz 2016 use their as a backbone for more complex systems . The view that GEC can be approached as a machine translation problem by translating from erroneous to correct text originates from Brockett et al . 2006 and resulted in many systems . Felice et 2014 Susanto et 2014 that represented the current at the time . In the field of machine translation proper the emergence of neural methods and their impressive results have lead to a paradigm shift away from SMT towards neural machine translation NMT . During WMT 2017 Bojar et 2017 authors of pure systems offered unconditional surrender 1 to methods . Based on these developments one would expect to see a rise of neural methods for GEC but as and Grundkiewicz 2016 already noted this is not the case . Interestingly even today the top systems on established GEC benchmarks are still mostly or hybrid systems Chollampatt and Ng 2017 Yannakoudakis et 2017 Napoles and CallisonBurch 2017 . The best pure neural systems Ji et 2017 Sakaguchi et 2017 Schmaltz et 2017 are several percent If we look at recent MT work with this in mind we find one area where SMT dominates over NMT: machine translation . Koehn and Knowles 2017 analyze the behavior of NMT versus SMT for systems trained on million to million words of parallel data illustrated in Figure 1 . Quality for NMT 1Ding et al . 2017 on their news translation shared task poster http : 2After submission of this work Chollampatt and Ng 2018 published impressive new results for neural GEC with some overlap with our methods . However our results stay ahead on all benchmarks while using simpler models . 595 Figure 1: BLEU scores for systems trained on to words of parallel data . Source: Koehn and Knowles 2017 Corpus Sent . Tokens Public NUCLE Yes NAIST Yes CLC FCE Yes CLC No Table 1: Statistics for existing GEC training data sets . Data sets marked with are used in this work . starts low for small corpora outperforms SMT at a corpus size of about 15 million words and with increasing size beats SMT with a large language model . Table 1 lists existing training resources for the English ESL grammatical error correction task . Publicly available resources NUS Corpus of Learner English NUCLE by Dahlmeier et al . 2013 NAIST Mizumoto et 2012 and CLC FCE Yannakoudakis et 2011 amount to about 27M tokens . Among these the corpus is quite noisy and of low quality . The Cambridge Learner Corpus CLC by Nicholls 2003 probably the best resource in this list is and we would strongly discourage reporting results that include it as training data as this makes comparisons difficult . Contrasting this with Fig . 1 we see that for about 20M tokens NMT systems start outperforming SMT models without additional large language models . Current GEC systems based on SMT however all include indomain language models either following the steps outlined in and Grundkiewicz 2016 or directly their language model . It seems that the current state of neural methods in GEC reflects the behavior for NMT systems trained on smaller data sets . Based on this we conclude that we can think of GEC as a lowresource or at most machine translation problem . This means that techniques proposed for neural MT should be applicable to improving neural GEC results . In this work we show that adapting techniques from neural MT and GEC methods allows neural GEC systems to catch up to and outperform systems . We improve over the previously neural GEC system Ji et 2017 on the CoNLL 2014 test set by more than 10 M2 over a comparable pure SMT system by and Grundkiewicz 2016 by 6 and outperform the result of Chollampatt and Ng 2017 by 2 . On the JFLEG data set we report the currently best results outperforming the previously best pure neural system Sakaguchi et 2017 by GLEU and the best reported results Chollampatt and Ng 2017 by 3 GLEU . In Section 2 we describe our baseline for GEC and follow recommendations from the MT community for a trustable neural GEC system . In Section 3 we adapt neural models to make better use of sparse data transferring MT and SMT methods to neural GEC . This includes a novel training objective for GEC . We investigate how to leverage monolingual data for neural GEC by transfer learning in Section 4 and experiment with language model ensembling in Section 5 . Section 6 explores deep NMT architectures . In Section 7 we provide an overview of the experiments and how results relate to the JFLEG benchmark . We also recommend a toolbox for neural GEC . 2 A trustable baseline for neural GEC In this section we combine insights from JunczysDowmunt and Grundkiewicz 2016 for grammatical error correction by statistical machine translation and from Denkowski and Neubig 2017 for trustable results in neural machine translation to propose a trustable baseline for neural grammatical error correction . 596 set Sent . Annot . Metric test 1 M2 test 2 M2 JFLEG dev 754 4 GLEU JFLEG test 747 4 GLEU Table 2: Statistics for test and development data . Training and test data To make our results comparable to results in the field of GEC we limit our training data strictly to public resources . In the case of data as marked in Table 1 these are the NUCLE Dahlmeier et 2013 and Lang8 NAIST Mizumoto et 2012 data sets . We do not include the FCE corpus Yannakoudakis et 2011 to maintain comparability to JunczysDowmunt and Grundkiewicz 2016 and Chollampatt and Ng 2017 . We strongly urge the community to not use the CLC corpus for training unless contrastive results without this corpus are provided as well . We choose the shared task test set Ng et 2014 as our main benchmark and the test set from the 2013 edition of the shared task Ng et 2013 as a development set . For these benchmarks we report MaxMatch M2 scores Dahlmeier and Ng 2012 . Where appropriate we will provide results on the JFLEG dev and test sets Napoles et 2017 using the GLEU metric Sakaguchi et 2016 to demonstrate the generality of our methods . Table 2 summarizes set statistics for both tasks . For most our experiments we report M2 on test Dev and precision Prec . recall Rec . M2 Test on the test set . Preprocessing and As both benchmarks CoNLL and JFLEG are provided in tokenization Bird et 2009 we use the same tokenization scheme for our training data . We truecase line beginnings and escape special characters using scripts included with Moses Koehn et 2007 . Following Sakaguchi et al . 2017 we apply the Enchant3 to the JFLEG data before evaluation . No spellchecking is used for the CoNLL test sets . We follow the recommendation by Denkowski and Neubig 2017 to use encoding BPE units Sennrich et 2016b to solve the 3https: problem of NMT . This is a well established procedure in neural machine translation and has been demonstrated to be generally superior to methods . It has been largely ignored in the field of grammatical error correction even when word segmentation issues have been explored Ji et 2017 Schmaltz et 2017 . To our knowledge this is the first work to use BPE for GEC however an analysis on advantages of word versus or character level segmentation is beyond the scope of this paper . A set of monolingual BPE units is trained on the data and we segment training and data accordingly . Segmentation is reversed before evaluation . Model and training procedure Implementations of all models explored in this work4 are available in the Marian5 toolkit JunczysDowmunt et 2018 . The attentional encoderdecoder model in Marian is a of the NMT model in Nematus Sennrich et 2017b . The model differs from the model introduced by Bahdanau et al . 2014 by several aspects the most important being the conditional GRU with attention for which Sennrich et al . 2017b provide a concise description . All embedding vectors consist of 512 units the RNN states of 1024 units . The number of BPE segments determines the size of the vocabulary of our models . entries . Source and target side use the same vocabulary . To avoid overfitting we use variational dropout Gal and Ghahramani 2016 over GRU steps and input embeddings with probability . We optimize with Adam Kingma and Ba 2014 with an average size of ca . 200 . All models are trained until convergence with a patience of 10 based on development set cost saving model checkpoints every . The best eight model checkpoints . the development set M2 score of each training run are averaged elementwise et 2016 resulting in a final single model . During decoding we use a of 24 and normalize model scores by 4Models system configurations and outputs are available from https : 5https : 6We used a larger than usual due to experiments with of lists not included in the paper . We did not see any differences compared to smaller beams . 597 CoNLL JFLEG Run Dev Prec . Rec . Test Dev Test 1 2 3 4 Avg Ens Table 3: Instable results for multiple baseline runs versus average and ensemble for the CoNLL benchmark . Optimizer instability and Grundkiewicz 2016 noticed that discriminative parameter tuning for GEC by SMT leads to unstable M2 results between tuning runs . This is a effect for SMT parameter tuning and Clark et al . 2011 recommend reporting results for multiple tuning runs . and Grundkiewicz 2016 perform four tuning runs and calculate parameter centroids following Cettolo et al . 2011 . Neural training is discriminative optimization and as such prone to instability . We already try to alleviate this by averaging over eight best checkpoints but as seen in Table 3 results for M2 remain unstable for runs with differently initialized weights . An amplitude of 3 points M2 on the test set is larger than most improvements reported in recent papers . None of the recent works on neural GEC account for instability hence it is unclear if observed outcomes are actual improvements or lucky picks among byproducts of instability . We therefore strongly suggest to provide results for multiple independently trained models . Otherwise improvements of less than 2 or 3 points of M2 remain doubtful . Interestingly GLEU on the JFLEG data seems to be more stable than M2 on CoNLL data . Ensembling of independent models Running multiple experiments to provide averaged results seems prohibitively expensive but Denkowski and Neubig 2017 and others . Sutskever et 2014 Sennrich et 2017a show that ensembling of independently trained models leads to consistent rewards for MT . For our baseline in Table 3 the opposite seems to be true for M2 . This is likely the reason why no other work on neural GEC mentions results for ensembles . . . . M 2 Average of 4 Ensemble of 4 Model Dev Prec . Rec . Test Baseline . . . . Table 4: Results M2 on the CoNLL benchmark for adaptations . On closer inspection however we see that the drop in M2 for ensembles is due to a precision bias . M2 being an penalizes increasing distance between precision and recall . The increase in precision for ensembles is to be expected and we see it later consistently for all experiments . Ensembles choose corrections for which all independent models are fairly confident . This leads to fewer but better corrections hence an increase in precision and a drop in recall . If the models are weak as our baseline this can result in a lower score . It would however be unwise to dismiss ensembles as we can use their bias towards precision to our advantage whenever they are combined with methods that aim to increase recall . This is true for nearly all remaining experiments . 3 Adaptations for GEC The methods described in this section turn our baseline into a more system . Most have been inspired by techniques from MT or closely related techniques for NMT . All modifications are applied incrementally later models include enhancements from the previous ones . 598 dropout as corruption GEC can be treated as a denoising task where grammatical errors are corruptions that have to be reduced . By introducing more corruption on the source side during training we can teach the model to reduce trust into the source input and to apply corrections more freely . Dropout is one way to introduce noise but for now we only drop out single units in the embedding or GRU layers something the model can easily recover from . To make the task harder we add dropout over source words setting the full embedding vector for a source word to with a probability of psrc . During our experiments we found psrc to work best . Table 4 show impressive gains for this simple method . Results for the ensemble match the previously best results on the CoNLL2014 test set for pure neural systems without the use of an additional monolingual language model by Ji et al . 2017 and Schmaltz et al . 2017 . Domain adaptation The NUCLE corpus matches the domain of the CoNLL benchmarks perfectly . It is however much smaller than the corpus . A setting like this seems to be a good fit for techniques . Sennrich et al . 2016a oversample news data in a larger training corpus . We do the same by adding the NUCLE corpus ten times to the training corpus . This can also be seen as similar to and Grundkiewicz 2016 who tune SMT parameters on the entire NUCLE corpus . Respectable improvements on both CoNLL test sets . in Table 4 are achieved . Error adaptation and Grundkiewicz 2016 noticed that when tuning on the entire NUCLE corpus even better results can be achieved if the error rate of NUCLE is adapted to the error rate of the original dev set . In NUCLE only 6 of tokens contain errors while the test set has an of about 15 . Following JunczysDowmunt and Grundkiewicz 2016 we remove correct sentences from the oversampled NUCLE data greedily until an of 15 is achieved . This can be interpreted as a type of domain adaptation . We mark this method as . in Table 4 and report for the ensemble the so far strongest results for any neural GEC system on the CoNLL benchmark . CoNLL JFLEG Λ Dev Prec . Rec . Test Dev Test 1 3 5 Table 5: Results for model type . trained with MLE and chosen Λ . Tied embeddings Press and Wolf 2016 showed that parameter tying between input and output embeddings7 for language models leads to improved perplexity . Similarly between source target and output embeddings for neural machine translation seems to improve translation quality in terms of BLEU while also significantly decreasing the number of parameters in the model . In monolingual cases like GEC where source and target vocabularies are mostly equal seems to arise naturally . Output layer decoder and encoder embeddings all share information which may further enhance the signal from corrective edits . The M2 scores for . in Table 4 are inconclusive but we see improvements in conjunction with later modifications . MLE objective Previously we applied adaptation to strengthen the signal from corrective edits in the training data . In this section we investigate the effects of directly modifying the training loss to incorporate weights for corrective edits . Assuming that each target token yj has been generated by a source token xi we scale the loss for each target token yj by a factor Λ if yj differs from xi . if yj is part of an edit . Hence loglikelihood loss takes the following form: L x y a X Ty λ xat yt log P yt y t λ xat yt Λ if xat yt 1 otherwise where x y is a training sentence pair and a is a word alignment at 0 1 . . . Tx such that source token xat generates target token yt . Alignments are computed for each sentence pair with Dyer et 2013 . 7Output embeddings are encoded in the last output layer of a neural language or translation model . 599 target embedding output embedding source embedding first output layer bidirectional GRU second cGRU block first cGRU block attention mechanism encoder decoder embeddings decoder parameters Randomly initialized parameters Figure 2: Parameters pretrained on monolingual data are marked with colors . Blue indicates embeddings with word2vec red parameters have been with the language model only . All embedding layers have tied parameters . This is comparable to reinforcement learning towards GLEU as introduced by Sakaguchi et al . 2017 or training against diffs by Schmaltz et al . 2017 . In combination with previous modifications Maximum Likelihood Estimation MLE weighting seem to outperform both methods . The parameter Λ introduces an additional that requires tuning for specific tasks and affects the . Table 5 shows Λ 3 seems to work best among the tested values when chosen to maximize M2 on the dev set . For this setting we achieve our strongest results of M2 on the CoNLL benchmark system yet . This outperforms the results of a SMT system with a large domainadapted language model from and Grundkiewicz 2016 by 1 M2 and is the first neural system to beat this strong SMT baseline . 4 Transfer learning for GEC Many ideas in neural MT are rooted in transfer learning . In general one first trains a neural model on data and then uses the resulting parameters to initialize parameters of a new model meant to be trained on lowresource data only . Various settings are possible . initializing from models trained on large data and continuing on data Miceli Barone et 2017 or using related language pairs Zoph et 2016 . Models can also be partially initialized by monolingual language models Ramachandran et 2017 or only Gangi and Federico 2017 . In GEC Yannakoudakis et al . 2017 apply pretrained monolingual as initializations for models to SMT lists . Approaches based on with monolingual data appear to be particularly wellsuited to the GEC task . and Grundkiewicz 2016 published 300GB of compressed monolingual data used in their work to create a large ngram language We use the first 100M lines . Preprocessing follows section including BPE segmentation . embeddings Similarly to Gangi and Federico 2017 or Yannakoudakis et al . 2017 we use Word2vec Mikolov et 2013 with standard settings to create word vectors . Since weights between source target and output embeddings are tied these embeddings are inserted once into the model but affect computations see the blue elements in Figure 2 . The remaining parameters of the model are initialized randomly . We refer to this adaptation as . 8https : 600 . . M2 . Average of 4 Ensemble of 4 Model Dev Prec . Rec . Test . . . Table 6: Results M2 on the CoNLL benchmark set for adaptations . decoder parameters Following Ramachandran et al . 2017 we first train a language model on the monolingual data . The architecture of the language model corresponds as much as possible to the structure of the decoder of the model . All pieces that rely on the attention mechanism or the encoder have been removed . After training for two epochs all red parameters including embedding layers in Figure 2 are copied from the language model to the decoder . Remaining parameters are initialized randomly . This configuration is called We pretrain each model separately to make sure that all weights have been initialized randomly . Results for transfer learning Table 6 summarizes the results for our transfer learning experiments . We compare the effects of with and without the MLE objective and can see that has significantly positive effects in both settings . The last result of M2 on the benchmark matches the currently highest reported numbers M2 by Chollampatt and Ng Model Dev Prec . Rec . Test Table 7: Ensembling with a neural language model . 2017 for a much more complex system and outperforms the highest neural GEC system Ji et 2017 by 8 M2 . 5 Ensembling with language models SMT systems benefit naturally from large monolingual language models also in the case of GEC as shown by and Grundkiewicz 2016 . Previous work Xie et 2016 Ji et 2017 on neural GEC used language models to incorporate monolingual data . Xie et al . 2016 built a large model and integrated it directly into their beam search algorithm while Ji et al . 2017 the language model provided by and Grundkiewicz 2016 for list . We already combined monolingual data with our GEC models via but exploiting separate language models is attractive as no additional training is required . Here we reuse the neural language model created for . Similarly to Xie et al . 2016 the score s for a correction y of sentence x is calculated as s 1 X 4 log Pi α log PLM y where Pi is a translation probability for the model in an ensemble of 4 . PLM y is the language model probability for y weighted by α . We normalize by sentence length . Using the dev set we choose α that maximizes this score via linear search in range 0 2 with step . Table 7 summarizes results for language model ensembling with three of our intermediate configurations . All configurations benefit from the language model in the ensemble although gains for the model are rather small . 601 6 Deeper NMT models So far we analyzed methods only training data parameter initialization and the objective function were modified . In this section we investigate if these techniques can be generalized to deeper or different architectures . Architectures We consider two NMT architectures implemented in Marian: Deep RNN A deep model Miceli Barone et 2017 proposed by Sennrich et al . 2017a for their WMT 2017 submissions . This model is based on the shallow model we used until now . It has single layer RNNs in the encoder and decoder but increases depth by stacking multiple blocks inside one RNN cell . A single RNN step passes through all blocks before recursion . The encoder RNN contains 4 stacked GRU blocks the decoder 8 1 7 due to the conditional GRU . Following Sennrich et al . 2017a we enable in the . State and embedding dimensions used throughout this work and in Sennrich et al . 2017a are the same . Transformer The model by Vaswani et al . 2017 . We base our model on their default architecture of 6 complex blocks in the encoder and decoder and use the same model dimensions embeddings vector size is 512 as before filter size is 2048 . Training settings As the deep models are less reliably trained with asynchronous SGD we change the training algorithm to synchronous SGD and for both models follow the recipe proposed in Vaswani et al . 2017 with an effective base learning rate of learning rate during the first iterations and an inverse decay after the warmup . As before we average the best 8 checkpoints . We increase dropout probability over RNN layers to for and similarly set dropout between transformer layers to . dropout as a noising technique remains unchanged . 9The procedure however needs to be adapted to model architecture if we want to take advantage of every shared parameter otherwise matching parameter subsets could probably be used successfully . Model Dev Prec . Rec . Test Table 8: Shallow versus deep ensembles with and without corresponding language models . deep models We reuse all methods included up to . The procedure as described in section needs to be modified in order to maximize the number of parameters for the larger model architectures . Again we train models as typical language models by removing all elements that depend on the encoder including over the source context . We can keep the decoder layers in the transformer model . We train for two epochs on our monolingual data reusing the for the parallel case above . Results Table 8 summarizes the results for deeper models on the CoNLL dev and test set . Both deep models improve significantly over the shallow model with the transformer model reaching our best result reported on the CoNLL 2014 test set . For that test set it seems that ensembling with language models that were used for is ineffective when measured with M2 while on the JFLEG data measured with GLEU we see strong improvements Fig . 3b . 7 A standard tool set for neural GEC We summarize the results for our experiments in Figure 3 and provide results on the JFLEG test set . Weights for the independent language model in the full ensemble were chosen on the respective dev sets for both tasks . Comparing results according to both benchmarks and evaluation metrics M2 for CoNLL GLEU for JFLEG it seems we can isolate the following set of reliable methods for neural grammatical error correction: Ensembling neural GEC models with monolingual language models Dropping out entire source embeddings 602 Baseline . . . . . Transformer Chollampatt and Ng 2017 and Grundkiewicz 2016 Ji et al . 2017 Average of 4 Ensemble of 4 Ens . with LM a test set M2 Baseline . . . . . Transformer Chollampatt and Ng 2017 Sakaguchi et al . 2017 Average of 4 Ensemble of 4 Ens . with LM b JFLEG test set GLEU Figure 3: Comparison on the test set and JFLEG test for all investigated methods . Weighting edits in the training objective during optimization on monolingual data Ensembling of independently trained models Domain and error adaptation . towards a specific benchmark Increasing model depth . Combinations of these generally10 modelindependent methods helped raising the performance of pure neural GEC systems by more than 10 M2 on the CoNLL 2014 benchmark also outperforming the previous Chollampatt and Ng 2017 a hybrid system with a complex system by 2 . We also showed that a pure neural system can easily 10Increasing depth or changing the architecture to the Transformer model is clearly not . outperform a strong pure SMT system and Grundkiewicz 2016 when similarly adapted to the GEC task . On the JFLEG benchmark we outperform the pure neural system Sakaguchi et 2017 by GLEU if no monolingual data is used . Improvements over system like Napoles and 2017 11 and Chollampatt and Ng 2017 are significant and constitute the new on the JFLEG test set . Acknowledgments This work was partially funded by Facebook . The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements either expressed or implied of Facebook . 11Results based on errata from https: errata 603