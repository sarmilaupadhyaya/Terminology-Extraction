Abstract A few approaches have been developed to improve neural machine translation NMT models with multiple passes of decoding . However their performance gains are limited because of lacking proper policies to terminate the process . To address this issue we introduce a novel architecture of RewriterEvaluator . Translating a source sentence involves multiple rewriting passes . In every pass a rewriter generates a new translation to improve the past translation . Termination of this process is determined by a score of translation quality estimated by an evaluator . We also propose prioritized gradient descent PGD to jointly and efficiently train the rewriter and the evaluator . Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods . An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy . Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting . 1 Introduction architecture Sutskever et 2014 has been widely used in natural language generation especially neural machine translation NMT Bahdanau et 2015 Gehring et 2017 Vaswani et 2017 Zhang et 2019 Kitaev et 2020 . Given a source sentence an encoder firstly converts it into hidden representations which are then conditioned by a decoder to produce a target sentence . In analogy to the development of statistical machine translation SMT Och and Ney 2002 Shen et 2004 Zhang and Gildea 2008 some recent methods in NMT attempt to improve the architecture with multipass decoding Xia et 2017 Zhang et 2018 Geng et 2018 Niehues et 2016 . In these models more than one translation is generated for a source sentence . Except for the first translation each of the later translations is conditioned on the previous one . While these methods have achieved promising results they lack a proper termination poqlicy for this process . For instance Xia et al . 2017 Zhang et al . 2018 adopt a fixed number of decoding passes which is inflexible and can be . Geng et al . 2018 utilize reinforcement learning RL Sutton et 2000 to automatically decide the number of decoding passes . However RL is known to be unstable due to the high variance in gradient estimation Boyan and Moore 1995 . To address this problem we introduce a novel architecture . This architecture contains a rewriter and an evaluator . The translation process involves multiple passes . Given a source sentence at every turn the rewriter generates a new target sequence to improve the translation from the prior pass and the evaluator measures the translation quality to determine whether to end the iterative rewriting process . Hence the translation process is continued until a certain condition is met such as no significant improvement in the measured translation quality . In implementations the rewriter is a conditional language model Sutskever et 2014 and the evaluator is a text matching model Wang et 2017 . We also propose prioritized gradient descent PGD that facilitates training the rewriter and the evaluator both jointly and efficiently . PGD uses a priority queue to store previous translation cases . The queue stores translations with descending order of their scores computed from the evaluator . The capacity of the queue is limited to be a few times of batch size . Due to its limited size the queue pops those translations with high scores and only keeps the translations with lower scores . The samples in 5702 Target Encoder! Source Encoder Decoder Estimator! Target Encoder! Source Encoder Rewriter Evaluator Target Sentence Source Sentence Target Sentence Quality Score Figure 1: General architecture of . the queue are combined together with new cases from the training data to train the rewriter . has been applied to improve two mainstream NMT models RNNSearch Bahdanau et 2015 and Transformer Vaswani et 2017 . We have conducted extensive experiments on three translation tasks NIST WMT 18 and WMT 14 . The results show that our architecture notably improves the performance of NMT models and significantly outperforms related approaches . We conduct oracle experiment to understand the source of improvements . The oracle can pick the best translation from all the rewrites . Results indicate that the evaluator helps our models achieve the performances close to the oracle outperforming the methods of fixing the number of rewriting turns . Compared against averaged performances using a fixed number of rewriting iterations performance gaps to the oracle can be reduced by in the case of RNNSearch and in the case of Transformer . Quantitatively we find the evaluator trained with PGD is significantly more accurate in determining the optimal number of rewriting turns . For example whereas the method in Geng et al . 2018 has accuracy in WMT 14 the evaluator achieves accuracy on Transformer . 2 consists of iterative processes involving a rewriting process ψ and an evaluation process φ . The process of translating an source sentence x x1 x2 xn is an application of the above processes . Assume we are at the iteration k 1 . The rewriter ψ generates a target sequence z k z k 1 z k 2 z k lk given the source sentence x and the past translation z z 1 z 2 z from the k 1 turn . lk and are the sentence lengths . The evaluator φ estimates the translation quality score q k of the new translation z k which is used for determining whether to end the multiturn process . Formally the pass of a translation process is defined as z k ψ x z q k φ x z k . 1 Initially z 0 and q 0 are respectively set as an empty string and . The above procedure is repeatedly carried out until not much improvement in the estimated quality score can be achieved q k q 0 2 where is a small value tuned on the development set . Alternatively the procedure is terminated if a certain number of iterations K 0 is reached . In the former case we adopt z as the final translation . In the latter case the last translation z K is accepted . Architecture A general architecture of using is illustrated in Fig . 1 . The rewriter ψ consists of a source encoder f SE a target decoder f T E and a decoder g DEC . The evaluator φ shares encoders with the rewriter and contains an estimator g EST . Assume it is at the pass . Firstly the source encoder f SE casts the source sentence x into word 5703 Algorithm 1: Prioritized Gradient Descent PGD Input: rewriter ψ evaluator φ training set T batch size B and expected iteration number Output: rewriter ψ and evaluator φ . 1 Initialize an empty priority queue A with the capacity C B 2 while Models are not converged do 3 Pop B cases with high quality scores from priority queue A and discard them . 4 Randomly sample a batch of training cases S from 5 for x y S do 6 Push the quadruple x y SOS EOS into queue A . 7 Initialize an empty priority queue D of limited size 8 Initialize an empty list F to collect samples for training . 9 for x y z r A do 10 Obtain translation z k and quality score q k respectively using Eq . 5 and Eq . 6 . 11 Push sample x y z k q k into list 12 Compute quality rate r k using Eq . 9 . 13 Push quadruple x y z k r k into queue 14 Optimize rewriter ψ with the samples in list F to reduce loss in Eq . 7 . 15 Optimize evaluator φ with the samples in list F to reduce loss in Eq . 8 . 16 Update priority queue A: A representations hi 1 i n: H h1 h2 hn f SE x 3 where operation is vector concatenation . Similarly the translation z from the previous turn k 1 is encoded as P p 1 p 2 p f T E z . 4 Then the decoder g DEC of the rewriter ψ produces a new translation z k as z k g DEC H P . 5 Ultimately the evaluator φ scores the new translation z k with the estimator g EST: P k f T E z k q k g EST H P k . 6 The implementation can be applied to a variety of architectures . The encoders f SE and f T E can be any sequence model such as CNN Kim 2014 . The decoder g DEC is compatible with any language model Transformer . The estimator g EST is a text matching model ESIM Chen et 2017 . In Sec . 4 we apply this implementation to improve generic NMT models . Training Criteria We represent the ground truth target sentence as a m 1 sequence y y0 y1 ym . The rewriter ψ is trained via teacher forcing . We use oi to denote the probability of the target word which is the prediction of feeding its prior words y0 y1 into the decoder g DEC . The training loss for the rewriter is J ψ X log oi yi . 7 where y0 SOS and ym EOS marking the ends of a target sentence . For the evaluator φ we incur a hinge loss between the translation score of the ground truth y and that of the current translation z k as q φ x y J φ max 0 1 q q k . 8 At training time translation z k is generated via greedy search instead of beam search to reduce training time . 3 Prioritized Gradient Descent We present prioritized gradient descent PGD to train the proposed architecture . Instead of the random sampling used in stochastic gradient descent 5704 RNN Encoder Source RNN Encoder Target!!!!!!!! Mechanism RNN Decoder!! Figure 2: RNNSearch with . SGD Bottou and Bousquet 2008 PGD uses a priority queue to store previous training cases that receive low scores from the evaluator . Randomly sampled training cases together with those from the priority queue are used for training . Details of PGD are illustrated in Algorithm 1 . Initially we set a priority queue A line with a limited size C B B is the batch size . E the expected number of rewriting iterations is set as K 2 . The queue A is ordered with a quality rate in descending order where the top one corresponds to the highest rate . The quality rate of a certain sample x y z k is computed as r k 1 ρ BLEU z k y ρ q k 9 where the weight ρ is controlled by an annealing schedule j with j being the current training epoch and BLEU Papineni et 2002 . The rate r k is dominated by BLEU in the first few epochs and is later dominated by the evaluation score q k with an increasing number of epochs . This design is to mitigate the cold start problem when training an evaluator φ . At every training epoch PGD firstly discards a certain number of previous training samples with high quality rates line from queue A . It then replaces them with newly sampled samples S to lines . Every sample x y z r in queue A is then rewritten into a new translation z k by the rewriter . These are scored by the evaluator φ lines . These new samples are used to respectively train the rewriter ψ and the evaluator φ to lines with Eq . 7 and Eq . 8 . PGD keeps translations in the queue A for rewriting until they are popped out from queue A with high scores from the evaluator φ . Hence the evaluator φ is jointly trained with the rewriter to learn discerning the quality of translations from the rewriter ψ in order to help the rewriter reduce loss in Eq . 7 . PGD uses a large queue to aggregate the past translations and newly sampled cases . Computationally this is more efficient than explicit B times of rewriting to obtain samples . This requires extra memory space in exchange for lowing training time . In Sec . we will show that the additional increase of training time by PGD is less than 20 which is tolerable . 4 Applications Following Sec . we use to improve RNNSearch and Transformer . RNNSearch . The improved RNNSearch is illustrated in Fig . 2 . The two encoders f SE and f T E and the decoder g DEC are GRU Chung et 2014 . We omit computation details of these modules and follow their settings in Bahdanau et al . 2015 . Note that at every decoding step the hidden state of decoder is attended to not only hi 1 i n but also p j 1 j . We apply mechanism Parikh et 2016 to model the estimator f EST . Firstly we capture the semantic alignment between the source sentence x and the translation z as αi j h T i Wp j hei X j exp αi j P j 0 exp αi j0 p j pe j X i exp αi j P i 0 exp αi 0 j hi . 10 Then we use average pooling to extract features and compute the quality score: q v T i hei n P j pe j 11 where is vector concatenation . Transformer . The Transformer Vaswani et 2017 is modified to an architecture in Fig . 3 . The input to the encoder contains a source sentence x a special symbol ALIGN and the past translation z: x 0 x ALIGN z 12 5705 Transformer Encoder Transformer Decoder Dot Product! ALIGN!!!!!!!!! Figure 3: Transformer with . where operation denotes the concatenation of two sequences . The following mask matrix is applied to every layer in the encoder : 0 T 1 0 T . 13 In this way the words in x can t attend to those in z and vice versa . ALIGN can attend to the words both in x and z . This design is to avoid attention in encoder layers . In earlier studies we find it slightly improves the performances of models . We denote the representation for ALIGN in the final encoder layer as h ALIGN . The estimator f EST obtains the quality score as q v Th ALIGN 14 in which v is a learnable vector . 5 Experiments We have conducted extensive experiments on three machine translation tasks: NIST WMT 18 and WMT 14 . The results show that significantly improves the performances of NMT models and notably outperforms prior methods . Oracle experiment verifies the effectiveness of the evaluator . Termination accuracy analysis shows our evaluator is much more accurate than prior methods in determining the optimal number of rewriting turns . We also perform ablation studies to explore the effects of some components . Experimental Setup For NIST the training set contains sentence pairs extracted from LDC corpora including LDC2002E18 LDC2003E07 LDC2003E14 a portion of LDC2004T07 LDC2004T08 and LDC2005T06 . We adopt NIST 2002 MT02 as the validation set . We use NIST 2003 MT03 NIST 2004 MT04 NIST 2005 MT05 and NIST 2006 MT06 for tests . For WMT 18 we use preprocessed data with byte pair encoding BPE tokenization Sennrich et 2016 . We use newstest2017 for validation and newstest2018 for test . For WMT 14 following the same setting as in Vaswani et al . 2017 we use preprocessed data that is tokenized via BPE with 32k merge operations and a shared vocabulary for English and German . We use newstest2013 for development and newstest2014 for test . We train all the models with 150k steps for NIST 300k steps for WMT 18 and 300k steps for WMT 14 . We select the model that performs the best on validations and report their performances on test sets . Using we measure BLEU scores and ones for NIST and WMT 14 respectively . For WMT 18 we use the BLEU scores calculated by . The improvements of the proposed models over the baselines are statistically significant with a reject probability smaller than Koehn 2004 . For RNNSearch the dimensions of word embeddings and hidden layers are both 600 . Encoder has 3 layers and decoder has 2 layers . Dropout rate is set to . For Transformer we follow the setting of in Vaswani et al . 2017 . Both models use beam size of 4 and the maximum number of training tokens at every step is 4096 . We use Adam Kingma and Ba 2014 for optimization . In all the experiments the proposed models run on NVIDIA Tesla V100 GPUs . For RewriterEvaluator the maximum number of rewriting iterations K is 6 and termination threshold is . are obtained by grid search except for the Transformer backbone . 1 http: . 2 http: . 3 https: . 4 https: . 5706 Method NIST MT03 MT04 MT05 MT06 Avg . Deliberation Networks Xia et 2017 Zhang et 2018 Adaptive Decoder Geng et 2018 Our Work RNNsearch Transformer Table 1: Experiment results of the proposed models and all the baselines on NIST . Method WMT 14 WMT 18 Adaptive Decoder Geng et 2018 Our Work RNNsearch Transformer Table 2: Experiment results on WMT 14 and WMT 18 . Results on NIST We adopt the following related baselines : 1 Deliberation Networks Xia et 2017 adopts a second decoder to polish the raw sequence produced by the decoder 2 Zhang et 2018 uses a backward decoder to generate a translation and a forward decoder to refine it with attention mechanism 3 Adaptive Decoder Geng et 2018 utilizes RL to model the iterative rewriting process . Table 1 shows the results of the proposed models and the baselines on NIST . Baseline BLEU scores are from Geng et al . 2018 . There are three observations . Firstly significantly improves the translation quality of NMT models . The averaged BLEU score of RNNSearch is raised by and that of Transformer is increased by . Secondly the proposed architecture notably outperforms prior decoding methods . The performance of RNNSearch surpasses those of Deliberation Network by by and Adaptive Decoder by . Because all of these systems use the same backbone of NMT models these results validate that is superior to other alternative methods . Lastly the proposed architecture can improve Transformer backbone by on average and the improvements are consistently observed on tasks from MT03 to MT06 . Results on WMT Tasks To further confirm the effectiveness of the proposed architecture we make additional comparisons on WMT 14 and WMT 18 . The results are demonstrated in Table 2 . Because the above methods don t have results on the two datasets we Adaptive Decoding for comparisons . These results are consistent with the observations in Sec . . We can see that the new architecture can improve BLEU scores on both RNNSearch and Transformer backbones . For example the improvements on RNNSearch backbone are on WMT 14 and on WMT 18 . On Transformer backbone scores are raised by on WMT 14 and on WMT 18 . Furthermore RNNSearch outperforms Adaptive Decoder by and respectively on the two tasks . Interestingly the proposed architecture on RNNSearch backbone even surpasses Transformer on these two datasets . For example the BLEU score on WMT 14 increases from to . Oracle Experiment We conduct oracle experiments on the test set of WMT 14 to understand potential improvements of our architecture . An oracle selects the iteration that the corresponding rewrite has the highest BLEU score . Its BLEU scores are shown on the 5707 1 2 3 4 5 6 Rewriting Turns RNNSearch BLEU Score Oracle Evaluator Avg . 1 2 3 4 5 6 Rewriting Turns Transformer Oracle Evaluator Avg . Figure 4: The oracle experiment conducted on WMT 14 . Method NIST WMT 14 WMT 18 Adaptive Decoder RNNSearch Transformer Table 3: PAT scores of different methods on NIST WMT 14 and WMT 18 . red dashed lines in Fig . 4 . The numbers on the green vertical bars are the BLEU scores of adopting a fixed number of rewriting iterations . Their averaged number is shown on the dashed blue line . BLEU score from using our evaluator is shown on the solid line . Results show that the evaluator with BLEU score and BLEU score is much better than the strategies of using a fixed number of rewriting turns . The gaps between oracle and the averaged performance by RNNSearch and Transformer with fixed iterations are and . Using the evaluator these gaps are reduced relatively by for RNNSearch and for Transformer respectively down to and . These results show that the evaluator is able to learn an appropriate termination policy approximating the performances of oracle policy . Termination Accuracy Analysis We define a metric percentage of accurate terminations PAT to measure how precise a termination policy can be . PAT is computed as 1 X x y δ w q x y w b x y 15 where δ is the indicator function that outputs 1 if its argument is true and 0 otherwise . For each pair x y in the test set U w q x y is the turn index k with the highest quality score maxk q k and w b x y is the one with the highest BLEU score Param . Sharing K NIST WMT 14 WMT 18 6 2 4 6 8 Table 4: Ablation studies conducted on the validation sets of NIST WMT 14 and WMT 18 . maxk BLEU z k y . The translations z k 1 k K and their scores q k 1 k K are obtained using Eq . 5 and Eq . 6 . For fair comparisons the maximum number of rewritings is set to 6 for both and Adaptive Decoder Geng et 2018 . Results in Table 3 show that PAT scores from are much higher than those of Adaptive Decoder . For instance RNNSearch surpasses Adaptive Decoder by on WMT 14 and on WMT 18 . Ablation Studies Table 4 shows the results of ablation studies on NIST WMT 14 and WMT 18 . Parameter Sharing . The encoders from Eq . 3 and Eq . 4 are shared between the rewriter and the evaluator . We find this improves the performances of the proposed models . For example on NIST sharing encoders increases our BLEU score 5708 Method WMT 14 Training Test RNNSearch 7h56m 11m26s 9h17m 39m50s Transformer 5h23m 14m11s 6h36m 52m02s Table 5: Running time comparisons on WMT 14 . from to with the same maximum iteration number of Maximum Number of Iterations . Increasing the maximum number of turns K generally improves the BLEU scores . For instance on NIST K 8 outperforms K 2 by K 4 by and K 6 by . However described in Sec . large K 8 can increase inference time cost . Moreover additional gains in performance from K 8 is small . We therefore set K 6 by default . Running Time Comparisons While achieving improved translation quality the models are trained with multiple passes of translation . Therefore a natural question is on the increase of training time and test time . We report results on 4 GPUs with the maximum rewriting turns K 6 and the beam size set to 8 . Results on WMT 14 are listed in Table 5 . It shows that increases the test time by approximately 4 times because of multiple passes of decoding . However training time is only relatively increased by 15 and 18 respectively on RNNSearch and Transformer due to the large priority queue used in PGD to store previous translation cases . 6 Related Work decoding has been well studied in statistical machine translation Brown et 1993 Koehn et 2003 2007 Och and Ney 2004 Chiang 2005 Dyer et 2013 . Och 2003 Och and Ney 2002 propose training models with minimum error rate criterion on lattices from decoder . Marie and Max 2015 introduce an iterative method to refine search space generated from simple feature with additional information from more complex feature . Shen et al . 2004 investigate reranking of hypothesis using neural models trained with discriminative criterion . Neubig et al . 2015 propose to reconfirm effectiveness of reranking . Chen et al . 2008 present a regeneration of search space from techniques such as expansion . These approaches are however applied to shallow models such as models Och and Ney 2002 . Our work is closely related to recent efforts in decoding on NMT . In these recent works Xia et 2017 Zhang et 2018 Geng et 2018 the models generate multiple target sentences for a source sentence and except for the first one each of them is based on the sentence generated in the previous turn . For example Xia et al . 2017 propose Deliberation Networks that uses a second decoder to polish the raw sequence produced by the decoder . While these methods have achieved promising results they lack a proper termination policy for the translation process . Zhang et al . 2018 adopt a predefined number of decoding passes which is not flexible . Geng et al . 2018 incorporate mechanism into NMT model via RL . However RL can be unstable for training because of the high variance in gradient estimation . The lack of a proper termination policy results in premature terminations or sentences which can largely limit the performance gains of these methods . 7 Conclusion This paper has introduced a novel architecture that achieves a proper termination policy for decoding in NMT . At every translation pass given the source sentence and its past translation a rewriter generates a new translation aiming at making further performance improvements over the past translations . An evaluator estimates the translation quality to determine whether to complete this iterative rewriting process . We also propose PGD that facilitates training the rewriter and the evaluator both jointly and efficiently . We have applied to improve mainstream NMT models . Extensive experiments have been conducted on three translation tasks NIST WMT 18 and WMT 14 showing that our architecture notably improves the results of NMT models and significantly outperforms other related methods . An oracle experiment and a termination accuracy analysis show that the performance gains can be attributed to the improvements in completing the rewriting process at proper iterations . 5709 Acknowledgments This work was done when the first author did internship at Ant Group . We thank anonymous reviewers for their valuable suggestions.