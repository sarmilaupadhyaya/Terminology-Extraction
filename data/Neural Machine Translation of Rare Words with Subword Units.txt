Abstract Neural machine translation NMT models typically operate with a fixed vocabulary but translation is an problem . Previous work addresses the translation of words by backing off to a dictionary . In this paper we introduce a simpler and more effective approach making the NMT model capable of translation by encoding rare and unknown words as sequences of subword units . This is based on the intuition that various word classes are translatable via smaller units than words for instance names via character copying or transliteration compounds via compositional translation and cognates and loanwords via phonological and morphological transformations . We discuss the suitability of different word segmentation techniques including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm and empirically show that subword models improve over a dictionary baseline for the WMT 15 translation tasks and by up to and BLEU respectively . 1 Introduction Neural machine translation has recently shown impressive results Kalchbrenner and Blunsom 2013 Sutskever et 2014 Bahdanau et 2015 . However the translation of rare words is an open problem . The vocabulary of neural models is typically limited to 30 000 words but translation is an probThe research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp . z . Samsung R D Institute Poland . lem and especially for languages with productive word formation processes such as agglutination and compounding translation models require mechanisms that go below the word level . As an example consider compounds such as the German sewage water treatment plant for which a segmented representation is intuitively more appealing than encoding the word as a vector . For NMT models the translation of words has been addressed through a to a dictionary Jean et 2015 Luong et 2015b . We note that such techniques make assumptions that often do not hold true in practice . For instance there is not always a correspondence between source and target words because of variance in the degree of morphological synthesis between languages like in our introductory compounding example . Also models are unable to translate or generate unseen words . Copying unknown words into the target text as done by Jean et 2015 Luong et 2015b is a reasonable strategy for names but morphological changes and transliteration is often required especially if alphabets differ . We investigate NMT models that operate on the level of subword units . Our main goal is to model translation in the NMT network itself without requiring a model for rare words . In addition to making the translation process simpler we also find that the subword models achieve better accuracy for the translation of rare words than models and dictionaries and are able to productively generate new words that were not seen at training time . Our analysis shows that the neural networks are able to learn compounding and transliteration from subword representations . This paper has two main contributions: We show that neural ma1715 chine translation is possible by encoding rare words via subword units . We find our architecture simpler and more effective than using large vocabularies and dictionaries Jean et 2015 Luong et 2015b . We adapt byte pair encoding BPE Gage 1994 a compression algorithm to the task of word segmentation . BPE allows for the representation of an open vocabulary through a vocabulary of character sequences making it a very suitable word segmentation strategy for neural network models . 2 Neural Machine Translation We follow the neural machine translation architecture by Bahdanau et al . 2015 which we will briefly summarize here . However we note that our approach is not specific to this architecture . The neural machine translation system is implemented as an network with recurrent neural networks . The encoder is a bidirectional neural network with gated recurrent units Cho et 2014 that reads an input sequence x x1 xm and calculates a forward sequence of hidden states 1 m and a backward sequence h 1 h m . The hidden states j and h j are concatenated to obtain the annotation vector hj . The decoder is a recurrent neural network that predicts a target sequence y y1 yn . Each word yi is predicted based on a recurrent hidden state si the previously predicted word and a context vector ci . ci is computed as a weighted sum of the annotations hj . The weight of each annotation hj is computed through an alignment model αij which models the probability that yi is aligned to xj . The alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation . A detailed description can be found in Bahdanau et 2015 . Training is performed on a parallel corpus with stochastic gradient descent . For translation a beam search with small beam size is employed . 3 Subword Translation The main motivation behind this paper is that the translation of some words is transparent in that they are translatable by a competent translator even if they are novel to him or her based on a translation of known subword units such as morphemes or phonemes . Word categories whose translation is potentially transparent include: named entities . Between languages that share an alphabet names can often be copied from source to target text . Transcription or transliteration may be required especially if the alphabets or syllabaries differ . Example: Barack Obama English German Áàðàê Îáàìà Russian Japanese cognates and loanwords . Cognates and loanwords with a common origin can differ in regular ways between languages so that translation rules are sufficient Tiedemann 2012 . Example: claustrophobia English Klaustrophobie German Êëàóñòðîôîáèÿ Klaustrofobiâ Russian morphologically complex words . Words containing multiple morphemes for instance formed via compounding affixation or inflection may be translatable by translating the morphemes separately . Example: solar system English Sonnensystem Sonne System German Naprendszer Nap Rendszer Hungarian In an analysis of 100 rare tokens not among the 50 000 most frequent types in our German training data1 the majority of tokens are potentially translatable from English through smaller units . We find 56 compounds 21 names 6 loanwords with a common origin 5 cases of transparent affixation sweetish sweet süßlich süß 1 number and 1 computer language identifier . Our hypothesis is that a segmentation of rare words into appropriate subword units is sufficient to allow for the neural translation network to learn transparent translations and to generalize this knowledge to translate and produce unseen We provide empirical support for this hy1 Primarily parliamentary proceedings and web crawl data . 2Not every segmentation we produce is transparent . While we expect no performance benefit from opaque segmentations . segmentations where the units cannot be translated independently our NMT models show robustness towards oversplitting . 1716 pothesis in Sections 4 and 5 . First we discuss different subword representations . Related Work For Statistical Machine Translation SMT the translation of unknown words has been the subject of intensive research . A large proportion of unknown words are names which can just be copied into the target text if both languages share an alphabet . If alphabets differ transliteration is required Durrani et 2014 . translation has also been investigated with models which proved especially successful for closely related languages Vilar et 2007 Tiedemann 2009 Neubig et 2012 . The segmentation of morphologically complex words such as compounds is widely used for SMT and various algorithms for morpheme segmentation have been investigated Nießen and Ney 2000 Koehn and Knight 2003 Virpioja et 2007 Stallard et 2012 . Segmentation algorithms commonly used for SMT tend to be conservative in their splitting decisions whereas we aim for an aggressive segmentation that allows for translation with a compact network vocabulary and without having to resort to dictionaries . The best choice of subword units may be taskspecific . For speech recognition language models have been used Bazzi and Glass 2000 . Mikolov et al . 2012 investigate subword language models and propose to use syllables . For multilingual segmentation tasks multilingual algorithms have been proposed Snyder and Barzilay 2008 . We find these intriguing but inapplicable at test time . Various techniques have been proposed to produce continuous word vectors based on characters or morphemes Luong et 2013 Botha and Blunsom 2014 Ling et 2015a Kim et 2015 . An effort to apply such techniques to NMT parallel to ours has found no significant improvement over approaches Ling et 2015b . One technical difference from our work is that the attention mechanism still operates on the level of words in the model by Ling et al . 2015b and that the representation of each word is . We expect that the attention mechanism benefits from our representation: the network can learn to place attention on different subword units at each step . Recall our introductory example Abwasserbehandlungsanlange for which a subword segmentation avoids the information bottleneck of a representation . Neural machine translation differs from phrasebased methods in that there are strong incentives to minimize the vocabulary size of neural models to increase time and space efficiency and to allow for translation without models . At the same time we also want a compact representation of the text itself since an increase in text length reduces efficiency and increases the distances over which neural models need to pass information . A simple method to manipulate the between vocabulary size and text size is to use shortlists of unsegmented words using subword units only for rare words . As an alternative we propose a segmentation algorithm based on byte pair encoding BPE which lets us learn a vocabulary that provides a good compression rate of the text . Byte Pair Encoding BPE Byte Pair Encoding BPE Gage 1994 is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single unused byte . We adapt this algorithm for word segmentation . Instead of merging frequent pairs of bytes we merge characters or character sequences . Firstly we initialize the symbol vocabulary with the character vocabulary and represent each word as a sequence of characters plus a special symbol which allows us to restore the original tokenization after translation . We iteratively count all symbol pairs and replace each occurrence of the most frequent pair A B with a new symbol AB . Each merge operation produces a new symbol which represents a character . Frequent character or whole words are eventually merged into a single symbol thus BPE requires no shortlist . The final symbol vocabulary size is equal to the size of the initial vocabulary plus the number of merge operations the latter is the only hyperparameter of the algorithm . For efficiency we do not consider pairs that cross word boundaries . The algorithm can thus be run on the dictionary extracted from a text with each word being weighted by its frequency . A minimal Python implementation is shown in Al1717 Algorithm 1 Learn BPE operations import re collections def vocab: pairs int for word freq in: symbols for i in range len symbols: pairs symbols i symbols freq return pairs def pair: bigram pair p r?! bigram r?! for word in: pair word word return vocab l o w : 5 l o w e r : 2 n e w e s t w i d e s t 10 for i in range: pairs vocab best max pairs vocab best vocab print best r l o lo lo w low e Figure 1: BPE merge operations learned from dictionary low lowest newer wider . gorithm 1 . In practice we increase efficiency by indexing all pairs and updating data structures incrementally . The main difference to other compression algorithms such as Huffman encoding which have been proposed to produce a encoding of words for NMT Chitnis and DeNero 2015 is that our symbol sequences are still interpretable as subword units and that the network can generalize to translate and produce new words unseen at training time on the basis of these subword units . Figure 1 shows a toy example of learned BPE operations . At test time we first split words into sequences of characters then apply the learned operations to merge the characters into larger known symbols . This is applicable to any word and allows for networks with fixed symbol In our example the OOV lower would be segmented into low . 3The only symbols that will be unknown at test time are unknown characters or symbols of which all occurrences in the training text have been merged into larger symbols like safeguar which has all occurrences in our training text merged into safeguard . We observed no such symbols at test time but the issue could be easily solved by recursively reversing specific merges until all symbols are known . We evaluate two methods of applying BPE: learning two independent encodings one for the source one for the target vocabulary or learning the encoding on the union of the two vocabularies which we call joint BPE The former has the advantage of being more compact in terms of text and vocabulary size and having stronger guarantees that each subword unit has been seen in the training text of the respective language whereas the latter improves consistency between the source and the target segmentation . If we apply BPE independently the same name may be segmented differently in the two languages which makes it harder for the neural models to learn a mapping between the subword units . To increase the consistency between English and Russian segmentation despite the differing alphabets we transliterate the Russian vocabulary into Latin characters with to learn the joint BPE encoding then transliterate the BPE merge operations back into Cyrillic to apply them to the Russian training 4 Evaluation We aim to answer the following empirical questions: Can we improve the translation of rare and unseen words in neural machine translation by representing them via subword units? Which segmentation into subword units performs best in terms of vocabulary size text size and translation quality? We perform experiments on data from the shared translation task of WMT 2015 . For our training set consists of million sentence pairs or approximately 100 million tokens . For the training set consists of million sentence pairs or approximately 50 million tokens . We tokenize and truecase the data with the scripts provided in Moses Koehn et 2007 . We use newstest2013 as development set and report results on newstest2014 and newstest2015 . We report results with BLEU and CHRF3 Popovic 2015 a character F3 score which was found to correlate well with 4 In practice we simply concatenate the source and target side of the training set to learn joint BPE . 5 Since the Russian training text also contains words that use the Latin alphabet we also apply the Latin BPE operations . 1718 human judgments especially for translations out of English Stanojevic et 2015 . Since our main claim is concerned with the translation of rare and unseen words we report separate statistics for these . We measure these through unigram F1 which we calculate as the harmonic mean of clipped unigram precision and We perform all experiments with Groundhog7 Bahdanau et 2015 . We generally follow settings by previous work Bahdanau et 2015 Jean et 2015 . All networks have a hidden layer size of 1000 and an embedding layer size of 620 . Following Jean et al . 2015 we only keep a shortlist of τ 30000 words in memory . During training we use Adadelta Zeiler 2012 a minibatch size of 80 and reshuffle the training set between epochs . We train a network for approximately 7 days then take the last 4 saved models models being saved every 12 hours and continue training each with a fixed embedding layer as suggested by Jean et 2015 for 12 hours . We perform two independent training runs for each models once with for gradient clipping Pascanu et 2013 of once with a of the latter produced better single models for most settings . We report results of the system that performed best on our development set newstest2013 and of an ensemble of all 8 models . We use a beam size of 12 for beam search with probabilities normalized by sentence length . We use a bilingual dictionary based on Dyer et 2013 . For our baseline this serves as dictionary for rare words . We also use the dictionary to speed up translation for all experiments only performing the softmax over a filtered list of candidate translations like Jean et al . 2015 we use K 30000 K0 10 . Subword statistics Apart from translation quality which we will verify empirically our main objective is to represent an open vocabulary through a compact subword vocabulary and allow for efficient training and Statistics for different segmentations of the Ger6Clipped unigram precision is essentially BLEU without brevity penalty . 8The time complexity of architectures is at least linear to sequence length and oversplitting harms efficiency . man side of the parallel data are shown in Table 1 . A simple baseline is the segmentation of words into character Character allow for different between sequence length tokens and vocabulary size types depending on the choice of The increase in sequence length is substantial one way to reduce sequence length is to leave a shortlist of the k most frequent word types unsegmented . Only the unigram representation is truly . However the unigram representation performed poorly in preliminary experiments and we report translation results with a bigram representation which is empirically better but unable to produce some tokens in the test set with the training set vocabulary . We report statistics for several word segmentation techniques that have proven useful in previous SMT research including compound splitting Koehn and Knight 2003 rulebased hyphenation Liang 1983 and Morfessor Creutz and Lagus 2002 . We find that they only moderately reduce vocabulary size and do not solve the unknown word problem and we thus find them unsuitable for our goal of translation without dictionary . BPE meets our goal of being and the learned merge operations can be applied to the test set to obtain a segmentation with no unknown Its main difference from the model is that the more compact representation of BPE allows for shorter sequences and that the attention model operates on Table 1 shows BPE with 59 500 merge operations and joint BPE with 89 500 operations . In practice we did not include infrequent subword units in the NMT network vocabulary since there is noise in the subword symbol sets . because of characters from foreign alphabets . Hence our network vocabularies in Table 2 are typically slightly smaller than the number of types in Table 1 . 9Our character do not cross word boundaries . We mark whether a subword is or not with a special character which allows us to restore the original tokenization . 10Joint BPE can produce segments that are unknown because they only occur in the English training text but these are rare of test tokens . 11We highlighted the limitations of attention in section . At the other end of the spectrum the character level is suboptimal for alignment Tiedemann 2009 . 1719 vocabulary BLEU CHRF3 unigram F1 name segmentation shortlist source target single single all rare OOV Sennrich and Haddow 2015 WUnk 300 000 500 000 WDict 300 000 500 000 50 000 60 000 60 000 BPE 60 000 60 000 BPE joint 90 000 90 000 Table 2: translation performance BLEU CHRF3 and unigram F1 on newstest2015 .: ensemble of 8 models . Best NMT system in bold . Unigram F1 with ensembles is computed for all words n 44085 rare words not among top 50 000 in training set n 2900 and OOVs not in training set n 1168 . segmentation tokens types UNK none 100 m 1 750 000 1079 characters 550 m 3000 0 character bigrams 306 m 20 000 34 character trigrams 214 m 120 000 59 compound splitting4 102 m 1 100 000 643 morfessor 109 m 544 000 237 186 m 404 000 230 BPE 112 m 63 000 0 BPE joint 111 m 82 000 32 character bigrams 129 m 69 000 34 shortlist : 50 000 Table 1: Corpus statistics for German training corpus with different word segmentation techniques . UNK: number of unknown tokens in newstest2013 . 4: Koehn and Knight 2003: Creutz and Lagus 2002: Liang 1983 . Translation experiments translation results are shown in Table 2 results in Table 3 . Our baseline WDict is a model with a dictionary . It differs from WUnk in that the latter uses no dictionary and just represents words as UNK12 . The dictionary improves unigram F1 for rare and unseen words although the improvement is smaller for since the dictionary is incapable of transliterating names . All subword systems operate without a dictionary . We first focus on unigram F1 where all systems improve over the baseline especially for rare words for for . For OOVs the baseline strategy of copying unknown words works well for . However when alphabets differ like in the subword models do much better . 12We use UNK for words that are outside the model vocabulary and OOV for those that do not occur in the training text . Unigram F1 scores indicate that learning the BPE symbols on the vocabulary union BPEJ90k is more effective than learning them separately and more effective than using character bigrams with a shortlist of 50 000 unsegmented words but all reported subword segmentations are viable choices and outperform the dictionary baseline . Our subword representations cause big improvements in the translation of rare and unseen words but these only constitute of the test sets . Since rare words tend to carry central information in a sentence we suspect that BLEU and CHRF3 underestimate their effect on translation quality . Still we also see improvements over the baseline in total unigram F1 as well as BLEU and CHRF3 and the subword ensembles outperform the WDict baseline by BLEU and CHRF3 . There is some inconsistency between BLEU and CHRF3 which we attribute to the fact that BLEU has a precision bias and CHRF3 a recall bias . For we observe the best BLEU score of with but the best CHRF3 score of with . For comparison to the to our knowledge best MT system on this data set we report syntaxbased SMT results Sennrich and Haddow 2015 . We observe that our best systems outperform the system in terms of BLEU but not in terms of CHRF3 . Regarding other neural systems Luong et al . 2015a report a BLEU score of on newstest2015 but we note that they use an ensemble of 8 independently trained models and also report strong improvements from applying dropout which we did not use . We are confident that our improvements to the translation of rare words are orthogonal to improvements achievable through other improvements in the network archi1720 tecture training algorithm or better ensembles . For the state of the art is the system by Haddow et al . 2015 . It outperforms our WDict baseline by BLEU . The subword models are a step towards closing this gap and yields an improvement of BLEU and CHRF3 over WDict . As a further comment on our translation results we want to emphasize that performance variability is still an open problem with NMT . On our development set we observe differences of up to 1 BLEU between different models . For single systems we report the results of the model that performs best on dev out of 8 which has a stabilizing effect but how to control for randomness deserves further attention in future research . 5 Analysis Unigram accuracy Our main claims are that the translation of rare and unknown words is poor in NMT models and that subword models improve the translation of these word types . To further illustrate the effect of different subword segmentations on the translation of rare and unseen words we plot words sorted by their frequency in the training To analyze the effect of vocabulary size we also include the system which is a system with the same vocabulary size as the WDict baseline and character bigrams to represent unseen words . Figure 2 shows results for the ensemble systems on newstest2015 . Unigram F1 of all systems tends to decrease for lowerfrequency words . The baseline system has a spike in F1 for OOVs . words that do not occur in the training text . This is because a high proportion of OOVs are names for which a copy from the source to the target text is a good strategy for . The systems with a target vocabulary of 500 000 words mostly differ in how well they translate words with rank 500 000 . A dictionary is an obvious improvement over producing UNK but the subword system achieves better performance . Note that all OOVs that the backoff dictionary produces are words that are copied from the source usually names while the subword 13We perform binning of words with the same training set frequency and apply bezier smoothing to the graph . systems can productively form new words such as compounds . For the 50 000 most frequent words the representation is the same for all neural networks and all neural networks achieve comparable unigram F1 for this category . For the interval between frequency rank 50 000 and 500 000 the comparison between and unveils an interesting difference . The two systems only differ in the size of the shortlist with representing words in this interval as single units and 50k via subword units . We find that the performance of degrades heavily up to frequency rank 500 000 at which point the model switches to a subword representation and performance recovers . The performance of remains more stable . We attribute this to the fact that subword units are less sparse than words . In our training set the frequency rank 50 000 corresponds to a frequency of 60 in the training data the frequency rank 500 000 to a frequency of 2 . Because subword representations are less sparse reducing the size of the network vocabulary and representing more words via subword units can lead to better performance . The F1 numbers hide some qualitative differences between systems . For WDict produces few OOVs recall but with high precision whereas the subword systems achieve higher recall but lower precision . We note that the character bigram model produces the most OOV words and achieves relatively low precision of for this category . However it outperforms the dictionary in recall . which suffers from transliteration or copy errors due to segmentation inconsistencies obtains a slightly better precision but a worse recall . In contrast to the joint BPE encoding of BPEJ90k improves both precision and recall . For unknown names can only rarely be copied and usually require transliteration . Consequently the WDict baseline performs more poorly for OOVs precision recall and the subword models improve both precision and recall precision and recall for . The full unigram F1 plot is shown in Figure 3 . 1721 vocabulary BLEU CHRF3 unigram F1 name segmentation shortlist source target single single all rare OOV Haddow et 2015 WUnk 300 000 500 000 WDict 300 000 500 000 50 000 60 000 60 000 BPE 60 000 60 000 BPE joint 90 000 100 000 Table 3: translation performance BLEU CHRF3 and unigram F1 on newstest2015 .: ensemble of 8 models . Best NMT system in bold . Unigram F1 with ensembles is computed for all words n 55654 rare words not among top 50 000 in training set n 5442 and OOVs not in training set n 851 . 100 101 102 103 104 105 106 0 1 50 000 500 000 training set frequency rank unigram F1 WDict WUnk Figure 2: unigram F1 on newstest2015 plotted by training set frequency rank for different NMT systems . 100 101 102 103 104 105 106 0 1 50 000 500 000 training set frequency rank unigram F1 WDict WUnk Figure 3: unigram F1 on newstest2015 plotted by training set frequency rank for different NMT systems . Manual Analysis Table 4 shows two translation examples for the translation direction Table 5 for . The baseline system fails for all of the examples either by deleting content health or by copying source words that should be translated or transliterated . The subword translations of health research institutes show that the subword systems are capable of learning translations when oversplitting or when the segmentation does not match morpheme boundaries: the segmentation would be linguistically more plausible and simpler to align to the English research institutes than the segmentation in the system but still a correct translation is produced . If the systems have failed to learn a translation due to data sparseness like for asinine which should be translated as dumm we see translations that are wrong but could be plausible for partial loanwords asinine . The examples show that the subword systems are capable of transliteration . However transliteration errors do occur either due to ambiguous transliterations or because of segmentations between source and target text which make it hard for the system to learn a transliteration mapping . Note that the system encodes Mirzayeva inconsistently for the two language pairs . This example is still translated correctly but we observe spurious insertions and deletions of characters in the system . An example is the transliteration of rakfisk where a ï is inserted and a ê is deleted . We trace this error back to translation pairs in the training data with inconsistent segmentations such as 1722 system sentence source health research institutes reference Gesundheitsforschungsinstitute WDict Forschungsinstitute source asinine situation reference dumme Situation WDict asinine situation UNK asinine situation situation situation Table 4: translation example . marks subword boundaries . system sentence source Mirzayeva reference Ìèðçàåâà Mirzaeva WDict Mirzayeva UNK Mirzayeva source rakfisk reference ðàêôèñêà rakfiska WDict rakfisk UNK rakfisk Table 5: translation examples . marks subword boundaries . from which the translation is erroneously learned . The segmentation of the joint BPE system is more consistent . 6 Conclusion The main contribution of this paper is that we show that neural machine translation systems are capable of translation by representing rare and unseen words as a sequence of subword This is both simpler and more effective than using a translation model . We introduce a variant of byte pair encoding for word segmentation which is capable of encoding open vocabularies with a compact symbol vocabulary of subword units . We show performance gains over the baseline with both BPE segmentation and a simple character bigram segmentation . Our analysis shows that not only words but also rare words are translated poorly by our baseline NMT 14The source code of the segmentation algorithms is available at https: . system and that reducing the vocabulary size of subword models can actually improve performance . In this work our choice of vocabulary size is somewhat arbitrary and mainly motivated by comparison to prior work . One avenue of future research is to learn the optimal vocabulary size for a translation task which we expect to depend on the language pair and amount of training data automatically . We also believe there is further potential in bilingually informed segmentation algorithms to create more alignable subword units although the segmentation algorithm cannot rely on the target text at runtime . While the relative effectiveness will depend on factors such as vocabulary size we believe that subword segmentations are suitable for most language pairs eliminating the need for large NMT vocabularies or models . Acknowledgments We thank Maja Popovic for her tion of CHRF with which we verified our reimplementation . The research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp . z . Samsung R D Institute Poland . This project received funding from the European Union s Horizon 2020 research and innovation programme under grant agreement 645452 QT21.