Abstract Multilingual neural machine translation aims at learning a single translation model for multiple languages . These jointly trained models often suffer from performance degradation on language pairs . We attribute this degeneration to parameter interference . In this paper we propose LaSS to jointly train a single unified multilingual MT model . LaSS learns Language Specific LaSS for each language pair to counter parameter interference . Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to BLEU . Besides LaSS shows its strong generalization performance at easy adaptation to new language pairs and translation . LaSS boosts translation with an average of BLEU on 30 language pairs . Codes and trained models are available at https: . 1 Introduction Neural machine translation NMT has been very successful for bilingual machine translation Bahdanau et 2015 Vaswani et 2017 Wu et 2016 Hassan et 2018 Su et 2018 Wang 2019 . Recent research has demonstrated the efficacy of multilingual NMT which supports translation from multiple source languages into multiple target languages with a single model Johnson et 2017 Aharoni et 2019 Zhang et 2020 Fan et 2020 Siddhant et 2020 . Multilingual NMT enjoys the advantage of deployment . Further the parameter sharing of multilingual NMT encourages transfer learning of different languages . An extreme case is translation where direct translation between a language pair never seen in training is possible Johnson et 2017 . contribution . En Zh En Fr En De a Full network En Zh En Fr En De b LaSS Figure 1: Illustration of a full network and languagespecific ones LaSS . represents shared weights . and represents weights for and respectively . Compared to the full multilingual model each LaSS learned model has language universal and language specific weights . While very promising several challenges remain in multilingual NMT . The most challenging one is related to the insufficient model capacity . Since multiple languages are accommodated in a single model the modeling capacity of NMT model has to be split for different translation directions Aharoni et 2019 . Therefore multilingual NMT models often suffer from performance degradation compared with their corresponding bilingual baseline especially for translation directions . The simplistic way to alleviate the insufficient model capacity is to enlarge the model parameters Aharoni et 2019 Zhang et 2020 . However it is not parameter or computation efficient and needs larger multilingual training datasets to avoid . An alternative solution is to design components such as division of the hidden cells into shared and ones Wang et 2018 adaptation layers Bapna and Firat 2019 Philip et 2020 layer normalization 294 and linear transformation Zhang et 2020 and latent layers Li et 2020 . In this work we propose LaSS a method to dynamically find and learn Language Specific Subnetwork for multilingual NMT . LaSS accommodates one for each language pair . Each has shared parameters with some other languages and at the same time preserves its language specific parameters . In this way multilingual NMT can model language specific and language universal features for each language pair in one single model without interference . Figure 1 is the illustration of vanilla multilingual model and LaSS . Each language pair in LaSS has both language universal and language specific parameters . The network itself decides the sharing strategy . The advantages of our proposed method are LaSS is parameter efficient requiring no extra trainable parameters to model language specific features . LaSS alleviates parameter interference potentially improving the model capacity and boosting performance . LaSS shows its strong generalization performance at easy adaptation to new language pairs and translation . LaSS can be easily extended to new language pairs without dramatic degradation of existing language pairs . Besides LaSS can boost translation by up to BLEU . 2 Related Work Multilingual Neural Machine Translation The standard multilingual NMT model uses a shared encoder and a shared decoder for different languages Johnson et 2017 . There is a in this architecture Arivazhagan et 2019: boosting the performance of low resource languages or maintain the performance of high resource languages . To solve this previous works assign some parts of the model to be language specific: Language specific decoders Dong et 2015 Language specific encoders and decoders Firat et 2016 Lyu et 2020 and Language specific hidden states and embeds Wang et 2018 . Sachan and Neubig 2018 compares different sharing methods and finds different sharing methods have a great impact on performance . Recently Zhang et al . 2021 analyze when and where language specific capacity matters . Li et al . 2020 uses a binary conditional latent variable to decide which language each layer belongs to . Model Pruning Our approach follows the standard pattern of model pruning: training finding the sparse network and Frankle and Carbin 2019 Liu et 2019 . Frankle and Carbin 2019 and Liu et al . 2019 highlight the importance of the sparse network architecture . Zhu and Gupta 2018 proposed a method to automatically adjust the sparse threshold . Sun et al . 2020 learns different sparse architecture for different tasks . Evci et al . 2020 iteratively redistribute the sparse network architecture by the gradient . 3 Methodology We describe LaSS method in this section . The goal is to learn a single unified model for many translation directions . Our overall idea is to find corresponding to each language pair and then only update the parameters of those subnetworks during the joint training . Multilingual NMT A multilingual NMT model learns a mapping function f from a sentence in one of many languages to another language . We adopt the multilingual Transformer mTransformer as the backbone network Johnson et 2017 . mTransformer has the same architecture with layers of multihead attention residual connection and layer normalization . In addition it has two lanuage identifying tokens for the source and target . Define a multilingual dataset N where si ti represents the source and target language . We train an initial multilingual MT model with the following loss . L X i X hx log Pθ y x 1 where hx yi is a sentence pair from the language si to ti and θ is the model parameter . Finding Language Specific Model Masks Training a single model jointly on multiple language directions will lead to performance degradation for rich resource pairs Johnson et 2017 . The single model will improve on low resource language pairs but will reduce performance on pairs like . Intuitively jointly training on all translation pairs will obtain an average 295 model . For rich resources such averaging may hurt the performance since a multilingual MT model must distribute its modeling capacity for all translation directions . Based on this intuition our idea is to find a of the original multilingual model . Such is specific to each language pair . We start from a multilingual base model θ0 . The θ0 is trained with Eq . 1 . A is indicated by a binary mask vector 0 1 for language pair si ti . Each element being 1 indicates to retain the weight and 0 to abandon the weight . Then the parameters associated with si ti is θ j 0 Mj 1 where j denotes the jth element in θ0 . The parameters are only responsible for the particular language si and ti . We intend to find such language specific . Figure 1 illustrates the original model and its language specific . Given an initial model θ0 we adopt a simple method to find the language specific mask for each language pairs . 1 . Start with a multilingual MT model θ0 jointly trained on N . 2 . For each language pair si ti θ0 on . Intuitively θ0 on specific language pair si ti will amplify the magnitude of the important weights for si ti and diminish the magnitude of the unimportant weights . 3 . Rank the weights in model and prune the lowest α percent . The mask is obtained by setting the remaining indices of parameters to be 1 . Joint Training Once we get masks for all language pairs we further continue to train θ0 with languagegrouped batching and updating . First we create random batches of bilingual sentence pairs where each batch contains only samples from one pair . This is different from the plain joint multilingual training where each batch can contain fully random sentence pairs from all languages . Specifically a batch is randomly drawn from the data . Second we evaluate the loss in Eq . 1 on the batch . During the step we only update the parameters in θ0 belonging to the indicated by . We iteratively update the parameters until convergence . In this way we still get a single final model θ that is able to translate all language directions . During the inference this model θ and its masks i 1 . . . N are used together to make predictions . For every given input sentence in language s and a target language t the forward inference step only uses the parameter θ to calculate model output . 4 Experiment Settings Datasets and Evaluation The experiments are conducted on IWSLT and WMT benchmarks . For IWSLT we collect 8 language pairs from IWSLT2014 whose size ranges from 89k to 169k . To simulate the scenarios of imbalanced datasets we collect 18 language pairs ranging from Gu 11k to Fr 37m from previous years WMT . The details of the datasets are listed in Appendix . We apply byte pair encoding BPE Sennrich et 2016 to preprocess multilingual sentences resulting in a vocabulary size of 30k for IWSLT and 64k for WMT . Besides we apply for IWSLT and WMT to balance the training data distribution with a temperature of T 2 and T 5 respectively . Similar to Lin et al . 2020 we divide the language pairs into 3 categories : 1M 1M and 10M and rich resource 10M . We perform multilingual translation throughout this paper and add special language tokens at both the source and the target side . In all our experiments we evaluate our model with commonly used standard testsets . For zeroshot where standard testsets for example of some language pairs are not available we use Zhang et 2020 testsets instead . We report tokenized BLEU as well as win ratio WR informing the proportion of language pairs we outperform the baseline . In translation we also report accuracy1 which is commonly used to measure the accuracy of translating into the right target language . Model Settings Considering the diversity of dataset volume we perform our experiments with variants of Transformer architecture . For IWSLT we adopt a smaller Transformer Transformersmall2 Wu et 2019 . For WMT we adopt 1https: langdetect with dff 1024 and nhead 4 296 Lang Fa Pl Ar He Size 89K 128k 140K 144K Baseline 29 LaSS Lang Nl De It Es Size 153K 160K 167K 169K Baseline LaSS Table 1: Results on IWLST dataset . Baseline denotes the multilingual baseline model . LaSS consistently outperforms multilingual baseline on all language pairs . We report the average BLEU of and within one language . Both the baseline and LaSS have the same number of parameters . and . The pruning rate α of IWSLT and WMT is and respectively . For simplicity we only report the highest BLEU from the best pruning rate and we also discuss the impact of different pruning rate on performance in . In Sec . 6 we discuss the relationship of performance and pruning rate . For more training details please refer to Appendix . 5 Experiment Results This section shows the efficacy and generalization of LaSS . Firstly we show that LaSS obtains consistent performance gains on IWSLT and WMT datasets with different Transformer architecture variants . Further we show that LaSS can easily generalize to new language pairs without losing the accuracy for previous language pairs . Finally we observe that LaSS can even improve translation obtaining performance gains by up to BLEU . Main Results Results on IWSLT We first show our results on IWSLT . As shown in Table 1 LaSS consistently outperforms the multilingual baseline on all language pairs confirming that using LaSS to alleviate parameter interference can help boost performance . Results on WMT To further verify the generalization of LaSS we also conduct experiments on 3 For details of the Transformer setting please refer to Vaswani et al . 2017 WMT where the dataset is more imbalanced across different language pairs . We adopt two different Transformer architecture variants Transformerbase and . As shown in Table 2 LaSS obtains consistent gains over multilingual baseline on WMT for both and . For LaSS achieves an average improvement of BLEU on 36 language pairs over baseline while for LaSS obtains BLEU improvement . We observe that with the dataset scale of language pairs increasing the improvements of BLEU and WR become larger suggesting that the language pairs with large scale dataset benefit more from LaSS than language pairs of low resource . This phenomenon is intuitive since rich resource dataset suffers more parameter interference than low resource dataset . We also find that the BLEU and WR gains obtained in are larger than that in . We attribute it to the more severe parameter interference for smaller models . For comparison we also include the results of LaSS with randomly initialized masks . Not surprising Random underperforms the baseline by a large margin since Random intensifies rather than alleviates the parameter interference . Generalization to New Language Pairs LaSS has shown its efficacy in the above section . A natural question arises that can LaSS adapt to a new language or language pair that it has not seen in training phase? In other words can LaSS generalize to other language pairs? In this section we show the generalization of LaSS in two settings . We firstly show that LaSS can easily adapt to new unseen languages to match bilingual models with training for only a few hundred steps while keeping the performance of the existing language pairs hardly dropping . Secondly we show that LaSS can also boost performance in translation scenario obtaining performance gains by up to BLEU . The model is trained on WMT dataset . and are both unseen language pairs . Extensibility to New Languages Previous works have studied the easy and rapid adaptation to a new task or language pair Bapna and Firat 2019 Rebuffi et 2017 . We show 297 Arch Setting Model Low Medium Rich All BLEU WR BLEU WR BLEU WR BLEU WR Baseline Random LaSS Baseline Random LaSS Table 2: Average and Win Ratio WR of WMT dataset on Low 1M Medium and Rich 10M resource dataset . Random denotes LaSS with random masks . LaSS obtains consistent gains for both and . that LaSS can also easily adapt to new unseen languages without dramatic drop for other existing languages . We distribute a new to each new language pair and train the with the specific language pair for fixed steps . In this way the new language pair will only update the corresponding parameters and it can alleviate the interference and catastrophic forgetting Kirkpatrick et 2016 to other language pairs . We verify the extensibility of LaSS on 4 language pairs . For LaSS as described in we first the multilingual base model and prune to obtain the specific mask for the new language pair . For both multilingual baseline and our method we train on only the specific language pair for fixed steps . Figure 2 shows the trend of BLEU score along with the training steps . We observe that 1 LaSS consistently outperforms the multilingual baseline model along with the training steps . LaSS reaches the bilingual model performance with fewer steps . 2 Besides the degradation of other language pairs is much smoother than the baseline . When reaching the bilingual baseline performance LaSS hardly drops on other language pairs while the multilingual baseline model dramatically drops by a large margin . We attribute the easy adaptation for specific languages to the language specific . LaSS only updates the corresponding parameters avoiding updating all parameters which will hurt the performance of other languages . Another benefit of updating corresponding parameters is its fast adaptation towards specific language pairs . translation is the translation between known languages that the model has never seen 0 250 500 750 1000 steps 10 20 30 BLEU Bilingual model LaSS baseline direction a 0 250 500 750 1000 steps 20 25 30 35 40 BLEU Bilingual model LaSS baseline direction b 0 250 500 750 1000 steps 0 5 10 15 20 25 BLEU Bilingual model LaSS baseline direction c 0 250 500 750 1000 steps 0 10 20 30 BLEU Bilingual model LaSS baseline direction d Figure 2: The trend of BLEU score of new extended language pairs and other existing language pairs along with the training steps on the specific language pair . Compared to multilingual baseline LaSS reaches the bilingual performance with fewer steps and only little performance degradation on other existing language pairs . together at training time and are both seen in training phase while is . It is the ultimate goal of Multilingual NMT and has been a common indicator to measure the model capability Johnson et 2017 Zhang et 2020 . One of the biggest challenges is the offtarget issue Zhang et 2020 which means that the model translates into a wrong target language . In previous experiments we apply specific masks to their corresponding language pairs . As the training dataset is masks are not available . We remedy it by merging two masks to create masks . For example We create mask by combining the encoder mask of and the 298 es it nl de pl ar fa he es it nl de pl ar fa he 47 48 49 50 51 a and es it nl de pl ar fa he es it nl de pl ar fa he 59 60 61 62 63 64 65 b and es it nl de pl ar fa he es it nl de pl ar fa he 48 50 52 54 56 c Figure 3: Mask similarity for language pairs within and within and yaxis and between and respectively . The mask similarity is positively correlated to the language family similarity . decoder mask of . We select 6 languages and evaluate translation in language pairs between each other . As shown in Table 3 surprisingly by directly applying masks LaSS obtains consistent gains over baselines in all language pairs for both BLEU and accuracy indicating that the superiority of LaSS in learning to bridge between languages . It is worth noting that for LaSS outperforms the baseline by BLEU reaching 32 BLEU . We also sample a few translation examples from to analyze why LaSS can help boost zeroshot More examples are listed in Appendix . As shown in Table 4 as well as translationlanguage accuracy in Table 3 we observe that the multilingual baseline has severe issue . As a counterpart LaSS significantly alleviates the issue translating into the right target language . We attribute the success of in to the language specific parameters as a strong signal apart from language indicator to the model to translate into the target language . 6 Analysis and Discussion In this section we conduct a set of analytic experiments to better understand the characteristics of language specific . We first measure the relationship between language specific subnetwork as well as its capacity and language family . Secondly we study how masks affect performance in scenario . Lastly we discuss the relationship between pruning rate α and performance . We conduct our analytic experiments on IWSLT dataset . For readers not familiar with language family and clustering Figure 4 is the hierarchical clustering according to language family . es it nl de pl ar fa he Romance Germanic Slavic Arabic Iranian Semitic Latin Latin Latin Latin Latin Arabic Arabic Hebrew Figure 4: Language clustering of 8 languages in IWSLT according to language family . Es Spanish It Italian De Germany Nl Dutch and Pl Polish are all European languages and written in Latin while Ar Arabic Fa Farsi and He Hebrew are similar languages . Mask similarity Language family Ideally similar languages should share more parameters since they share more language characteristics . Therefore a natural question arises: Does the model automatically capture the relationship of language family defined by human? We calculate the similarity of masks between language pairs to measure the relationship between language pairs . We define mask similarity as the number of 1 where two masks share divided by the number of 1 of the first mask: Sim M1 M2 kM1 M2k0 kM1k0 2 where represent L0 norm . Mask similarity reflects the degree of sharing among different language pairs . Figure 3 a and 3 b shows the mask similarity in and . We observe that for both and the mask similarity is positively correlated to the language family similarity . The color of grids in Figure is deeper between similar languages for example es and it while more shallow between dissimilar languages for example es and he . We also plot the similarity between and in Figure 3 c . We observe that unlike or the mask similarity does not correspond to language family similarity . We suspect that the mask similarity is determined by combination of source and target languages . That means that does not necessarily share more parameters with than . Where language specific capacity matters? To take a step further we study how model schedule language specific capacity across layers . Figure 5 299 Target Languages Fr Cs De Es Ru Zh BLEU ACC BLEU ACC BLEU ACC BLEU ACC BLEU ACC BLEU ACC Source Languages Fr baseline LaSS Cs baseline LaSS De baseline LaSS Es baseline LaSS Ru baseline LaSS Zh baseline LaSS 18 Table 3: BLEU score and Accuracy ACC in percentage of translation for multilingual baseline and LaSS . LaSS outperforms the multilingual baseline on both BLEU and ACC by a large margin for most language pairs . Low accuracy indicates severe translation . Src La production annuelle d acier était le symbole incontesté de la vigueur économique des nations . Ref 钢的年产量是国家经济实力的重要象征 Baseline Annual steel production was the undisputed symbol of nations economic strength . LaSS 年度钢铁生产是各国经济活力的无可争 辩的象征 . Src De l avis de ma délégation donc l ONU devrait élargir ces activités de la faon suivante . Ref 因此 我国代表团认为 联合国现在应该 Baseline 因此 in my delegation s view the United Nations should expand these activities in the following manner . LaSS 因此 我国代表团认为 联合国应该扩大 这些活动 如下 . Table 4: Case Study . The multilingual baseline suffers from severe issue while LaSS greatly alleviates the issue . shows the similarity of different components on the encoder and decoder side along with the increase of layer . More concretely we plot query key value on the attention and layer on the . We observe that a On both the encoder and decoder side the model tends to distribute more language specific components on the top and bottom layers rather than the middle ones . This phenomenon is intuitive . The bottom layers deal more with embedding which is language specific while the top layers are near the output layer which is also language specific . b For layer the model tends to distribute more language specific capacity on the middle layers for the encoder while distribute more language specific capacity in the decoder for the top layers . How masks affect? In we show that simply applying masks can boost performance . We conduct experiments to analyze how masks affect zeroperformance . Concretely we take as an example replacing the encoder or decoder mask with another language mask respectively . As shown in Table 5 we observe that replacing the encoder mask with other languages causes only littler performance drop while replacing the decoder mask causes dramatic performance drop . It suggests that the decoder mask is the key ingredient of performance improvement . About Sparsity To better understand the pruning rate we plot the performance along with the increase of pruning 300 0 1 2 3 4 5 layer 45 50 55 60 similarity all fc a Encoder 0 1 2 3 4 5 layer 45 50 55 60 similarity all fc b Decoder Figure 5: The mask similarity of different components attention layer and layer on the encoder and decoder side along with the increase of layer . The model tends to distribute more language specific capacity on the top and bottom layers . Fr Fr Cs De Es Ru Zh X Zh Fr Cs De Es Ru Zh Table 5: Performance of applying or mask to testset . Replacing encoder mask causes only little performance drop while replacing decoder mask causes dramatic performance drop . rate in Figure 6 . For WMT the best choice for α is for both and while for IWSLT the best α lies between . The results are consistent with our intuition that large scale training data need a smaller pruning rate to keep the model capacity . Therefore we suggest tuning α based on both the dataset and model size . For large datasets such as WMT setting a smaller α is better while a larger α will slightly decrease the performance . less than BLEU score . For small datasets like IWSLT setting a larger α may yield better performance . 7 Conclusion In this paper we propose to learn LanguageSpecific LaSS for multilingual alpha 26 27 BLEU a IWSLT Big alpha Base b WMT Figure 6: BLEU score along with the increase of pruning rate α . Large α indicates small . Small dataset requires a larger α to yield better performance . IWSLT uses and WMT uses and . NMT . Extensive experiments on IWSLT and WMT have shown that LaSS is able to alleviate parameter interference and boost performance . Further LaSS can generalize well to new language pairs by training with a few hundred steps while keeping the performance of existing language pairs . Surprisingly in translation LaSS surpasses the multilingual baseline by up to BLEU . Extensive analytic experiments are conducted to understand the characteristics of language specific . Future work includes designing a more dedicated training strategy and incorporating the insight we gain from analysis to design a further improved LaSS.