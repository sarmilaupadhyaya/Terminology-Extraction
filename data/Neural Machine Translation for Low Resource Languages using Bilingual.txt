Abstract Resources for the languages are scarce and this paper addresses this problem in the context of machine translation by automatically extracting parallel sentence pairs from the multilingual articles available on the Internet . In this paper we have used an Siamese bidirectional recurrent neural network to generate parallel sentences from comparable multilingual articles in Wikipedia . Subsequently we have showed that using the harvested dataset improved BLEU scores on both NMT and SMT systems for the language pairs: Hindi and when compared to training exclusively on the limited bilingual corpora collected for these language pairs . 1 Introduction Both neural and statistical machine translation approaches are highly reliant on the availability of large amounts of data and are known to perform poorly in low resource settings . Recent crowdsourcing efforts and workshops on machine translation have resulted in small amounts of parallel texts for building viable machine translation systems for pairs Post et 2012 . But they have been shown to suffer from low accuracy incorrect translation and low coverage high rates due to insufficient training data . In this project we try to address the high OOV rates in machine translation systems by leveraging the increasing amount of multilingual content available on the Internet for enriching the bilingual lexicon . Comparable corpora such as Wikipedia are collections of but multilingual documents which are rich resources for extracting parallel sentences from . For example Figure 1 shows that there are equivalent sentences on the page about Donald Trump in Tamil Language ISO Bilingual Wiki articles Curated sent . pairs Urdu ur Hindi hi Tamil ta Telugu te Bengali bn Malayalam ml Table 1: Number of bilingual articles in Wikipedia against the number of parallel sentences in the largest corpora available . and English and the phrase alignment for an example sentence is shown in Table 2 . Table 1 shows that there are at least tens of thousands of bilingual articles on Wikipedia which could potentially have at least as many parallel sentences that could be mined to address the scarcity of parallel sentences as indicated in column 2 which shows the number of sentencepairs in the largest available bilingual corpora for . As shown by Irvine and CallisonBurch 2013 the illustrated data sparsity can be addressed by extending the scarce parallel with those automatically extracted from Wikipedia and thereby improving the performance of statistical machine translation systems . In this paper we will propose a neural approach to parallel sentence extraction and compare the BLEU scores of machine translation systems with and without the use of the extracted sentence pairs to justify the effectiveness of this method . Compared to previous approaches which require spe1: http: : http: : https : 112 Figure 1: A comparison of nearly parallel sentences from bilingual Wikipedia articles about Donald Trump in English and Tamil . Table 2: pairs from Fig 1 cialized from document structure or significant amount of features the neural model for extracting parallel sentences is learned using only a small bootstrap set of parallel sentence pairs . 2 Related Work A lot of work has been done on the problem of automatic sentence alignment from comparable corpora but a majority of them and Schwenk 2009 Irvine and 2013 Yasuda and Sumita 2008 use a translation system as a precursor to ranking the candidate sentence pairs which the low resource language pairs are not at the luxury of having or use statistical machine learning approaches where a Maximum Entropy classifier is used that relies on surface level features such as word overlap in order to obtain parallel sentence pairs Munteanu and Marcu 2005 . However the deep neural network model used in our paper is probably the first of its kind which does not need any feature engineering and also does not need a translation system . Munteanu and Marcu 2005 proposed a parallel sentence extraction system which used comparable corpora from newspaper articles to extract the parallel sentence pairs . In this procedure a maximum entropy classifier is designed for all sentence pairs possible from the Cartesian product of a pair of documents and passed through a ratio filter in order to obtain candidate sentence pairs . SMT systems were trained on the extracted sentence pairs using the additional features from the comparable corpora like distortion and position of current and previously aligned sentences . This resulted in a state of the art approach with respect to the translation performance of low resource languages . Similar to our proposed approach Cedeno et al . 2015 showed how using parallel documents from Wikipedia for domain specific alignment would improve translation quality of SMT systems on data . In this method 113 similarity between all pairs of sentences with different text similarity measures are estimated . The issue of domain definition is overcome by the use of IR techniques which use the characteristic vocabulary of the domain to query a Lucene search engine over the entire corpus . The candidate sentences are defined based on word overlap and the decision whether a sentence pair is parallel or not using the maximum entropy classifier . The difference in the BLEU scores between out of domain and translation is proved clearly using the word embeddings from characteristic vocabulary extracted using the extracted additional bitexts . and Schwenk 2009 extract parallel sentences without the use of a classifier . Target language candidate sentences are found using the translation of source side comparable corpora . Sentence tail removal is used to strip the tail parts of sentence pairs which differ only at the end . This along with the use of parallel sentences enhanced the BLEU score and helped to determine if the translated source sentence and candidate target sentence are parallel by measuring the word and translation error rate . This method succeeds in eliminating the need for domain specific text by using the target side as a source of candidate sentences . However this approach is not feasible if there isn t a good source side translation system to begin with like in our case . Yet another approach which uses an existing translation system to extract parallel sentences from comparable documents was proposed by Yasuda and Sumita 2008 . They describe a framework for machine translation using multilingual Wikipedia articles . The parallel corpus is assembled iteratively by using a statistical machine translation system trained on a preliminary corpus to score BLEU scores . After filtering out the unaligned pairs based on the MT evaluation metric the SMT is retrained on the filtered pairs . 3 Approach In this section we will describe the entire pipeline depicted in Figure 2 which is involved in training a parallel sentence extraction system and also to infer and decode from bilingual article pages collected from Wikipedia . Bootstrap Dataset The parallel sentence extraction system needs a sentence aligned corpus which has been curated . These sentences were used as the ground truth pairs when we trained the model to classify parallel sentence pair from pairs . Negative Sampling The binary classifier described in the next section assigns a translation probability score to a given sentence pair after learning from examples of translations and negative examples of nontranslation pairs . For this we make a simplistic assumption that the parallel sentence pairs found in the bootstrap dataset are unique combinations which fail being translations of each other when we randomly pick a sentence from both the sets . Thus there might be cases of false negatives due to the reliance on unsupervised random sampling for generation of negative labels . Therefore at the beginning of every epoch we randomly sample m negative sentences of the target language for every source sentence . From a few experiments and also from the literature we converged on m 7 to be performing the best given our compute constraints . Model Here we describe the neural network architecture as shown in Gregoire and Langlais 2017 where the network learns to estimate the probability that the sentences in a given sentence pair are translations of each other p yi S i s T i where s S i is the candidate source sentence in the given pair and s T i is the candidate target sentence . Training As illustrated in Figure 2 d the architecture uses a siamese network Bromley et 1994 consisting of a bidirectional RNN Schuster and Paliwal 1997 sentence encoder with recurrent units such as long memory units or LSTMs Hochreiter and Schmidhuber 1997 and gated recurrent units or GRUs Cho et 2014 learning a vector representation for the source and target sentences and the probability of any given pair of sentences being translations of each other . For seq2seq architectures especially in translation we have found the that the recommended recurrent unit is GRU and all our experiments use this over LSTM . 114 Figure 2: Architecture for the parallel sentence extraction system showing training and inference pipelines . EN English TA Tamil The forward RNN reads the sentence and updates its recurrent state from the first token until the last one to create a continuous vector representation of the sentence . The backward RNN processes the sentence in reverse . In our experiments we use the concatenation of the last recurrent state in both directions as a final representation h S i S i N h S wS i t E S wk 1 S i t φ S i wS i t 2 h S i t φ h S i wS i t 3 where φ is the gated recurrent unit GRU . After both source and target sentences have been encoded we capture their matching information by using their product and absolute difference . We estimate the probability that the sentences are translations of each other by feeding the matching vectors into fully connected layers: h 1 i h S i h T i 4 h 2 i S i h T i 5 hi tanh W 1 h 1 i W 2 h 2 i b 6 p yi σ W 3 hi c 7 where σ is the sigmoid function W 1 W 2 W 3 b and c are model parameters . The model is trained by minimizing the cross entropy of our labeled sentence pairs: L n Xm yi log σ W 3 hi c 1 yi log 1 σ W 3 hi c 8 where n is the number of source sentences and m is the number of candidate target sentences being considered . Inference For prediction a sentence pair is classified as parallel if the probability score is greater than or equal to a decision threshold ρ that we need to fix . We found that to get high precision sentence pairs we 115 Table 3: A sample of parallel sentences extracted from Wiki articles . The translation of the extracted Tamil sentence in English is also provided . Translation probability corresponds to our model s score of how likely the sentences are translations of each other as calculated in Equation had to use ρ and if we were able to sacrifice some precision for recall a lower ρ of would work in the favor of reducing OOV rates . yˆi 1 if p yi ρ 0 otherwise 9 4 Experiments Dataset We experimented with two language pairs: English Hindi and English Tamil . The parallel sentence extraction systems for both and were trained using the architecture described in on the following bootstrap set of parallel corpora: An parallel corpus Ramasamy et 2014 containing a total of 169 871 sentence pairs composed of 3 984 038 English Tokens and 2 776 397 Tamil Tokens . An parallel corpus Kunchukuttan et 2017 containing a total of 1 492 827 sentence pairs from which a set of 200 000 sentence pairs were picked randomly . Subsequently we extracted parallel sentences using the trained model and parallel articles collected from Wikipedia2 . There were 67 449 bilin2Tamil: Hindi: gual and 58 802 titles on the Wikimedia dumps collected in December 2017 . Evaluation Metrics For the evaluation of the performance of our sentence extraction models we looked at a few sentences manually and have done a qualitative analysis as there was no gold standard evaluation set for sentences extracted from Wikipedia . In Table 3 we can see the qualitative accuracy for some parallel sentences extracted from Tamil . The sentences extracted from Tamil have been translated to English using Google Translate so as to facilitate a comparison with the sentences extracted from English . For the statistical machine translation and neural machine translation evaluation we use the BLEU score Papineni et 2002 as an evaluation metric computed using the script from Moses Koehn et 2007 . Sentence Alignment Figures 3a shows the number of high precision sentences that were extracted at ρ without greedy decoding . Greedy decoding could be thought of as sampling without replacement where a sentence that s already been extracted on one side of the extraction system is precluded from being considered again . Hence the number of sentences without greedy decoding are of an order of magnitude higher than with decoding as can be seen in Figure 3b . 116 a Without greedy decoding b With greedy decoding Figure 3: Number of parallel sentences extracted from parallel Wikipedia article pairs using different thresholds and decoding methods Training Data Model BLEU Sents IIT Bombay SMT Wiki Extracted SMT IIT Bombay NMT Wiki Extracted NMT Ramasamy SMT Wiki Extracted SMT Ramasamy NMT Wiki Extracted NMT Table 4: BLEU score results for and Machine Translation We evaluated the quality of the extracted parallel sentence pairs by performing machine translation experiments on the augmented parallel corpus . SMT As the dataset for training the machine translation systems we used high precision sentences extracted with greedy decoding by ranking the on their translation probabilities . SMT systems were trained using Moses Koehn et 2007 . We used the heuristic for extracting phrases lexicalised reordering and Batch MIRA Cherry and Foster 2012 for tuning the default parameters on Moses . We trained language models with smoothing using KenLM Heafield et 2013 . With these parameters we trained SMT systems for and language pairs with and without the use of extracted parallel sentence pairs . NMT For training neural machine translation models we used the TensorFlow Abadi et 2016 implementation of OpenNMT Klein et al . with transformer architecture Vaswani et 2017 . The BLEU scores for the NMT models were higher than for SMT models for both ta and pairs as can be seen in Table 4 . 5 Conclusion In this paper we evaluated the benefits of using a neural network procedure to extract parallel sentences . Unlike traditional translation systems which make use of classification procedures this method requires just a parallel corpus to extract parallel sentence pairs using a Siamese BiRNN encoder using GRU as the activation function . This method is extremely beneficial for translating language pairs with very little parallel corpora . These parallel sentences facilitate significant improvement in machine translation quality when compared to a generic system as has been shown in our results . The experiments are shown for and language pairs . Our model achieved a marked percentage increase in the BLEU score for both and language 117 pairs . We demonstrated a percentage increase in BLEU scores of and for and pairs respectively due to the use of parallelsentence pairs extracted from comparable corpora using the neural architecture . As a to this work we would be comparing our framework against other sentence alignment methods described in Resnik and Smith 2003 Ayan and Dorr 2006 Rosti et 2007 and Smith et 2010 . It has also been interesting to note that the 2018 edition of the Workshop on Machine Translation WMT has released a new shared task called Parallel Corpus Filtering 3 where participants develop methods to filter a given noisy parallel corpus crawled from the web to a smaller size of high quality sentence pairs . This would be the perfect avenue to test the efficacy of our neural network based approach of extracting parallel sentences from unaligned corpora.