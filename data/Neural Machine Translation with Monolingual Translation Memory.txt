Abstract Prior work has proved that Translation memory TM can boost the performance of Neural Machine Translation NMT . In contrast to existing work that uses bilingual corpus as TM and employs similarity search for memory retrieval we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a crosslingual manner . Our framework has unique advantages . First the memory retriever allows abundant monolingual data to be TM . Second the memory retriever and NMT model can be jointly optimized for the ultimate translation goal . Experiments show that the proposed method obtains substantial improvements . Remarkably it even outperforms strong NMT baselines using bilingual TM . Owning to the ability to leverage monolingual data our model also demonstrates effectiveness in and domain adaptation scenarios . 1 Introduction Augmenting parametric neural network models with memory Khandelwal et 2019 Guu et 2020 Lewis et 2020a b has recently emerged as a promising direction to relieve the demand for model size Devlin et 2019 Radford et 2019 Brown et 2020 . For the task of Machine Translation MT inspired by the Translation CAT tools by professional human translators for increasing productivity for decades Yamada 2011 the usefulness of Translation Memory TM has long been recognized Huang et 2021 . In general TM is a database that stores pairs of source text and its corresponding translations . Like for human work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region China Project Code : 14200719 . Correspondence to Yan Wang . translation early work Koehn and Senellart 2010 He et 2010 Utiyama et 2011 Wang et 2013 inter alia presents translations for similar source input to statistical translation models as additional cues . Recent work has confirmed that TM can help Neural Machine Translation NMT models as well . In a similar spirit to prior work NMT models do not discard the training corpus after training but keep exploiting it in the test time . These models perform translation in two stages: In the retrieval stage a retriever searches for nearest neighbors pairs from the training corpus based on similarity such as lexical overlaps Gu et 2018 Zhang et 2018 Xia et 2019 matches Cao and Xiong 2018 or a hybrid Bulte and Tezcan 2019 Xu et 2020 In the generation stage the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks Gu et 2018 Cao and Xiong 2018 Xia et 2019 He et 2021 or directly concatenating them to the source input Bulte and Tezcan 2019 Xu et 2020 or biasing the word distribution during decoding Zhang et 2018 . Most recently Khandelwal et al . 2020 propose a nearest neighbor search using complete translation context both the input and prefix . Despite their differences we identify two major limitations in previous research . First the translation memory has to be a bilingual corpus consisting of aligned pairs . This requirement limits the memory bank to bilingual pairs and precludes the use of abundant monolingual data which can be especially helpful for scenarios . Second the memory retriever is not optimized and lacks for the ability to adapt to specific downstream NMT models . Concretely current retrieval mechanisms BM25 7308 are generic similarity search adopting a simple heuristic . That is the more a source sentence overlaps with the input sentence the more likely its translation pieces will appear in the correct translation . Although this observation is true the most similar one does not necessarily serve the best for NMT models . Ideally the retrieval metric would be learned from the data in a way: we wish to consider a memory only if it can indeed boost the quality of final translation . In this work we propose to augment NMT models with monolingual TM and a learnable crosslingual memory retriever . Specifically we align sentences and the corresponding targetside translations in a latent vector space using a simple framework Bromley et 1993 such that the distance in the latent space yields a score function for retrieval . As a result our memory retriever directly connects the dots between the input and translations enabling monolingual data in the target language to be used alone as TM . Before running each translation the memory retriever selects the memories from a large collection of monolingual sentences TM which may include but are not limited to the target side of training corpus and then the downstream NMT model attends over those memories to help inform its translation . We design the memory retriever with differentiable neural networks . To unify the memory retriever and its downstream NMT model into a learnable whole the retrieval scores are used to bias the attention scores to the most useful retrieved memories . In this way our memory retrieval can be optimized for the translation objective: a retrieval that improves the golden translation s likelihood is helpful and should be rewarded while an uninformative retrieval should be penalized . One challenge for training our proposed framework is that when starting from random initialization the retrieved memories will likely be totally unrelated to the input . Since the memory retriever does not exert positive influence on NMT model s performance it cannot receive a meaningful gradient and improve . This causes the NMT model to learn to ignore all retrieved memories . To avoid this problem we propose to the retrieval model using two tasks . Experiments show that 1 Our model leads to significant improvements over baseline NMT model even outperforming strong TMaugmented baselines . This is remarkable given that previous models rely on bilingual TM while our model only exploits the target side . 2 Our model can substantially boost translation quality in scenarios by utilizing extra monolingual TM that is not present in training pairs . 3 Our model gains a strong transferability by monolingual memory . 2 Related Work NMT This work contributes primarily to the research line of Translation Memory TM augmented Neural Machine Translation NMT . Feng et al . 2017 augmented NMT with a bilingual dictionary to tackle infrequent word translation . Gu et al . 2018 proposed a model that retrieves examples similar to the test source sentence and encodes retrieved pairs with keyvalue memory networks . Cao and Xiong 2018 Cao et al . 2019 used a gating mechanism to balance the impact of the translation memory . Zhang et al . 2018 proposed guiding models by retrieving and the probabilities of retrieved . Bulte and Tezcan 2019 and Xu et al . 2020 used with translation memories and augment source sequences with retrieved pairs . Xia et al . 2019 directly ignored the source side of a TM and packed the target side into a compact graph . Khandelwal et al . 2020 ran existing translation model on large corpora and recorded all hidden states for later nearest neighbor search at each decoding step which is very . The distinctions between our work and prior work are obvious : 1 The TM in our framework is a collection of monolingual sentences rather than bilingual sentence pairs 2 We use learnable retrieval rather than generic retrieval mechanisms . Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks . One typical use is to retrieve supporting evidence for question answering Chen et 2017 Lee et 2019 Karpukhin et 2020 . Recently generation has gained increasing interest in a wide range of text generation tasks such as language modeling Guu et 2018 Khandelwal et 2019 Guu et 2020 dialogue response generation Weston et 2018 Wu et 2019 Cai et 7309 source sentence encoder target sentence encoder Translation Memory dense index MIPS Source Encoder Memory Encoder 23 Decoder Retrieval Model Output y Translation Model bias attention Input x relevant TM relevance scores Figure 1: Overall framework . For an input sentence x in the source language the retrieval model uses Maximum Inner Product Search MIPS to find the TM sentences zi M in the target language . The translation model takes zi M and corresponding relevance scores f x zi M as input and generate the translation y . 2019a b code generation Hashimoto et 2018 and other generation Lewis et 2020b . It can be observed that there is a shift from using search engines to learning retrievers . Our work draws inspiration from this line of research . However generation has so far been mainly investigated for knowledge retrieval in the same language . The memory retrieval in this work is more challenging due to the setting . NMT using Monolingual Data To our knowledge the integration of monolingual data for NMT was first investigated by Gulcehre et al . 2015 who separately trained language models using monolingual data and then integrated them during decoding either through the beam or by feeding the hidden state of the language model to the NMT model . Jean et al . 2015 also explored the NMT output with a language model . Another successful method for leveraging monolingual data in NMT is Sennrich et 2016 Fadaee et 2017 Edunov et 2018 He et 2016 where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences . Recent studies Jiao et 2021 He et 2019 showed that where the synthetic parallel sentences are created by translating monolingual sentences in the source language is also helpful . Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without see . 3 Proposed Approach We start by formalizing the translation task as a process in . Then in we describe the model design for the crosslingual memory retrieval model . In we describe the model design for the translation model . Lastly we show how to optimize the two components jointly using standard maximum likelihood training in and therein we address the problem via . Overview Our approach decomposes the whole translation processing into two steps: retrieve then generate . The overall framework is illustrated in Figure 1 . The Translation Memory TM in our approach is a collection of sentences in the target language Z . Given an input x in the source language the retrieval model first selects a number of possibly helpful sentences zi M from Z where M according to a relevance function f x zi . Then the translation model conditions on both the retrieved set zi f x zi M and the original input x to generate the output y using a probabilistic model p z1 f x z1 . . . zM f x zM . Note that the relevance scores f x zi M are also part of the input to the translation model encouraging the translation model to focus more on more relevant sentences . During training maximizing the likelihood of the translation references improves both the translation model and the retrieval model . Retrieval Model The retrieval model is responsible for selecting the most relevant sentences for a source sentence from a large monolingual TM . This could involve measuring the relevance scores between the source sentence and millions of candidate target sentences which poses a serious computational challenge . To address this we implement the retrieval model using a simple framework Bromley et 1993 such that the selection of the most 7310 relevant sentences can be reduced to Maximum Inner Product Search MIPS . With performant data structures and search algorithms Shrivastava and Li 2014 Malkov and Yashunin 2018 the retrieval can be done efficiently . Specifically we define the relevance score f x z between the source sentence x and the candidate sentence z as the dot product of their dense vector representations: f x z Esrc x TEtgt z where Esrc and Etgt are the source sentence encoder and the target sentence encoder that map x and z to vectors respectively . We implement the two sentence encoders using two independent Transformers Vaswani et 2017 . For an input sentence we prepend the BOS token to its token sequence and then feed it into a Transformer . We take the representation at the BOS token as the output denoted Trans src tgt x z and perform a linear projection W src tgt to reduce the dimensionality of the vector . Finally we normalize the vectors to regulate the range of relevance scores . Esrc x normalize WsrcTranssrc x Etgt z normalize WtgtTranstgt z The normalized vectors have zero means and unit lengths . Therefore the relevance scores always fall in the interval 1 . We let θ denote all parameters associated with the retrieval model . In practice the dense representations of all sentences in TM can be and indexed using FAISS Johnson et 2019 an toolkit for efficient vector search . Given a source sentence x in hand we compute the vector representation vx Esrc x and retrieve the top M target sentences with vectors closest to vx . Translation Model Given a source sentence x a small set of relevant TM zi M and relevance scores f x zi M the translation model defines the conditional probability p z1 f x z1 . . . zM f x zM . Our translation model is built upon the standard NMT model Bahdanau et 2015 Vaswani et 2017: the source encoder transforms the source sentence x into dense vector representations . The decoder generates an output sequence y in an fashion . At each time step t the decoder attends over both previously generated sequence y1: and the output of the source encoder generating a hidden state ht . The hidden state ht is then converted to probabilities through a linear projection followed by softmax function Pv softmax Wvht bv . To accommodate the extra memory input we extend the standard NMT framework with a memory encoder and allow crossattention from the decoder to the memory encoder . Specifically the memory encoder encodes each TM sentence zi individually resulting in a set of contextualized token embeddings zi k Li where Li is the length of the token sequence zi . We compute a cross attention over all TM sentences: αij exp ht TWmzi j PM PLi exp ht TWmzi k 1 ct Wc X M X Li αijzi j where αij is the attention score of the token in zi ct is a weighted combination of memory embeddings and Wm and Wc are trainable matrices . The cross attention is used twice during decoding . First the decoder s hidden state ht is updated by a weighted sum of memory embeddings ht ht ct . Second we consider each attention score as a probability of copying the corresponding token Gu et 2016 See et 2017 . Formally the probabilities are computed as: p yt 1 λt Pv yt λt X M X Li where 1 is the indicator function and λt is a gating variable computed by another network λt g ht ct . Inspired by Lewis et al . 2020a to enable the gradient flow from the translation output to the retrieval model we bias the attention scores with the relevance scores rewriting Eq . 1 as: αij exp ht TWmzi j βf x zi PM PLi exp ht TWmzi k βf x zi 2 where β is a trainable scalar that controls the weight of the relevance scores . We let φ denote all parameters associated with the translation model . Training We optimize the model parameters θ and φ using stochastic gradient descent on 7311 the negative loss function log p y z1 f x z1 . . . zM f x zM where y refers to the reference translation . As implied by Eq . 2 TM sentences that improve the likelihood of reference translations should receive higher attention scores and higher relevance scores so gradient descent on the loss function will improve the quality of the retrieval model as well . However if the retrieval model starts from random initialization all top TM sentences zi will likely be unrelated to x or equally useless . This leads to a problem that the retrieval model cannot receive meaningful gradients and improve and the translation model will learn to completely ignore the TM input . To avoid this problem we propose two crossalignment tasks to the retrieval model . The first task is . This task aims to find the right translation for a source sentence given a set of other translations which is directly related to our retrieval function . Concretely We sample B pairs from the training corpus at each training step . Let X and Z be the B d matrix of the source and target vectors encoded by Esrc and Etgt respectively . S XZT is a B matrix of relevance scores where each row corresponds to a source sentence and each column corresponds to a target sentence . Any Xi Zj pair should be aligned when i j and should not otherwise . The objective is to maximize the scores along the diagonal of the matrix and henceforth reduce the values in other entries . The loss function can be written as: L i snt exp Sii exp Sii P exp Sij . The second task is which aims to predict the tokens in the target language given the source sentence representation and vice versa . Formally we use losses: L i tok X log p X log p where Xi Yi represents the set of tokens in the source target sentence and the token probabilities are computed by a linear projection followed by the softmax function . The joint loss for is 1 B PB L i snt L i tok . In practice we find that both the and objectives are crucial for achieving superior performance . Dataset Train Pairs Dev Pairs Test Pairs Table 1: Data statistics for the corpus . Asynchronous Index Refresh To employ fast MIPS we must Etgt z for every z Z and build an index . However the index cannot remain consistent with the running model during training as θ will be updated over time . One straightforward solution to fix the parameters of Etgt after the described above and only the parameters of Esrc . However this may hurt performance since Etgt cannot adapt to the translation objective . Another solution is to asynchronously refresh the index by and all TM sentences at regular intervals . The index is slightly outdated between refreshes however we use fresh Etgt in gradient estimate . We explore both options in our experiments . 4 Experiments We experiment with the proposed approach in three settings : 1 the conventional setting where the available TM is limited to the bilingual training corpus 2 the setting where bilingual training pairs are scarce but extra monolingual data is exploited as additional TM and 3 nonparametric domain adaptation using monolingual TM . Note that existing NMT models are only applicable to the first setting the last two settings only become possible with our proposed model . We use BLEU score Papineni et 2002 as the evaluation metric . Implementation Details We build our model using Transformer blocks with the same configuration as Transformer Base Vaswani et 2017 8 attention heads 512 dimensional hidden state and 2048 dimensional state . The number of Transformer blocks is 3 for the retrieval model 4 for the memory encoder in the translation model and 6 for the architecture in the translation model . We retrieve the top 5 TM sentences . The FAISS index code is IVF1024 HNSW32 SQ8 and the search depth is 64 . We follow the learning rate schedule dropout and label smoothing settings described in Vaswani et al . 2017 . We use Adam optimizer Kingma and Ba 2014 and train models with up to 100K 7312 System Retriever Dev Test Dev Test Dev Test Dev Test Existing NMT systems Gu et al . 2018 source similarity Zhang et al . 2018 source similarity Xia et al . 2019 source similarity Our NMT systems 1 this work None 2 source similarity 3 fixed 4 fixed Etgt 5 Table 2: Experimental results BLEU scores on four translation tasks . are from Xia et al . 2019 . two variants of our method model 4 and model 5 are significantly better than other baselines with tested by bootstrap Koehn 2004 . steps throughout all experiments . When trained with asynchronous index refresh the interval is 3K training Conventional Experiments Following prior work in NMT we first conduct experiments in a setting where the bilingual training corpus is the only source for TM . Data We use the corpus Steinberger et 2006 for our experiments . The corpus contains the total body of European Union EU law applicable to the EU member states . This corpus was also used by Gu et al . 2018 Zhang et al . 2018 Xia et al . 2019 and we managed to get the datasets originally preprocessed by Gu et al . 2018 making it possible to fairly compare our results with previously reported BLEU scores . Specifically we select four translation directions namely and for evaluation . Detailed data statistics are shown in Table 1 . Models To study the effect of each model component we implement a series of model variants model 1 to 5 in Table 2 . 1 . NMT without TM . To measure the help from TM we remove the model components related to TM including the retrieval model and the memory encoder and only employ the architecture for NMT . The resulted model is equivalent to the Transformer Base model Vaswani et 2017 . 1Our code is released at https: . 2 . NMT using source similarity search . To isolate the effect of architectural changes in NMT models we replace our memory retriever with traditional similarity search . Specifically we use the fuzzy match system used in Xia et al . 2019 and many others which is based on BM25 and edit distance . 3 . NMT using crosslingual retriever . To study the effect of optimization of the retrieval model we the retrieval model using the tasks introduced in and keep it fixed in the following NMT training . 4 . Our full model using a fixed TM index After we fix the parameter of Etgt during NMT training . 5 . Our full model trained with asynchronous index refresh . Results The results of the above models are presented in Table 2 . We have the following observations : 1 Our full model trained with asynchronous index refresh model 5 delivers the best performance on test sets across all four translation tasks outperforming the baseline model 1 by BLEU points in average and up to BLEU points . This result confirms that monolingual TM can boost NMT performance 2 The learning of the retriever model is the key for substantial performance improvement . We can see that using a fixed crosslingual retriever only gives moderate test performance Esrc and fixing Etgt significantly boosts the performance and both Esrc 7313 Figure 2: Test results with bilingual pairs upper and bilingual pairs lower across different TM sizes . and Etgt leads to the strongest performance model 5 model 4 model 3 3 retrieval model 4 and model 5 can obtain better results than that of the source similarity search model 2 . This is remarkable since the crosslingual retrieval only requires monolingual TM while the source similarity search relies on bilingual TM . We attribute the success again to the adaptability of our retriever . This is manifested by the fact that model 3 even slightly underperforms model 2 in some of translation tasks . Contrast to Previous Bilingual TM Systems We also compare our results with the best previously reported We can see that our results significantly outperform previous arts . Notably our best model model 5 surpasses the best reported model Xia et 2019 by BLEU points in average and up to BLEU points . This result verifies the effectiveness of our proposed models . In fact we can see that our translation model using traditional similarity search model 2 already outperforms the best previously reported results which reveals that the architectural design of our translation model is surprisingly effective despite its simplicity . 2 Some recent work used different datasets other than JRCAcquis with unspecified data split which makes it hard to make an exhaustive comparison . However note that our inhouse baseline model 2 is quite strong . Scenarios One most unique characteristic of our proposed model is that it uses monolingual TM . This motivates us to conduct experiments in scenarios where we use extra monolingual data in the target language to boost translation quality . Data We create scenarios by randomly partitioning each training set in corpus into four subsets of equal size . We set up two series of experiments : 1 We only use the bilinguals pairs in the first subset and gradually enlarge the TM by including more monolingual data in other subsets . 2 Similar to 1 but we instead use the bilingual pairs in the first two subsets . Models As shown in the model trained with asynchronous index refresh model 5 is slightly better than the model using fixed Etgt model 4 however the computational cost of training model 5 is much bigger . For simplicity and environmental consideration we only test model 4 in lowresource scenarios . Nevertheless we note there are still two modeling choices : 1 train the model once with the TM limited to training pairs and only enlarge the TM during testing 2 the model with every enlarged TM . Note that when using the first choice the model may retrieve a TM sentence that has never been seen during training . To measure the performance improvements from additional monolingual TM we also include a Transformer Base baseline model 1 denoted as 7314 Data Model dev test dev test dev test dev test bilingual monolingual Ours BT bilingual monolingual Ours BT Table 3: Comparison with BT . Medical Law IT Koran Subtitle Avg . Avg . Bilingual Pairs Monolingual Sents Using Bilingual Pairs Only Transformer Base Ours Monolingual Memory Ours Ours Table 4: Test results on domain adaptation . base and a bilingual TM baseline model 2 . Results Figure 2 shows the main results on the test sets . The general patterns are consistent across all experiments: the larger the TM becomes the better translation performance the model achieves . When using all available monolingual data the translation quality is boosted significantly . Interestingly the performance of models without retraining is comparable to if not better than those with . We also observe that when the training pairs are very scarce only bilingual pairs are available a small size of TM even hurts the model performance . The reason could be overfitting . We speculate that better results would be obtained by tuning the model according to different TM sizes . Contrast to We compare our models with BT Sennrich et 2016 a popular way of utilizing monolingual data for NMT . We train a Transformer Base model using bilingual pairs and use the resultant model to translate monolingual sentences to obtain additional synthetic parallel data . As shown in Table 3 our method performs better than BT with bilingual pairs but performs worse with bilingual pairs . Interestingly the combination of BT and our method yields significant further gains which demonstrates that our method is not only orthogonal but also complementary to BT . Domain Adaptation Lastly the plug and play property of TM further motivates us to domain adaptation where we adapt a single model to a specific domain by using monolingual TM . Data To simulate a diverse setting we use the data splits in Aharoni and Goldberg 2020 originally collected by Koehn and Knowles 2017 . It includes parallel data for sets in five domains: Medical Law IT Koran and Subtitles . Similar to the experiments in we only use one fourth of bilingual pairs for training . The target side of the remaining data is treated as additional monolingual data for building TM and the source side is discarded . The data statistics can be found in the upper block of Table 4 . The dev and test sets for each domain contains 2K instances . Models We first train a Transformer Base baseline model 1 on the concatenation of bilingual pairs in all domains . As in we train our model using fixed Etgt model 4 . One advantage of our approach is the possibility of training a single model which can be adapted to any new domain at the inference time without any by just switching the TM . When adapting to a new TM we do not our model . As the purpose here is to verify that our approach can tackle domain adaptation without any training we leave the comparison and combination of other domain adaptation techniques Moore and Lewis 7315 2010 Chu and Wang 2018 as future work . Results The results are presented in Table 4 . We can see that when only using the bilingual data the model obtains higher BLEU scores in domains with less data but slightly lower scores in other domains compared to the baseline . However as we switch the TM to TM the translation quality is significantly boosted in all domains improving the baseline by an average of BLEU points with improvements as large as BLEU points on Law and BLEU point on Medical . We also attempt to combine all TMs to one and use it for all domains the last row in Table 4 . However we do not obtain noticeable improvement . This reveals that the data can provide little help so that a smaller TM is sufficient which is also confirmed by the fact that about of the retrieved sentences come from the corresponding domain in the combined TM . Running Speed With the help of FAISS index search over millions of vectors can be made incredibly efficient often in tens of milliseconds . In our implementation the memory search performs even faster than naive BM253 . For the results in Table 2 taking the vanilla Transformer Base model model 1 as the baseline . The inference latency of our models both model 4 and model 5 is about times of the baseline all use a single Nividia V100 GPU . Note that the corresponding number for the previous model Xia et 2019 is . As for training cost the averaged time cost per training step of model 4 and model 5 is times and times of the baseline respectively which are on par with traditional baselines model 2 is times all use two Nividia V100 GPUs . Table 5 presents the results . In addition we also observe that models converge much faster than vanilla models in terms of training steps . 5 Conclusion We introduced an effective approach that augments NMT models with monolingual TM . We show that a memory retriever can be learned by MT training . Our approach achieves new results on sev3Elasticsearch Implementation: https: . Model Training Inference 1 Transformer Base 2 source similarity 4 fixed Etgt 5 Xia et al . 2019 Table 5: Latency cost for training and inference . For training we measure the averaged time cost per training step . The number of Xia et al . 2019 is inferred from their paper . eral datasets leads to large gains in scenarios where the bilingual data is limited and can specialize a NMT model for specific domains without further training . Future work should aim to build over our proposed framework . Two obvious directions are : 1 Even though our experiments validated that the whole framework can be learned from scratch using standard MT corpora it is possible to initialize each model component in our framework with massively models for performance enhancement and 2 The NMT model can benefit from aggregating over a set of diverse memories which is not explicitly encouraged in current design.