Abstract We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large dataset is adapted to a small dataset . In this scenario overfitting is a major challenge . We investigate a number of techniques to reduce overfitting and improve transfer learning including regularization techniques such as dropout and regularization towards an prior . In addition we introduce tuneout a novel regularization technique inspired by dropout . We apply these techniques alone and in combination to neural machine translation obtaining improvements on IWSLT datasets for and . We also investigate the amounts of training data needed for domain adaptation in NMT and find a logarithmic relationship between the amount of training data and gain in BLEU score . 1 Introduction Neural machine translation Bahdanau et 2015 Sutskever et 2014 has established itself as the new state of the art at recent shared translation tasks Bojar et 2016 Cettolo et 2016 . In order to achieve good generalization accuracy neural machine translation like most other large machine learning systems requires large amounts of training examples sampled from a distribution as close as possible to the distribution of the inputs seen during execution . However in many applications only a small amount of parallel text is available for the specific application domain and it is therefore desirable to leverage larger datasets . Owing to the incremental nature of stochastic training algorithms a simple yet effective approach to transfer learning for neural networks is Hinton and Salakhutdinov 2006 Mesnil et 2012 Yosinski et 2014: to continue training an existing model which was trained on data with indomain training data . This strategy was also found to be very effective for neural machine translation Luong and Manning 2015 Sennrich et 2016b . Since the amount of data is typically small overfitting is a concern . A common solution is early stopping on a small validation dataset but this reduces the amount of data available for training . In this paper we show that we can make finetuning strategies for neural machine translation more robust by using several regularization techniques . We consider with varying amounts of training data showing that improvements are logarithmic in the amount of indomain data . We investigate techniques where domain adaptation starts from a model and only needs to process the corpus . Since we do not need to process the large corpus during adaptation this is suitable for scenarios where adaptation must be performed quickly or where the original outdomain corpus is not available . Other works consider techniques that jointly train on the outdomain and corpora distinguishing them using specific input features Daume III 2007 Finkel and Manning 2009 Wuebker et 2015 . These techniques are largely orthogonal to 1489 ours1 and can be used in combination . In fact Chu et al . 2017 successfully apply in combination with joint training . 2 Regularization Techniques for Transfer Learning Overfitting to the small amount of training data that may be available is a major challenge in transfer learning for domain adaptation . We investigate the effect of different regularization techniques to reduce overfitting and improve the quality of transfer learning . Dropout The first variant that we consider is with dropout . Dropout Srivastava et 2014 is a stochastic regularization technique for neural networks . In particular we consider Bayesian dropout for recurrent neural networks Gal and Ghahramani 2016 . In this technique during training the columns of the weight matrices of the neural network are randomly set to zero independently for each example and each epoch but with the caveat that when the same weight matrix appears multiple times in the unrolled computational graph of a given example the same columns are zeroed . For an arbitrary layer that takes an input vector h and computes the vector v ignoring the bias parameter vi j W MW i j hi j 1 where MW i j 1 p diag p is the dropout mask for matrix W and training example i seen in epoch j . This mask is a diagonal matrix whose entries are drawn from independent Bernoulli random variables with probability p and then scaled by . Gal and Ghahramani 2016 have shown that this corresponds to approximate variational Bayesian inference over the weight matrices considered as random variables where the individual weights have a Gaussian prior with zero mean and small diagonal covariance . During execution we simply set the dropout masks to identity matrices as in the standard approximation scheme . Since dropout is not a specific transfer learning technique per se we can apply it during finetuning irrespective of whether or not the orig1 although in the special case of linear models they are related to . inal model was also trained with dropout . regularization is widely used for machine learning and statistical models . For linear models it corresponds to imposing a diagonal Gaussian prior with zero mean on the weights . Chelba and Acero 2006 extended this technique to transfer learning by penalizing the weights of the model by their from the weights of the previously trained model . For each parameter matrix W the penalty term is LW λ W Wˆ 2 2 2 where W is the parameter matrix to be learned and Wˆ is the corresponding fixed parameter matrix . Bias parameters may be regularized as well . For linear models this corresponds to maximum a posteriori inference . a diagonal Gaussian prior with mean equal to the parameters and variance . To our knowledge this method has not been applied to neural networks except for a recent work by Kirkpatrick et al . 2017 which investigates a variant of it for continual learning learning a new task while preserving performance on previously learned task rather than domain adaptation . In this work we investigate from penalization as a domain adaptation technique for neural machine translation . Tuneout We also propose a novel transfer learning technique which we call tuneout . Like Bayesian dropout we randomly drop columns of the weight matrices during training but instead of setting them to zero we set them to the corresponding columns of the parameter matrices . This can be alternatively seen as learning matrices of parameter differences between and models with standard dropout starting from a zero initialization at the beginning of . Therefore equation 2 becomes vi j Wˆ i j hi j 3 where Wˆ is the fixed parameter matrix and is the parameter difference matrix to be learned and i j is a Bayesian dropout mask . 1490 3 Evaluation We evaluate transfer learning on test sets from the IWSLT shared translation task Cettolo et 2012 . Data and Methods Test sets consist of transcripts of TED talks and their translations small amounts of training data are also provided . For we use IWSLT 2015 training data while for we use IWSLT 2014 training data . For the systems we use training data from the WMT shared translation which is considered permissible for IWSLT tasks including of monolingual training data Sennrich et 2016b automatic translations of data available only in target language back into the source . We train systems following tools and hyperparameters reported by Sennrich et al . 2016a using Nematus Sennrich et 2017 as the neural machine translation toolkit . We differ from their setup only in that we use Adam Kingma and Ba 2015 for optimization . Our baseline models use the same hyperparameters except that the learning rate is 4 times smaller and the validation frequency for early stopping 4 times higher . Early stopping serves an important function as the only form of regularization in the baseline model . We also use this configuration for the only baselines . After some exploratory experiments for we set dropout retention probabilities to for and for all the other parameter matrices . Tuneout retention probabilities are set to and other parameters . For regularization we found that a penalty of per performs best . For retention probabilities of other parameters for both dropout and tuneout performed best . The training data consists of about sentence pairs for and sentence pairs for . training data is about 206k sentence pairs for and 181k sentence pairs for . Training 2 http : 3 http : 0 2 4 6 8 10 27 28 29 30 31 Training BLEU dropout tuneout Figure 1: validation BLEU over training . data is tokenized truecased and segmented into subword units using encoding BPE Sennrich et 2016c . For replicability and ease of adoption we include our implementation of dropout and in the master branch of Nematus . Tuneout regularization is available in a separate code branch of Results We report the translation quality in terms of NISTBLEU scores of our models in Table 1 for and Table 2 for . Statistical significance on the concatenated test sets scores is determined via bootstrap resampling Koehn 2004 . Dropout and improve translation quality when both separately and in combination . When the two methods are used in combination the improvements are significant at 5 for both language pairs while in isolation dropout is and is only significant for . Tuneout does not yield improvements for in fact it is significantly worse but yields a small nonsignificant improvement for . In order to obtain a better picture of the training dynamics we plot training curves5 for several of our models in Figure 1 . 4 https : 5 These BLEU scores are computed using Moses which gives slightly different results than NIST that is used for Table 1 . 1491 Table 1: translation BLEU scores valid test System tst2010 tst2011 tst2012 tst2013 avg only only dropout tuneout dropout: different from the baseline at 5 significance . Table 2: translation BLEU scores valid test System dev2010 tst2011 tst2012 tst2013 avg only dropout tuneout dropout: different from the baseline at 5 significance . Baseline starts to noticeably overfit between the second and third epoch 1 epoch 104 while dropout and tuneout seem to converge without displaying noticeable overfitting . In our experiments all forms of regularization including early stopping have shown to be successful at mitigating the effect of overfitting . Still our results suggest that there is value in not relying only on early stopping: our results suggest that multiple regularizers outperform a single one . if the amount of data is very small we may want to use all of it for and not hold out any for early stopping . To evaluate different streategies on varying amounts of data we tested finetuning with random samples of data ranging from 10 sentence pairs to the full data set of 206k sentence pairs . with low amounts of training data is of special interest for online adaptation scenarios where a system is fed back Results are shown 6 We expect even bigger gains in that scenario because we would not train on a random sample but on translations that are conceivably from the same document . 101 102 103 104 105 25 26 27 28 29 30 baseline training sentence pairs BLEU 1 epoch 5 epochs early stop early stop dropout 5 epochs dropout Figure 2: test BLEU with finetuning on different data set size . Baseline trained on WMT data . 1492 in Figure 2 . The results show an approximately logarithmic relation between the size of the training set and BLEU . We consider three baseline approaches: for a fixed number of epochs 1 or 5 or early stopping . All three baseline approaches have their disadvantages . for 1 epoch shows underfitting on small amounts of data less than sentence pairs for 5 epochs overfits on sentence pairs . Early stopping is generally a good strategy but it requires an dataset . On the same amount of data regularization leads to performance that is better or no worse than the baseline with only early stopping . with regularization is also more stable and if we have no access to a valdiation set for early stopping can be run for a fixed number of epochs with little or no accuracy loss . 4 Conclusion We investigated for domain adaptation in neural machine translation with different amounts of training data and strategies to avoid overfitting . We found that our baseline that relies only on early stopping has a strong performance but with recurrent dropout and with regularization yield additional small improvements of the order of BLEU points for both and while the improvements in terms of final translation accuracy of tuneout appear to be less consistent . Furthermore we found that regularization techniques that we considered make training more robust to overfitting which is particularly helpful in scenarios where only small amounts of data is available making impractical as it relies on a sufficiently large validation set . Given the results of our experiments we recommend using both dropout and regularization for tasks since they are easy to implement efficient and yield improvements while stabilizing training . We also present a learning curve that shows a logarithmic relationship between the amount of training data and the quality of the adapted system . Our techniques are not specific to neural machine translation and we propose that they could be also tried for other neural network architectures and other tasks . Acknowledgments This project has received funding from the European Union s Horizon 2020 research and innovation programme under grant agreements 644333 TraMOOC and 645487 ModernMT . We also thank for their support.