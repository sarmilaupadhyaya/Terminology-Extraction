,Terminology,Rasul,Sharmila
0,phrase memory,1,1
1,monolingual data,1,1
2,parallel sentences,1,1
3,language pairs,1,1
4,retrieval model,1,1
5,stability training,1,1
6,z k,0,0
7,NMT models,1,1
8,adversarial stability,1,1
9,language model,1,1
10,vocabulary size,1,1
11,source sentence,1,1
12,language models,1,1
13,pivot language,1,1
14,optimal transport,1,1
15,phrase generation,1,1
16,external knowledge,1,1
17,translation line,1,1
18,multilingual baseline,1,1
19,subword units,1,1
20,target word,1,1
21,target phrases,1,1
22,translation model,1,1
23,reference translations,1,1
24,transport matrix,1,1
25,source line,1,1
26,translation z,0,0
27,attention model,1,1
28,training data,1,1
29,source language,1,1
30,local attention,1,1
31,logographic languages,1,1
32,level data,1,1
33,linear transformations,1,1
34,candidate translation,1,1
35,adversarial learning,1,1
36,memory retriever,1,1
37,Saami languages,1,1
38,translation options,1,1
39,sentence pairs,1,1
40,target language,1,1
41,language pair,1,1
42,beam search,1,1
43,rare words,1,1
44,baseline system,1,1
45,monolingual corpora,1,1
46,parallel sentence,1,1
47,Transformer Base,1,1
48,pretrained embeddings,1,1
49,bilingual pairs,1,1
50,relevance scores,1,1
51,attentional models,1,1
52,context vector,1,1
53,Jean et,0,0
54,brevity penalty,1,1
55,phrase table,1,1
56,source side,1,1
57,long sentences,1,0
58,human evaluation,1,1
59,training set,1,1
60,reference translation,1,1
61,gloss line,1,1
62,o t,0,0
63,BERT regressor,1,1
64,priority queue,1,1
65,Adaptive Decoder,1,1
66,global attention,1,1
67,new language,0,0
68,phrase tables,1,1
69,word order,1,1
70,final output,1,1
71,domain adaptation,1,1
72,source words,1,1
73,neural network,1,1
74,target words,1,1
75,GPU hours,1,1
76,X P,0,0
77,sequence level,1,1
78,unseen words,1,1
79,grammatical error,1,1
80,error correction,1,1
81,NIST WMT,0,0
82,zi M,0,0
83,source similarity,1,1
84,pruning rate,1,1
85,source word,1,1
86,system performance,1,1
87,unigram precision,1,1
88,masked language,1,1
89,evaluation score,1,1
90,language modeling,1,1
91,translation performance,1,1
92,low resource,1,1
93,objective function,1,1
94,translation tasks,1,1
95,target sentence,1,1
96,translation system,1,1
97,target phrase,1,1
98,language family,1,1
99,source positions,1,1
100,vector ct,0,0
101,human judgments,1,1
102,token candidates,1,1
103,stroke sequences,1,1
104,subword models,1,1
105,sentence extraction,1,1
106,Boston March,0,0
107,Workshop LoResMT,0,1
108,similarity search,1,1
109,good words,0,0
110,parameter interference,1,1
111,mask similarity,1,1
112,et al,0,0
113,training objective,1,1
114,bilingual dictionary,1,1
115,translation process,1,1
116,translation systems,1,1
117,test set,1,1
118,validation set,1,1
119,source sentences,1,1
120,Machine Translation,1,1
121,statistical machine,1,1
122,translation quality,1,1
123,character level,1,1
124,baseline model,1,1
125,input sentence,1,1
126,parallel corpora,1,1
127,dev test,1,1
128,word segmentation,1,1
129,training text,1,1
130,neural machine,1,1
131,neural networks,1,1
132,translation task,1,1
133,attention mechanism,1,1
134,target side,1,1
135,translation models,1,1
136,neural models,1,1
137,translation directions,1,1
138,test time,1,1
139,Fr Ru He,0,0
140,P i,0,0
141,Ru He Ar,0,0
142,Ru He,0,0
143,Nl Ro,0,0
144,Fr Ru,0,0
145,He Ar,0,0
146,Tr De,0,0
147,discrete optimization,1,1
148,different characters,0,0
149,stroke data,1,1
150,NA NA,0,0
151,development languages,1,1
152,final prediction,1,1
153,output gate,1,1
154,unigram F1,1,1
155,human translations,1,1
156,monolingual group,1,1
157,Dev Prec,0,0
158,MLE ASTlexical ASTfeature,0,0
159,loss functions,1,1
160,small perturbations,1,1
161,ASTlexical ASTfeature,0,0
162,MLE ASTlexical,0,0
163,quality score,1,1
164,queue A,0,0
165,generation mode,1,1
166,phrasal recommendations,1,1
167,shallow fusion,1,1
168,middle position,1,1
169,syntactic analysis,1,1
170,BLEU ACC,1,1
171,specific capacity,1,1
172,hidden state,1,1
173,test sets,1,1
174,training time,1,1
175,Wang et,0,0
176,target languages,1,1
177,unknown words,1,1
178,NMT systems,1,1
179,optimal vocabulary,1,1
180,translation word,1,1
181,best system,1,1
182,new translation,1,1
183,Johnson et,0,0
184,syntactic information,1,1
185,machine translation,1,1
186,other hand,0,0
187,machine learning,1,1
188,h t,0,0
189,sentence pair,1,1
190,single model,1,1
191,previous approaches,0,0
192,parallel data,1,1
193,parallel corpus,1,1
194,source target,1,1
195,prior work,1,1
196,model parameters,1,1
197,Tr De Vi,0,0
198,Nl Ro Tr,0,0
199,PTbr Fr Ru,0,0
200,Es PTbr Fr,0,0
201,Marginal Utility,1,1
202,bilingual translation,1,1
203,Es PTbr,0,0
204,Ro Tr De,0,0
205,De Vi,0,0
206,PTbr Fr,0,0
207,Ro Tr,0,0
208,trial training,1,1
209,Chinese characters,1,1
210,NMT tasks,1,1
211,logographic language,1,1
212,source morpheme,1,1
213,NA NA NA,0,0
214,previous step,1,1
215,c t,0,0
216,bilingual group,1,1
217,bigram precision,1,1
218,test corpus,1,1
219,Il Ragazzini,0,0
220,leva b,0,0
221,noisy perturbations,1,1
222,extraction system,1,1
223,small corpus,1,1
224,whole corpus,1,1
225,decoder g,0,0
226,termination policy,1,1
227,unsupervised setting,1,1
228,static embeddings,1,1
229,linear mapping,1,1
230,dependency parent,1,1
231,memory retrieval,1,1
232,memory encoder,1,1
233,lexical selection,1,1
234,alignment vector,1,1
235,attentional mechanism,1,1
236,reverse dropout,1,1
237,current target,1,1
238,alignment functions,1,1
239,global location,1,1
240,decoder mask,1,1
241,more language,0,0
242,BLEU ACC BLEU,0,0
243,ACC BLEU ACC,0,0
244,ACC BLEU,1,1
245,sentence length,1,0
246,De Es,0,0
247,promising results,0,1
248,arg max,1,1
249,multilingual translation,1,1
250,recurrent models,1,1
251,t g,0,0
252,specific language,1,1
253,unsupervised way,1,1
254,consistent gains,1,1
255,source position,1,1
256,learning rate,1,1
257,search space,1,1
258,character sequences,1,1
259,learning framework,1,1
260,input features,1,1
261,training steps,1,1
262,step t,0,0
263,target text,1,1
264,human translator,1,1
265,phrase translations,1,1
266,monolingual sentences,1,1
267,time step,1,1
268,training corpus,1,1
269,byte pair,1,1
270,pair encoding,1,1
271,better performance,1,1
272,performance gains,1,1
273,word embeddings,1,1
274,Neural Machine,1,1
275,gradient descent,1,1
276,batch size,1,1
277,development set,1,1
278,other language,0,0
279,Transformer Vaswani,0,0
280,Neural Machine Translation,1,1
281,Sutskever et,0,0
282,significant improvements,0,1
283,data sets,1,1
284,linguistic information,1,1
285,total number,0,0
286,training sets,1,1
287,target sentences,1,1
288,loss function,1,1
289,final vocabulary,1,1
290,average length,1,1
291,hP Di,0,0
292,t t,0,0
293,S t,0,0
294,marginal utility,1,1
295,vocabulary construction,1,1
296,distance matrix,1,1
297,alphabetic languages,1,1
298,ideograph data,1,1
299,NMT BLEU,1,1
300,Natural language processing,1,1