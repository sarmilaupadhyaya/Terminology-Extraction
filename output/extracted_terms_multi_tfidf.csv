,Terminology
0,phrase memory
1,monolingual data
2,parallel sentences
3,language pairs
4,retrieval model
5,stability training
6,z k
7,NMT models
8,adversarial stability
9,language model
10,vocabulary size
11,source sentence
12,language models
13,pivot language
14,optimal transport
15,phrase generation
16,external knowledge
17,translation line
18,multilingual baseline
19,subword units
20,target word
21,target phrases
22,translation model
23,reference translations
24,transport matrix
25,source line
26,translation z
27,attention model
28,training data
29,source language
30,local attention
31,logographic languages
32,level data
33,linear transformations
34,candidate translation
35,adversarial learning
36,memory retriever
37,Saami languages
38,translation options
39,sentence pairs
40,target language
41,language pair
42,beam search
43,rare words
44,baseline system
45,monolingual corpora
46,parallel sentence
47,Transformer Base
48,pretrained embeddings
49,bilingual pairs
50,relevance scores
51,attentional models
52,context vector
53,Jean et
54,brevity penalty
55,phrase table
56,source side
57,long sentences
58,human evaluation
59,training set
60,reference translation
61,gloss line
62,o t
63,BERT regressor
64,priority queue
65,Adaptive Decoder
66,global attention
67,new language
68,phrase tables
69,word order
70,final output
71,domain adaptation
72,source words
73,neural network
74,target words
75,GPU hours
76,X P
77,sequence level
78,unseen words
79,grammatical error
80,error correction
81,NIST WMT
82,zi M
83,source similarity
84,pruning rate
85,source word
86,system performance
87,unigram precision
88,masked language
89,evaluation score
90,language modeling
91,translation performance
92,low resource
93,objective function
94,translation tasks
95,target sentence
96,translation system
97,target phrase
98,language family
99,source positions
100,vector ct
101,human judgments
102,token candidates
103,stroke sequences
104,subword models
105,sentence extraction
106,Boston March
107,Workshop LoResMT
108,similarity search
109,good words
110,parameter interference
111,mask similarity
112,et al
113,training objective
114,bilingual dictionary
115,translation process
116,translation systems
117,test set
118,validation set
119,source sentences
120,Machine Translation
121,statistical machine
122,translation quality
123,character level
124,baseline model
125,input sentence
126,parallel corpora
127,dev test
128,word segmentation
129,training text
130,neural machine
131,neural networks
132,translation task
133,attention mechanism
134,target side
135,translation models
136,neural models
137,translation directions
138,test time
139,Fr Ru He
140,P i
141,Ru He Ar
142,Ru He
143,Nl Ro
144,Fr Ru
145,He Ar
146,Tr De
147,discrete optimization
148,different characters
149,stroke data
150,NA NA
151,development languages
152,final prediction
153,output gate
154,unigram F1
155,human translations
156,monolingual group
157,Dev Prec
158,MLE ASTlexical ASTfeature
159,loss functions
160,small perturbations
161,ASTlexical ASTfeature
162,MLE ASTlexical
163,quality score
164,queue A
165,generation mode
166,phrasal recommendations
167,shallow fusion
168,middle position
169,syntactic analysis
170,BLEU ACC
171,specific capacity
172,hidden state
173,test sets
174,training time
175,Wang et
176,target languages
177,unknown words
178,NMT systems
179,optimal vocabulary
180,translation word
181,best system
182,new translation
183,Johnson et
184,syntactic information
185,machine translation
186,other hand
187,machine learning
188,h t
189,sentence pair
190,single model
191,previous approaches
192,parallel data
193,parallel corpus
194,source target
195,prior work
196,model parameters
197,Tr De Vi
198,Nl Ro Tr
199,PTbr Fr Ru
200,Es PTbr Fr
201,Marginal Utility
202,bilingual translation
203,Es PTbr
204,Ro Tr De
205,De Vi
206,PTbr Fr
207,Ro Tr
208,trial training
209,Chinese characters
210,NMT tasks
211,logographic language
212,source morpheme
213,NA NA NA
214,previous step
215,c t
216,bilingual group
217,bigram precision
218,test corpus
219,Il Ragazzini
220,leva b
221,noisy perturbations
222,extraction system
223,small corpus
224,whole corpus
225,decoder g
226,termination policy
227,unsupervised setting
228,static embeddings
229,linear mapping
230,dependency parent
231,memory retrieval
232,memory encoder
233,lexical selection
234,alignment vector
235,attentional mechanism
236,reverse dropout
237,current target
238,alignment functions
239,global location
240,decoder mask
241,more language
242,BLEU ACC BLEU
243,ACC BLEU ACC
244,ACC BLEU
245,sentence length
246,De Es
247,promising results
248,arg max
249,multilingual translation
250,recurrent models
251,t g
252,specific language
253,unsupervised way
254,consistent gains
255,source position
256,learning rate
257,search space
258,character sequences
259,learning framework
260,input features
261,training steps
262,step t
263,target text
264,human translator
265,phrase translations
266,monolingual sentences
267,time step
268,training corpus
269,byte pair
270,pair encoding
271,better performance
272,performance gains
273,word embeddings
274,Neural Machine
275,gradient descent
276,batch size
277,development set
278,other language
279,Transformer Vaswani
280,Neural Machine Translation
281,Sutskever et
282,significant improvements
283,data sets
284,linguistic information
285,total number
286,training sets
287,target sentences
288,loss function
289,final vocabulary
290,average length
291,hP Di
292,t t
293,S t
294,marginal utility
295,vocabulary construction
296,distance matrix
297,alphabetic languages
298,ideograph data
299,NMT BLEU
