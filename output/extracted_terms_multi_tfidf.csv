,Terminology
0,phrase memory
1,monolingual data
2,parallel sentences
3,language pairs
4,retrieval model
5,stability training
6,adversarial stability
7,language model
8,vocabulary size
9,source sentence
10,language models
11,pivot language
12,optimal transport
13,phrase generation
14,external knowledge
15,translation line
16,multilingual baseline
17,subword units
18,target word
19,target phrases
20,translation model
21,reference translations
22,transport matrix
23,source line
24,attention model
25,training data
26,source language
27,local attention
28,logographic languages
29,level data
30,linear transformations
31,candidate translation
32,adversarial learning
33,memory retriever
34,Saami languages
35,translation options
36,sentence pairs
37,target language
38,language pair
39,beam search
40,rare words
41,baseline system
42,monolingual corpora
43,parallel sentence
44,Transformer Base
45,pretrained embeddings
46,bilingual pairs
47,relevance scores
48,attentional models
49,context vector
50,brevity penalty
51,phrase table
52,source side
53,long sentences
54,human evaluation
55,reference translation
56,gloss line
57,BERT regressor
58,priority queue
59,Adaptive Decoder
60,global attention
61,phrase tables
62,word order
63,final output
64,domain adaptation
65,source words
66,neural network
67,target words
68,sequence level
69,unseen words
70,error correction
71,grammatical error
72,source similarity
73,pruning rate
74,source word
75,system performance
76,unigram precision
77,evaluation score
78,masked language
79,language modeling
80,translation performance
81,objective function
82,translation tasks
83,target sentence
84,translation system
85,target phrase
86,language family
87,source positions
88,human judgments
89,token candidates
90,stroke sequences
91,subword models
92,sentence extraction
93,Workshop LoResMT
94,Boston March
95,similarity search
96,good words
97,parameter interference
98,mask similarity
99,training objective
100,bilingual dictionary
101,translation process
102,translation systems
103,source sentences
104,Machine Translation
105,statistical machine
106,translation quality
107,character level
108,baseline model
109,input sentence
110,parallel corpora
111,word segmentation
112,training text
113,neural machine
114,neural networks
115,translation task
116,target side
117,attention mechanism
118,translation models
119,translation directions
120,neural models
121,test time
122,discrete optimization
123,stroke data
124,different characters
125,final prediction
126,development languages
127,output gate
128,human translations
129,monolingual group
130,small perturbations
131,ASTlexical ASTfeature
132,loss functions
133,quality score
134,generation mode
135,phrasal recommendations
136,shallow fusion
137,middle position
138,syntactic analysis
139,specific capacity
140,hidden state
141,test sets
142,training time
143,target languages
144,unknown words
145,optimal vocabulary
146,translation word
147,best system
148,syntactic information
149,machine translation
150,other hand
151,machine learning
152,sentence pair
153,single model
154,previous approaches
155,parallel data
156,source target
157,parallel corpus
158,prior work
159,model parameters
160,Marginal Utility
161,trial training
162,bilingual translation
163,logographic language
164,Chinese characters
165,source morpheme
166,previous step
167,bilingual group
168,bigram precision
169,test corpus
170,noisy perturbations
171,extraction system
172,small corpus
173,whole corpus
174,termination policy
175,unsupervised setting
176,static embeddings
177,linear mapping
178,dependency parent
179,memory encoder
180,memory retrieval
181,lexical selection
182,alignment vector
183,reverse dropout
184,global location
185,alignment functions
186,attentional mechanism
187,current target
188,more language
189,decoder mask
190,sentence length
191,promising results
192,multilingual translation
193,recurrent models
194,specific language
195,consistent gains
196,source position
197,learning rate
198,search space
199,character sequences
200,learning framework
201,training steps
202,input features
203,target text
204,human translator
205,phrase translations
206,monolingual sentences
207,training corpus
208,time step
209,byte pair
210,pair encoding
211,performance gains
212,better performance
213,word embeddings
214,Neural Machine
215,gradient descent
216,batch size
217,other language
218,Transformer Vaswani
219,Neural Machine Translation
220,significant improvements
221,data sets
222,training sets
223,linguistic information
224,total number
225,target sentences
226,loss function
227,distance matrix
228,final vocabulary
229,marginal utility
230,average length
231,vocabulary construction
232,stroke models
233,ideograph models
234,alphabetic languages
235,ideograph data
236,writing systems
237,gram list
238,Stem Gram
239,current input
240,BLEU Para
241,current step
242,efficient parallelization
243,frequency rank
244,subword systems
245,BLEU CHRF3
246,character bigrams
247,human judges
248,word choice
249,single reference
250,monolingual language
251,neural methods
252,example sentences
253,general translation
254,basic idea
255,simultaneous translation
256,noisy input
257,score evaluation
258,language sentences
259,correlation score
260,reference sentences
261,Shared Task
262,XLM15 XLM100
263,evaluation scores
264,multilingual BERT
265,past translation
266,prior methods
267,Adaptive Decoder Geng
268,oracle experiment
269,Decoder Geng
270,proper termination
271,multiple passes
272,unsupervised phrase
273,target source
274,BLEU CHRF1
275,monolingual embeddings
276,lexical alignment
277,chunk phrases
278,highest probability
279,relevant target
280,first layer
281,Machine translation
282,translation applications
283,parameter matrix
284,regularization techniques
285,asynchronous index
286,manual translation
287,other Saami languages
288,output text
289,North Saami
290,global approach
291,known techniques
292,Central Bank
293,Miranda Kerr
294,attention models
295,soft attention
296,attention approach
297,hard attention
298,attentional model
299,Orlando Bloom
