,Terminology,Rasul,Sharmila
0,machine translation,1,1
1,language pairs,1,1
2,training data,1,1
3,source sentence,1,1
4,language model,1,1
5,NMT models,1,1
6,translation quality,1,1
7,neural machine,1,1
8,language models,1,1
9,parallel sentences,1,1
10,monolingual data,1,1
11,sentence pairs,1,1
12,et al,0,0
13,target language,1,1
14,translation model,1,1
15,vocabulary size,1,1
16,source language,1,1
17,language pair,1,1
18,neural network,1,1
19,phrase memory,1,1
20,translation tasks,1,1
21,test sets,1,1
22,source side,1,1
23,subword units,1,1
24,target word,1,1
25,translation performance,1,1
26,translation systems,1,1
27,test set,1,1
28,beam search,1,1
29,attention model,1,1
30,Machine Translation,1,1
31,target sentence,1,1
32,source word,1,1
33,translation system,1,1
34,translation task,1,1
35,reference translations,1,1
36,training set,1,1
37,reference translation,1,1
38,neural networks,1,1
39,source words,1,1
40,rare words,1,1
41,baseline system,1,1
42,source sentences,1,1
43,monolingual corpora,1,1
44,parallel sentence,1,1
45,retrieval model,1,1
46,attention mechanism,1,1
47,target side,1,1
48,translation models,1,1
49,context vector,1,1
50,bilingual dictionary,1,1
51,stability training,1,1
52,z k,0,0
53,learning rate,1,1
54,statistical machine,1,1
55,adversarial stability,1,1
56,target phrases,1,1
57,low resource,1,1
58,word order,1,1
59,translation process,1,1
60,pivot language,1,1
61,optimal transport,1,1
62,target languages,1,1
63,baseline model,1,1
64,input sentence,1,1
65,long sentences,1,1
66,target words,1,1
67,human judgments,1,1
68,human evaluation,1,1
69,model parameters,1,1
70,parallel corpora,1,1
71,phrase generation,1,1
72,local attention,1,1
73,external knowledge,1,1
74,training time,1,1
75,objective function,1,1
76,Sennrich et,0,1
77,translation line,1,1
78,final output,1,1
79,Transformer Base,1,1
80,parallel data,1,1
81,domain adaptation,1,1
82,validation set,1,1
83,multilingual baseline,1,1
84,translation directions,1,1
85,transport matrix,1,1
86,neural models,1,1
87,Cho et,0,0
88,source line,1,1
89,test time,1,1
90,gradient descent,1,1
91,sentence length,1,1
92,other hand,0,0
93,brevity penalty,1,1
94,Jean et,0,0
95,training objective,1,1
96,translation z,0,0
97,phrase table,1,1
98,NMT systems,1,1
99,word embeddings,1,1
100,level data,1,1
101,character level,1,1
102,logographic languages,1,1
103,unknown words,1,1
104,linear transformations,1,1
105,Neural Machine,1,1
106,candidate translation,1,1
107,adversarial learning,1,1
108,phrase tables,1,1
109,memory retriever,1,1
110,Saami languages,0,0
111,translation options,1,1
112,beam size,1,1
113,natural language,1,1
114,same time,0,0
115,significant improvements,1,1
116,Sutskever et,0,0
117,system performance,1,1
118,performance gains,1,1
119,better performance,1,1
120,Vaswani et,1,1
121,unigram precision,1,1
122,evaluation score,1,1
123,masked language,1,1
124,Wang et,0,0
125,language modeling,1,1
126,pretrained embeddings,1,1
127,relevance scores,1,1
128,bilingual pairs,1,1
129,attentional models,1,1
130,training corpus,1,1
131,time step,1,1
132,batch size,1,1
133,pair encoding,1,1
134,byte pair,1,1
135,significant improvement,1,1
136,gloss line,1,1
137,o t,0,0
138,hidden state,1,1
139,source target,1,1
140,parallel corpus,1,1
141,prior work,1,1
142,BERT regressor,1,1
143,priority queue,1,1
144,Adaptive Decoder,1,1
145,target phrase,1,1
146,language family,1,1
147,source positions,1,1
148,vector ct,1,1
149,global attention,1,1
150,new language,1,1
151,GPU hours,1,1
152,X P,0,0
153,anonymous reviewers,0,1
154,development set,1,1
155,Introduction Neural,1,1
156,machine learning,1,1
157,other language,1,1
158,stochastic gradient,1,1
159,sequence level,1,1
160,h t,0,0
161,dev test,1,1
162,previous work,1,1
163,Transformer Vaswani,1,1
164,word segmentation,1,1
165,training text,1,1
166,unseen words,1,1
167,error rate,1,1
168,error correction,1,1
169,sentence pair,1,1
170,grammatical error,1,1
171,single model,1,1
172,monolingual sentences,1,1
173,Neural Machine Translation,1,1
174,previous approaches,1,1
175,NIST WMT,0,0
176,source similarity,1,1
177,zi M,0,0
178,pruning rate,1,1
179,search space,1,1
180,character sequences,1,1
181,optimal vocabulary,1,1
182,token candidates,1,1
183,stroke sequences,1,1
184,data sets,1,1
185,better results,0,0
186,future work,0,0
187,learning framework,1,1
188,translation word,1,1
189,training sets,1,1
190,total number,1,1
191,linguistic information,1,1
192,step t,0,0
193,training steps,1,1
194,input features,1,1
195,subword models,1,1
196,target text,1,1
197,different models,1,1
198,correct translation,1,1
199,target sentences,1,1
200,human translator,1,1
201,best results,1,1
202,best system,1,1
203,loss function,1,1
204,BLEU Papineni,0,0
205,sentence extraction,1,1
206,new translation,1,1
207,phrase translations,1,1
208,Johnson et,0,0
209,syntactic information,1,1
210,Workshop LoResMT,0,0
211,Boston March,1,0
212,similarity search,1,1
213,good words,0,0
214,parameter interference,1,1
215,mask similarity,1,1
216,language processing,1,1
217,P i,0,0
218,arg max,1,1
219,target vocabulary,1,1
220,promising results,1,1
221,discrete optimization,1,1
222,multilingual translation,1,1
223,Fr Ru He,0,0
224,Ru He Ar,0,0
225,Encoding BPE,1,1
226,De Es,0,0
227,Fr Ru,0,0
228,Ru He,0,0
229,He Ar,0,0
230,Nl Ro,0,0
231,Tr De,0,0
232,stroke data,1,1
233,recurrent unit,1,1
234,large number,1,1
235,different characters,1,1
236,best model,1,1
237,development languages,1,1
238,precision scores,1,1
239,Related Work,1,1
240,final prediction,1,1
241,NA NA,0,0
242,t g,0,0
243,output gate,1,1
244,layer normalization,1,1
245,weight matrices,1,1
246,recurrent models,1,1
247,short sentences,1,1
248,recurrent neural networks,1,1
249,translation results,1,1
250,unigram F1,1,1
251,European Union,1,1
252,translation evaluation,1,1
253,human translation,1,1
254,human translations,1,1
255,good translation,1,1
256,monolingual group,1,1
257,Vaswani et al,0,0
258,Dev Prec,1,0
259,translation hypothesis,1,1
260,vector representation,1,1
261,specific language,1,1
262,same hyperparameters,1,1
263,other languages,1,1
264,Machine Translation NMT,1,1
265,Translation NMT,1,1
266,loss functions,1,1
267,small perturbations,1,1
268,strong baseline,1,1
269,MLE ASTlexical ASTfeature,0,0
270,MLE ASTlexical,0,0
271,ASTlexical ASTfeature,0,0
272,bilingual training,1,1
273,quality score,1,1
274,queue A,0,0
275,unsupervised way,0,1
276,generation mode,1,0
277,phrasal recommendations,1,1
278,conditional probability,1,1
279,p yt,0,0
280,yt t,0,0
281,transfer learning,1,1
282,shallow fusion,1,1
283,middle position,0,0
284,consistent gains,1,1
285,syntactic analysis,1,1
286,source position,1,1
287,specific capacity,1,1
288,BLEU ACC,1,1
289,trial training,1,1
290,model capacity,1,1
291,bilingual translation,1,1
292,best performance,1,1
293,Es PTbr Fr,0,0
294,PTbr Fr Ru,0,0
295,Nl Ro Tr,0,0
296,Ro Tr De,0,0
297,Tr De Vi,0,0
298,Marginal Utility,1,1
299,BPE Sennrich,1,1