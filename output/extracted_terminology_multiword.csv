,Terminology
0,machine translation
1,language pairs
2,training data
3,source sentence
4,language model
5,NMT models
6,translation quality
7,neural machine
8,language models
9,parallel sentences
10,monolingual data
11,sentence pairs
12,et al
13,target language
14,translation model
15,vocabulary size
16,source language
17,language pair
18,neural network
19,phrase memory
20,translation tasks
21,test sets
22,source side
23,subword units
24,target word
25,translation performance
26,translation systems
27,test set
28,beam search
29,attention model
30,Machine Translation
31,target sentence
32,source word
33,translation system
34,translation task
35,reference translations
36,training set
37,reference translation
38,neural networks
39,source words
40,rare words
41,baseline system
42,source sentences
43,monolingual corpora
44,parallel sentence
45,retrieval model
46,attention mechanism
47,target side
48,translation models
49,context vector
50,bilingual dictionary
51,stability training
52,z k
53,learning rate
54,statistical machine
55,adversarial stability
56,target phrases
57,low resource
58,word order
59,translation process
60,pivot language
61,optimal transport
62,target languages
63,baseline model
64,input sentence
65,long sentences
66,target words
67,human judgments
68,human evaluation
69,model parameters
70,parallel corpora
71,phrase generation
72,local attention
73,external knowledge
74,training time
75,objective function
76,Sennrich et
77,translation line
78,final output
79,Transformer Base
80,parallel data
81,domain adaptation
82,validation set
83,multilingual baseline
84,translation directions
85,transport matrix
86,neural models
87,Cho et
88,source line
89,test time
90,gradient descent
91,sentence length
92,other hand
93,brevity penalty
94,Jean et
95,training objective
96,translation z
97,phrase table
98,NMT systems
99,word embeddings
100,level data
101,character level
102,logographic languages
103,unknown words
104,linear transformations
105,Neural Machine
106,candidate translation
107,adversarial learning
108,phrase tables
109,memory retriever
110,Saami languages
111,translation options
112,beam size
113,natural language
114,same time
115,significant improvements
116,Sutskever et
117,system performance
118,performance gains
119,better performance
120,Vaswani et
121,unigram precision
122,evaluation score
123,masked language
124,Wang et
125,language modeling
126,pretrained embeddings
127,relevance scores
128,bilingual pairs
129,attentional models
130,training corpus
131,time step
132,batch size
133,pair encoding
134,byte pair
135,significant improvement
136,gloss line
137,o t
138,hidden state
139,source target
140,parallel corpus
141,prior work
142,BERT regressor
143,priority queue
144,Adaptive Decoder
145,target phrase
146,language family
147,source positions
148,vector ct
149,global attention
150,new language
151,GPU hours
152,X P
153,anonymous reviewers
154,development set
155,Introduction Neural
156,machine learning
157,other language
158,stochastic gradient
159,sequence level
160,h t
161,dev test
162,previous work
163,Transformer Vaswani
164,word segmentation
165,training text
166,unseen words
167,error rate
168,error correction
169,sentence pair
170,grammatical error
171,single model
172,monolingual sentences
173,Neural Machine Translation
174,previous approaches
175,NIST WMT
176,source similarity
177,zi M
178,pruning rate
179,search space
180,character sequences
181,optimal vocabulary
182,token candidates
183,stroke sequences
184,data sets
185,better results
186,future work
187,learning framework
188,translation word
189,training sets
190,total number
191,linguistic information
192,step t
193,training steps
194,input features
195,subword models
196,target text
197,different models
198,correct translation
199,target sentences
200,human translator
201,best results
202,best system
203,loss function
204,BLEU Papineni
205,sentence extraction
206,new translation
207,phrase translations
208,Johnson et
209,syntactic information
210,Workshop LoResMT
211,Boston March
212,similarity search
213,good words
214,parameter interference
215,mask similarity
216,language processing
217,P i
218,arg max
219,target vocabulary
220,promising results
221,discrete optimization
222,multilingual translation
223,Fr Ru He
224,Ru He Ar
225,Encoding BPE
226,De Es
227,Fr Ru
228,Ru He
229,He Ar
230,Nl Ro
231,Tr De
232,stroke data
233,recurrent unit
234,large number
235,different characters
236,best model
237,development languages
238,precision scores
239,Related Work
240,final prediction
241,NA NA
242,t g
243,output gate
244,layer normalization
245,weight matrices
246,recurrent models
247,short sentences
248,recurrent neural networks
249,translation results
250,unigram F1
251,European Union
252,translation evaluation
253,human translation
254,human translations
255,good translation
256,monolingual group
257,Vaswani et al
258,Dev Prec
259,translation hypothesis
260,vector representation
261,specific language
262,same hyperparameters
263,other languages
264,Machine Translation NMT
265,Translation NMT
266,loss functions
267,small perturbations
268,strong baseline
269,MLE ASTlexical ASTfeature
270,MLE ASTlexical
271,ASTlexical ASTfeature
272,bilingual training
273,quality score
274,queue A
275,unsupervised way
276,generation mode
277,phrasal recommendations
278,conditional probability
279,p yt
280,yt t
281,transfer learning
282,shallow fusion
283,middle position
284,consistent gains
285,syntactic analysis
286,source position
287,specific capacity
288,BLEU ACC
289,trial training
290,model capacity
291,bilingual translation
292,best performance
293,Es PTbr Fr
294,PTbr Fr Ru
295,Nl Ro Tr
296,Ro Tr De
297,Tr De Vi
298,Marginal Utility
299,BPE Sennrich
