,Terminology
0,machine translation
1,language pairs
2,training data
3,source sentence
4,language model
5,translation quality
6,neural machine
7,language models
8,parallel sentences
9,monolingual data
10,sentence pairs
11,translation model
12,target language
13,vocabulary size
14,language pair
15,neural network
16,phrase memory
17,translation tasks
18,test sets
19,source side
20,source language
21,subword units
22,target word
23,translation performance
24,beam search
25,attention model
26,target sentence
27,source word
28,translation system
29,translation systems
30,reference translations
31,reference translation
32,translation task
33,neural networks
34,source words
35,rare words
36,baseline system
37,source sentences
38,monolingual corpora
39,parallel sentence
40,retrieval model
41,attention mechanism
42,target side
43,context vector
44,bilingual dictionary
45,stability training
46,Machine Translation
47,learning rate
48,statistical machine
49,translation models
50,adversarial stability
51,target phrases
52,word order
53,translation process
54,pivot language
55,optimal transport
56,target languages
57,baseline model
58,input sentence
59,long sentences
60,target words
61,human judgments
62,human evaluation
63,model parameters
64,parallel corpora
65,phrase generation
66,local attention
67,external knowledge
68,training time
69,objective function
70,translation line
71,final output
72,Transformer Base
73,parallel data
74,domain adaptation
75,multilingual baseline
76,translation directions
77,transport matrix
78,neural models
79,source line
80,test time
81,gradient descent
82,sentence length
83,brevity penalty
84,training objective
85,phrase table
86,word embeddings
87,level data
88,character level
89,logographic languages
90,unknown words
91,linear transformations
92,other hand
93,candidate translation
94,adversarial learning
95,phrase tables
96,memory retriever
97,Saami languages
98,translation options
99,beam size
100,same time
101,significant improvements
102,system performance
103,performance gains
104,better performance
105,unigram precision
106,Neural Machine
107,evaluation score
108,masked language
109,language modeling
110,pretrained embeddings
111,relevance scores
112,bilingual pairs
113,attentional models
114,training corpus
115,time step
116,batch size
117,natural language
118,pair encoding
119,byte pair
120,significant improvement
121,gloss line
122,hidden state
123,source target
124,parallel corpus
125,prior work
126,BERT regressor
127,priority queue
128,Adaptive Decoder
129,target phrase
130,language family
131,source positions
132,global attention
133,anonymous reviewers
134,Introduction Neural
135,machine learning
136,other language
137,stochastic gradient
138,sequence level
139,previous work
140,Transformer Vaswani
141,word segmentation
142,training text
143,unseen words
144,error rate
145,error correction
146,sentence pair
147,grammatical error
148,single model
149,monolingual sentences
150,previous approaches
151,source similarity
152,pruning rate
153,search space
154,character sequences
155,optimal vocabulary
156,token candidates
157,stroke sequences
158,data sets
159,better results
160,future work
161,learning framework
162,translation word
163,training sets
164,total number
165,linguistic information
166,training steps
167,input features
168,subword models
169,different models
170,correct translation
171,target sentences
172,best results
173,best system
174,loss function
175,Neural Machine Translation
176,BLEU Papineni
177,sentence extraction
178,phrase translations
179,syntactic information
180,Workshop LoResMT
181,Boston March
182,similarity search
183,good words
184,parameter interference
185,mask similarity
186,language processing
187,target vocabulary
188,promising results
189,discrete optimization
190,multilingual translation
191,stroke data
192,recurrent unit
193,large number
194,different characters
195,best model
196,development languages
197,precision scores
198,Related Work
199,final prediction
200,output gate
201,layer normalization
202,weight matrices
203,recurrent models
204,short sentences
205,recurrent neural networks
206,target text
207,translation results
208,European Union
209,translation evaluation
210,human translations
211,good translation
212,monolingual group
213,translation hypothesis
214,vector representation
215,specific language
216,same hyperparameters
217,other languages
218,loss functions
219,small perturbations
220,strong baseline
221,ASTlexical ASTfeature
222,bilingual training
223,quality score
224,generation mode
225,phrasal recommendations
226,conditional probability
227,transfer learning
228,shallow fusion
229,middle position
230,consistent gains
231,syntactic analysis
232,source position
233,specific capacity
234,trial training
235,model capacity
236,bilingual translation
237,best performance
238,Marginal Utility
239,word level
240,output layer
241,training step
242,evaluation metric
243,translation examples
244,smaller units
245,English words
246,logographic language
247,Chinese characters
248,structural information
249,frequent words
250,many times
251,simple method
252,source phrase
253,source morpheme
254,training phase
255,word alignments
256,linguistic knowledge
257,native speaker
258,sequence length
259,performance drop
260,previous step
261,hidden states
262,linear transformation
263,subword representations
264,speech recognition
265,recurrent units
266,unknown word
267,high precision
268,Moses Koehn
269,bigram precision
270,test corpus
271,maximum number
272,other words
273,bilingual group
274,corpus size
275,neural system
276,training examples
277,source phrases
278,parallel training
279,noisy perturbations
280,different languages
281,bilingual corpora
282,extraction system
283,sentence encoder
284,bilingual lexicon
285,comparable corpora
286,sentence representation
287,small corpus
288,whole corpus
289,termination policy
290,final translation
291,unsupervised setting
292,recent work
293,generation task
294,static embeddings
295,word generation
296,linear mapping
297,dependency parent
298,memory retrieval
299,memory encoder
